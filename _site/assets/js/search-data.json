{"0": {
    "doc": "Git Concepts",
    "title": "Before you Begin",
    "content": "Reference . In the above link, follow the procedures, but instead of using username and password each time, setup the ssh keys and use them more often . ssh keys are found in ./.ssh folder (or lookup keygen to generate your keys) . ",
    "url": "http://localhost:4000/git_concepts#before-you-begin",
    "relUrl": "/git_concepts#before-you-begin"
  },"1": {
    "doc": "Git Concepts",
    "title": "Basics of generating new content in local and pushing to github",
    "content": " ",
    "url": "http://localhost:4000/git_concepts#basics-of-generating-new-content-in-local-and-pushing-to-github",
    "relUrl": "/git_concepts#basics-of-generating-new-content-in-local-and-pushing-to-github"
  },"2": {
    "doc": "Git Concepts",
    "title": "Process for adding to a github page",
    "content": "git add . git commit -m “made new code” git push or git push origin develop (if you cloned from develop branch) . ",
    "url": "http://localhost:4000/git_concepts#process-for-adding-to-a-github-page",
    "relUrl": "/git_concepts#process-for-adding-to-a-github-page"
  },"3": {
    "doc": "Git Concepts",
    "title": "If you want to track a different branch",
    "content": ". | git branch –set-upstream-to=origin/master git add . git push | . or make a new remote . | git remote add ts_origin_wiki git@github.com:sjayanth21/BR_Wiki.git git push –set-upstream ts_origin_wiki master git push ts_origin_wiki_master | . ",
    "url": "http://localhost:4000/git_concepts#if-you-want-to-track-a-different-branch",
    "relUrl": "/git_concepts#if-you-want-to-track-a-different-branch"
  },"4": {
    "doc": "Git Concepts",
    "title": "Working with remotes",
    "content": "Any folder can have a number of remotes like: origin and ts_origin_github . To make local branch master track a different remote branch (branch in your cloud github repo) do: git branch –set-upstream-to=origin/master . or git branch –set-upstream-to=origin/develop . ",
    "url": "http://localhost:4000/git_concepts#working-with-remotes",
    "relUrl": "/git_concepts#working-with-remotes"
  },"5": {
    "doc": "Git Concepts",
    "title": "Handling merge conflicts",
    "content": "If you cloned a repo or forked your own branch you may need to pull from upstream to update your codebase. However, running a simple ‘git pull’ may throw merge conflicts . Let’s begin by assuming our local branch is called ‘feature/sj’ Our remote branch is called ‘develop’ . So do the following . | Run a ‘git fetch’ to get the updates on all branches (and if any new branch has been added) | In your personal branch commit all changes by doing: git add, commit and push | sudo apt install meld | Now to get the upstream updates do ‘git checkout main’ (or whichever is the upstream branch with latest updates) | Now to put this in your personal branch run ‘git checkout feature/sj’ | Now we do the actual merging using ‘git merge develop’ (this will merge everythin in deveop into the current branch viz feature/sj) | The above step would have thrown some merge conflicts, to solve that run ‘git mergetool’ | The above step opens meld, make all necessary resolutions and save | Now our codebase would have been updated to whatever we resolved in meld | Now run ‘git commit’ without any arguments as it is a ‘merge commit’ | Now as usual do ‘git push origin feature/sj’ to push your updated personal branch to github | . Note. When you open meld, the middle window is the final merged file which you will finally get as output. Your job will be to choose which lines of code : . | from develop | from feature/sj that will be going into your final output. develop may be on the right window and feature/sj may be on the left window. You will have two options: . | Move the lines from the left into both middle and right windows (the code highlight for that line should vanish) | Move the lines from the right into both middle and left windows (the code highlight for that line should vanish) | . | . ",
    "url": "http://localhost:4000/git_concepts#handling-merge-conflicts",
    "relUrl": "/git_concepts#handling-merge-conflicts"
  },"6": {
    "doc": "Git Concepts",
    "title": "Points to Note",
    "content": ". | If you checkout a file ‘git checkout blade.py’ it resets the file to whatever is the latest from that branch in upstream . | If you want to physically add or change remotes go to the respective folder and do ‘nano .git/config’ . | the correct syntax for the merge command is: ‘git merge ts_origin/master’ What this does is that if the current branch is origin/develop it will merge the files of current branch i.e origin/develop with ts_origin/master . | Note that even if ts_origin/master is in ts_github account and origin/master is in sushanthj github account, it will still merge as long as remotes exist for both these accounts. If remotes don’t exist, you can always add as shown up above . | . Concepts for working with two repos or two repos on two different github accounts: . Basically locally you will have ‘master’ branch if you do ‘git branch’ This master can track two upstream branches using two different remotes One remote is added automatically when you clone the repo The next remote will have to be added manually to your other git account or other repo . Then to push the same commit to both branches first do ‘git push’ and see which repo it pushes to (say it pushes to origin/master Then do ‘git push –set-upstream ts_origin/develop’ to push to your second repo However, do note that your local branch always tracks to the latest branch you pushed to i.e if you do a git pull, it will pull from the latest branch to which you pushed in this case it will pull from ts_origin/develop . Saving a patch file . If you have changes made which you want to save locally and not push to remote, you can save a patch file . git diff &gt; new_changes.patch . Now to apply this patch onto any branch, do: . git apply new_changes.patch . Saving changes by stashing . Instead of saving a specific file for changes (such as a patch file), you could also stash your changes locally . git stash . The above command will stash all tracked changes. You could also stash only committed changes. Refer: stashing . To then apply the stashed changes (one time use only as pop will remove from stash) . git stash pop . To apply without popping do: . git stash apply . To remove any particular item in stash: . git stash drop . To view all entries in stash and then apply specific one do: . git stash list git stash apply n . n = stash item number . ",
    "url": "http://localhost:4000/git_concepts#points-to-note",
    "relUrl": "/git_concepts#points-to-note"
  },"7": {
    "doc": "Git Concepts",
    "title": "Nomenclature of Git Branches",
    "content": "Usually you will need to follow the branch nomenclature which your company uses. However, it’s good to know the common industry practice: . | The branch which will be running on all production would be main (or master) | The main branch in which developers will push finalized code would be develop (sometimes develop can itself be production branch) | For any new addition that an employee wants to propose to the codebase, a branch called ‘feature/sj’ will be created (sj is my initials). This feature branch will be merged with the develop branch only after some extensive code review and testing | . Maintaining better commit history . It is also common practice to tag certain commits of the develop branch. (see tagging) . This tagging allows for easy identification as to what major release correspondeds to that commit . ",
    "url": "http://localhost:4000/git_concepts#nomenclature-of-git-branches",
    "relUrl": "/git_concepts#nomenclature-of-git-branches"
  },"8": {
    "doc": "Git Concepts",
    "title": "Git Concepts",
    "content": ". | Before you Begin | Basics of generating new content in local and pushing to github . | Process for adding to a github page | If you want to track a different branch | Working with remotes | Handling merge conflicts | Points to Note . | Concepts for working with two repos or two repos on two different github accounts: | Saving a patch file | Saving changes by stashing | . | Nomenclature of Git Branches . | Maintaining better commit history | . | . | . ",
    "url": "http://localhost:4000/git_concepts",
    "relUrl": "/git_concepts"
  },"9": {
    "doc": "Intro",
    "title": "To better your experience of writing in code",
    "content": "Download the following extensions in vscode: . | Markdown All in one | code runner (see youtube video on how to setup vscode for C++) | . ",
    "url": "http://localhost:4000/intro/#to-better-your-experience-of-writing-in-code",
    "relUrl": "/intro/#to-better-your-experience-of-writing-in-code"
  },"10": {
    "doc": "Intro",
    "title": "Shortcuts in general pour toi",
    "content": ". | Once Markdown all in one is installed, you can do ctrl+shift+v to see preview of markdown immediately | To run any C++ file it’s just ctrl+shift+n | If you want to bold any specific text in markdown just select the text by holding down ctrl+shift and using arrow keys to select the required text. Then once text is selected just do ctrl+b to bolden and ctrl+i to italicize . | click on tab after using - for normal bullet pointing to get sub-points | . | To get numbered list continuously, in-between two headings 1. and 2. all content should be indented with 4 spaces in the markdown script . | To shift between windows in ubuntu, just do windows_key+shift+right/left_arrow | To minimize or unmaximize any window in hold down alt and press space, then choose to minimize | To then maximize or move window to right half/left half of screen, windows_key+shift+right/left_arrow | . ",
    "url": "http://localhost:4000/intro/#shortcuts-in-general-pour-toi",
    "relUrl": "/intro/#shortcuts-in-general-pour-toi"
  },"11": {
    "doc": "Intro",
    "title": "Intro",
    "content": "For Jekyll reference see just_the_docs . The following pages are built in order to understand Computer Vision and Machine Learning . To deploy on heroku follow the steps in the link below (and use the gem files, rake files and proc files in this repo for reference) . The following files will need to be copied from this repo: . | config.ru | Rakefile | Procfile | static.json | config.yaml (modify this file as per requirement) | Gemfile | . And only if necessary: . | Gemfile.lock | remove _sites from .gitignore | . Run bundle once to intialize Run bundle exec jekyll serve Go to the specified webpage by the above command . After copying these files (or their necessary contents), install heroku cli and do heroku login: . curl https://cli-assets.heroku.com/install.sh | sh heroku login . Then directly start with heroku create as per the below link and the other steps necessary (git push heroku master) . Deploy jekyll on heroku . Finally, go to heroku page -&gt; settings -&gt; change the name of the app and find the url . ",
    "url": "http://localhost:4000/intro/",
    "relUrl": "/intro/"
  },"12": {
    "doc": "Numpy",
    "title": "Before you Begin",
    "content": "Official Documentation . Numpy and Scipy are two resources to compute a variety of functions on matrices. Scipy is built on top of numpy and has a larger codebase of modules which we can utilize . ",
    "url": "http://localhost:4000/numpy/#before-you-begin",
    "relUrl": "/numpy/#before-you-begin"
  },"13": {
    "doc": "Numpy",
    "title": "Images and Arrays",
    "content": " ",
    "url": "http://localhost:4000/numpy/#images-and-arrays",
    "relUrl": "/numpy/#images-and-arrays"
  },"14": {
    "doc": "Numpy",
    "title": "Image Operations",
    "content": "Importing Images . In the below code we input an image and convert it into an array. Shape of an array is just it’s size . im = array(Image.open('empire.jpg')) print im.shape, im.dtype . The output would look lik this: . (800, 569, 3) uint8 (RGB image) . Converting image to Greyscale . This uses an extra library called Python Pillow . from PIL import Image, ImageOps im = array(Image.open('empire.jpg').convert('L'),'f') print im.shape, im.dtype . Plotting an image . img = np.array(Image.open('House2.jpg')) plt.figure(figsize=(8,8)) plt.imshow(img) plt.show . ",
    "url": "http://localhost:4000/numpy/#image-operations",
    "relUrl": "/numpy/#image-operations"
  },"15": {
    "doc": "Numpy",
    "title": "Array Functions and Operations",
    "content": " ",
    "url": "http://localhost:4000/numpy/#array-functions-and-operations",
    "relUrl": "/numpy/#array-functions-and-operations"
  },"16": {
    "doc": "Numpy",
    "title": "Array Nomenclature",
    "content": ". It’s important to realise that we only care about shapes of a matrix and our computation revolves around the shape tuple (1,2,3) irrespective of which is row or column. Develop a generalized version of matrix definitions!! . An image can have shape as (640,540,3). Here we need to think in the way that there are 640 rows and 540 columns and 3 RGB channels. Therefore, rows, columns, pages don’t matter much. Just think in terms of shapes. sum() function in 1D . import numpy as np arr = [20, 2, .2, 10, 4] print(\"\\nSum of arr : \", np.sum(arr)) print(\"Sum of arr(uint8) : \", np.sum(arr, dtype = np.uint8)) print(\"Sum of arr(float32) : \", np.sum(arr, dtype = np.float32)) . Output: . Sum of arr : 36.2 Sum of arr(uint8) : 36 Sum of arr(float32) : 36.2 . In 1D it just computes the sum of all elements in the array. It can also do type conversion on the go. We can extend this same logic to 2D, there too it calculates the sum of all matrix elements . sum() in 2D along axes . Axis along which we want to calculate the sum value. Otherwise, it will consider arr to be flattened(works on all the axis). axis = 0 means it calculates sum of all elements in ith column and (i=1)th column.. axis = 1 means it calculates sum of all elements in (j)th column and (j+1)th column.. arr = [[14, 17, 12, 33, 44], [15, 6, 27, 8, 19], [23, 2, 54, 1, 4,]] print(\"\\nSum of arr : \", np.sum(arr)) print(\"Sum of arr(axis = 0) : \", np.sum(arr, axis = 0)) print(\"Sum of arr(axis = 1) : \", np.sum(arr, axis = 1)) . Output would be: . Sum of arr : 279 Sum of arr(axis = 0) : [52 25 93 42 67] Sum of arr(axis = 1) : [120 75 84] . But notice how the vector of axis = 1 has been transposed to show as a row vector . We change that behaviour by adding a second argument to the sum() function: . print(\"\\nSum of arr (keepdimension is True): \\n\", np.sum(arr, axis = 1, keepdims = True)) . Output . Sum of arr (keepdimension is True): [[120] [ 75] [ 84]] . Looping over an Image and Grayscale . We can loop over individual elements in a matrix after knowing the shape of the matrix . The shape of the image is given as a tuple eg. (640, 540, 3) . | the last item of that tuple is the RGB spectrum (3 dimensions per pixel) | the first two items in the tuple is the actual size of the image | . for i in range(img.shape[1]): print() . In the above code we are looping over the rows. Therefore we are looping 640 times. Method 1 : Consider this method of converting image into greyscale: . import numpy as np import matplotlib.pyplot as plt from PIL import Image, ImageOps img = np.array(Image.open('B1.jpg')) print(img.shape) for i in range(img.shape[0]): for j in range(img.shape[1]): grey_value = 0 for k in range(img.shape[2]): grey_value += img[i,j,k] img[i,j,0] = int(grey_value/3) img2 = img[:,:,1] plt.figure(figsize=(8,8)) plt.imshow(img2) plt.show() . Also note how we removed the third (extra) dimensions using: . img2 = img[:,:,1] . This method uses averaging to find grayscale. However a slightly modified version is usually preferred: . Method 2: Accounting for Luminance Perception . import numpy as np import matplotlib.pyplot as plt from PIL import Image, ImageOps weight = [0.2989, 0.5870, 0.1140] img = np.array(Image.open('B1.jpg')) print(img.shape) for i in range(img.shape[0]): for j in range(img.shape[1]): grey_value = 0 for k in range(len(weight)): grey_value += (img[i,j,k]*weight[k]) img[i,j,0] = int(grey_value) img2 = img[:,:,1] plt.figure(figsize=(8,8)) plt.imshow(img2, cmap=plt.get_cmap(\"gray\")) plt.show() . Method 3: Simpler code using numpy.mean . from PIL import Image import numpy as np import matplotlib.pyplot as plt color_img = np.array(Image.open('B1.jpg')) / 255 img = np.mean(color_img, axis=2) plt.figure(figsize=(8,8)) plt.imshow(img, cmap=plt.get_cmap(\"gray\")) plt.show() . ",
    "url": "http://localhost:4000/numpy/#array-nomenclature",
    "relUrl": "/numpy/#array-nomenclature"
  },"17": {
    "doc": "Numpy",
    "title": "Built-in Numpy functions",
    "content": " ",
    "url": "http://localhost:4000/numpy/#built-in-numpy-functions",
    "relUrl": "/numpy/#built-in-numpy-functions"
  },"18": {
    "doc": "Numpy",
    "title": "Difference between dot, matmul, and *",
    "content": ". ",
    "url": "http://localhost:4000/numpy/#difference-between-dot-matmul-and-",
    "relUrl": "/numpy/#difference-between-dot-matmul-and-"
  },"19": {
    "doc": "Numpy",
    "title": "Plotting a pixel-wise histogram",
    "content": "img = np.array(Image.open('emma_stone.jpg')) img_flat = img.flatten() plt.hist(img_flat, bins=200, range=[0, 256]) plt.title(\"Number of pixels in each intensity value\") plt.xlabel(\"Intensity\") plt.ylabel(\"Number of pixels\") plt.show() . ",
    "url": "http://localhost:4000/numpy/#plotting-a-pixel-wise-histogram",
    "relUrl": "/numpy/#plotting-a-pixel-wise-histogram"
  },"20": {
    "doc": "Numpy",
    "title": "Reshaping Arrays",
    "content": "x = np.arange(4).reshape((2,2)) x &gt;&gt;array([[0, 1], [2, 3]]) . ",
    "url": "http://localhost:4000/numpy/#reshaping-arrays",
    "relUrl": "/numpy/#reshaping-arrays"
  },"21": {
    "doc": "Numpy",
    "title": "Transpose of a matrix",
    "content": "Simple transpose is done using the matrix.transpose() or matrix.T method (both are same). One of them is showed below: . # (refer matrix x in above example) np.transpose(x) array([[0, 2], [1, 3]]) . However the transpose function takes more arguments and this is important for 3D matrices. Note that if a 3D matrix say ‘A’ has shape (1,2,3), the result of transpose without specifying any extra argument will be (3,2,1) . x = np.ones((1, 2, 3)) np.transpose(x, (1, 0, 2)).shape &gt;&gt;(2, 1, 3) . Note. While declaring array as in np.ones(1,2,3). This can be interpreted in two ways: . | If we are printing the array in terminal we will read it as: there are 1 pages, 2 rows and 3 columns | If it’s an image, the shape will be 1 row, 2 coulmns and 3 will be for 3 RGB channels | . It’s important to realise that we only care about shapes of a matrix and our computation revolves around the shape tuple (1,2,3) irrespective of which is row or column. Develop a generalized version of matrix definitions!! . However, we will access each row/column starting from 0 as x[0,0,0] or x[1,1,1]. The second argument stands for the axes parameter. Axes are numbered as 0,1,2 . i.e. default configuration of axes is (0,1,2) for a 3D array and (0,1) for a 2D array . Therefore if we specify &lt;np.transpose(x,(1,0,2))&gt; we’re saying that we want the first two shapes interchanged. Remember that first two shapes are pages and rows. Hence, those two will interchange. ",
    "url": "http://localhost:4000/numpy/#transpose-of-a-matrix",
    "relUrl": "/numpy/#transpose-of-a-matrix"
  },"22": {
    "doc": "Numpy",
    "title": "Padding of Matrices",
    "content": "Padding is used to ensure overall image size does not reduce while run filters/convulutions on it . import numpy as np x = np.ones(3) y = np.pad(x, pad_width=1) y # Expected result # array([0., 1., 1., 1., 0.]) . ",
    "url": "http://localhost:4000/numpy/#padding-of-matrices",
    "relUrl": "/numpy/#padding-of-matrices"
  },"23": {
    "doc": "Numpy",
    "title": "newaxis method",
    "content": "ref: newaxis . This method can be used to convert a row vector to a column vector and at the same time add another dimension as shown below: . a = np.array([0,1,2]) print(a.shape) . Output: (3,) . Now lets do the newaxis modification: . c = (a[:, np.newaxis]) print(c) print(c.shape) . Output: . [[0] [1] [2]] (3, 1) . Therefore we can see that the vector has been rotated and another dimension has been added to the shape tuple . ",
    "url": "http://localhost:4000/numpy/#newaxis-method",
    "relUrl": "/numpy/#newaxis-method"
  },"24": {
    "doc": "Numpy",
    "title": "einsum",
    "content": "Refer to this document: einsum . ",
    "url": "http://localhost:4000/numpy/#einsum",
    "relUrl": "/numpy/#einsum"
  },"25": {
    "doc": "Numpy",
    "title": "Stacking rows using vstack",
    "content": "We can use this function to stack rows onto an exiting numpy array. in_arr1 = geek.array([ 1, 2, 3] ) in_arr2 = geek.array([ 4, 5, 6] ) # Stacking the two arrays vertically out_arr = geek.vstack((in_arr1, in_arr2)) print (out_arr) . Practically we can use this in a specific case. If we don’t know the number of rows we will be adding to a numpy array: . | We will define the array as a 0 row array | We then add rows as we progress using the vstack function | . word_array = np.array([]).reshape(0, maxlength) for message in messages: word_count = np.zeros((1, word_num)) for word in word_list: if word == \"yes\": word_count[0, word_dictionary[word]] += 1 word_array = np.vstack([word_array, word_count]) return word_array . ",
    "url": "http://localhost:4000/numpy/#stacking-rows-using-vstack",
    "relUrl": "/numpy/#stacking-rows-using-vstack"
  },"26": {
    "doc": "Numpy",
    "title": "Saving a numpy matrix in a text file",
    "content": "np.savetxt('./output/p06_sample_train_matrix', train_matrix[:100,:]) . ",
    "url": "http://localhost:4000/numpy/#saving-a-numpy-matrix-in-a-text-file",
    "relUrl": "/numpy/#saving-a-numpy-matrix-in-a-text-file"
  },"27": {
    "doc": "Numpy",
    "title": "Numpy",
    "content": ". | Before you Begin | Images and Arrays . | Image Operations . | Importing Images | Converting image to Greyscale | Plotting an image | . | Array Functions and Operations | Array Nomenclature . | sum() function in 1D | sum() in 2D along axes | Looping over an Image and Grayscale . | Method 1 : Consider this method of converting image into greyscale: | Method 2: Accounting for Luminance Perception | Method 3: Simpler code using numpy.mean | . | . | . | Built-in Numpy functions . | Difference between dot, matmul, and * | Plotting a pixel-wise histogram | Reshaping Arrays | Transpose of a matrix | Padding of Matrices | newaxis method | einsum | Stacking rows using vstack | Saving a numpy matrix in a text file | . | . ",
    "url": "http://localhost:4000/numpy/",
    "relUrl": "/numpy/"
  },"28": {
    "doc": "Planar Homography",
    "title": "Before you Begin",
    "content": "Reference Book 1 Reference Book 2 . ",
    "url": "http://localhost:4000/planar_homography/#before-you-begin",
    "relUrl": "/planar_homography/#before-you-begin"
  },"29": {
    "doc": "Planar Homography",
    "title": "PDFs",
    "content": "Assignment Writeup . My Answers . ",
    "url": "http://localhost:4000/planar_homography/#pdfs",
    "relUrl": "/planar_homography/#pdfs"
  },"30": {
    "doc": "Planar Homography",
    "title": "Some Basics on Camera Projection",
    "content": " ",
    "url": "http://localhost:4000/planar_homography/#some-basics-on-camera-projection",
    "relUrl": "/planar_homography/#some-basics-on-camera-projection"
  },"31": {
    "doc": "Planar Homography",
    "title": "Projection of 3D to 2D image plane",
    "content": "To understand how a camera views the 3D world, first we look at the projection of 3D points onto an image plane. We use basic high school physics and some similar triangle properties to derive the following formula: . Notice that the minus sign is bit irritating to work with. (Also we don’t see inverted images as the formula suggests. This is becauase our brain does the inversion in real time) . Therefore, let’s start with the below version of the formula by ignoring this inversion effect . Now, the above equation can be written in matrix form, but we’ll form one artifact in this conversion i.e. lambda . It’s clear that we can find this lambda as shown. However, why do we even need this? Ans. We want to represent the coordinates in homogenous coordinates . ",
    "url": "http://localhost:4000/planar_homography/#projection-of-3d-to-2d-image-plane",
    "relUrl": "/planar_homography/#projection-of-3d-to-2d-image-plane"
  },"32": {
    "doc": "Planar Homography",
    "title": "Camera Matrices",
    "content": "Generic Representation . Now, let’s add another constraint on this equation. Suppose we rotate our 3D point in space or we rotate the camera itself by a certain angle. In the world of robotics we call such transforms as a rotation matrix. Reference: Rotation Matrices . To get a good grasp of rotation matrices, I highly recommend some linear algebra brush-up using 3B1B (3 Blue 1 Brown). Specifically (watch 8th minute of this video) The rotation shown in the above video in the 8th minute is a rotation matrix in 2D. Now, adding a 3D translation (just 3 numbers which add to the x,y,z component of a 3D vector) along with a 3D rotation we get the basic projection equation . Where the two matrices are called the camera intrinsics (captures focal lengths) and the camera extrinsics (capturing rotation and translation) . This rotation (r-matrix) can also be visualized as fixing a world coordinate frame onto some plane in the 3D world (think of a it as a flat table top) and then thinking how our camera is rotated w.r.t that frame: . Now, most cameras also distort images due to lens optics or other properties inherent in building the camera itself. These are captured as shown below: . Now, adding these intrinsic and extrinsic factors, we get: . Alternate notation of camera matrices . ",
    "url": "http://localhost:4000/planar_homography/#camera-matrices",
    "relUrl": "/planar_homography/#camera-matrices"
  },"33": {
    "doc": "Planar Homography",
    "title": "The Homography Situation",
    "content": " ",
    "url": "http://localhost:4000/planar_homography/#the-homography-situation",
    "relUrl": "/planar_homography/#the-homography-situation"
  },"34": {
    "doc": "Planar Homography",
    "title": "Single View",
    "content": "Now, if we focus on only planes (table top and human holding camera situation): We can make certain simplifying assumptions. This is primarily that the 3D point we’re looking at has constant depth in it’s immediate neighbourhood. Using this we simplify our equations to: . This 3x3 m-matrix now represents the mapping of 3D points on a plane to 2D point in an image . ",
    "url": "http://localhost:4000/planar_homography/#single-view",
    "relUrl": "/planar_homography/#single-view"
  },"35": {
    "doc": "Planar Homography",
    "title": "Multiple Views",
    "content": "Now, by simple extension of the above logic we can derive the following: . | We have just 1 plane in the 3D world | We have two cameras looking at this plane | Each camera has it’s own 3x3 m-matrix which maps 3D plane points onto 2D image frame | Therefore if two cameras can see the same 3D point, we can find a mapping between the two cameras | This mapping between the two cameras is given by a new 3x3 matrix called the homography matrix | . ",
    "url": "http://localhost:4000/planar_homography/#multiple-views",
    "relUrl": "/planar_homography/#multiple-views"
  },"36": {
    "doc": "Planar Homography",
    "title": "Limitations of Planar Homography",
    "content": ". | When the scene is very far away from the camera, all objects can be said to have the same depth. This is because the relative depth distances between foreground and background will be negligible in comparison to the average scene depth. Therefore, in such cases all objects in scene can be said to lie on a plane and as proved above, can be captured by two cameras related by a homography matrix. | For nearby scenes where the variation in scene depth is more apparent, a homography mapping works well only under pure rotation. | . ",
    "url": "http://localhost:4000/planar_homography/#limitations-of-planar-homography",
    "relUrl": "/planar_homography/#limitations-of-planar-homography"
  },"37": {
    "doc": "Planar Homography",
    "title": "Implementation of Homography Estimation",
    "content": " ",
    "url": "http://localhost:4000/planar_homography/#implementation-of-homography-estimation",
    "relUrl": "/planar_homography/#implementation-of-homography-estimation"
  },"38": {
    "doc": "Planar Homography",
    "title": "The Pipeline",
    "content": "The main application of homography transforms is to find how some reference template has been warped due to movement of the camera. This is seem below as: . The applications of this are: . | image stitching (think of two images from two views as a warped version of view 0) | augmented reality (projecting some images onto a fixed/known plane in the real world) | . To perform any of the above cool applications, we first need to compute the homography between any two views. The pipeline for this would be: . | Have one reference view and another view with the camera having moved slightly | Detect some keypoints (interest points like corners/edges) in each image | Describe these keypoints in some way (maybe capture the histogram of pixel intensities in a small patch around the keypoint) | Match the keypoints in one image to another using the keypoint descriptions | Use the spatial information of these matched keypoints (i.e. the x,y coordinates of each of these keypoints) to find the Homography matrix | **Apply the homography matrix as a transformation on one of the images **to warp and match the images | . Let’s go deeper into the each of the above steps: . Keypoint Detection . | There are several methods to find keypoints in an image. Usually these keypoints are corners since other features like edges may warp or curve due to distortion and may be difficult to trace. | The common methods are Harris Corner Detector, polygon fitting, FAST detectors etc. | Here we use the FAST detector | . Keypoint Descriptors . Common descriptors include BRIEF, ORB, SIFT etc. Here we’ve used the BRIEF descriptor . The BRIEF descriptor works by creating a binary feature vector of a patch from random (x,y) point pairs. This randomness in generating point pairs ensures changes in pixel intensities are captuerd in multiple directions thereby being sensitive to a large variety of edges or corners. The BRIEF descriptor also compares these binary strings using hamming distance further reduces compute time. Due to this computational cost of calculating histograms for each filter bank it would not make sense to use filterbanks instead of BRIEF. Further, just filterbanks cannot encode patch descriptions, i.e. without any form of histograms (like SIFT), the filterbanks themselves cannot be used instead of BRIEF. Reference: BRIEF Descriptor . The implementation of keypoint detection, description and matching are shown below: . import numpy as np import cv2 import skimage.color from helper import briefMatch from helper import computeBrief from helper import corner_detection # Q2.1.4 def matchPics(I1, I2, opts): \"\"\" Match features across images Input ----- I1, I2: Source images opts: Command line args Returns ------- matches: List of indices of matched features across I1, I2 [p x 2] locs1, locs2: Pixel coordinates of matches [N x 2] \"\"\" print(\"computing image matches\") ratio = opts.ratio #'ratio for BRIEF feature descriptor' sigma = opts.sigma #'threshold for corner detection using FAST feature detector' # Convert Images to GrayScale I1 = skimage.color.rgb2gray(I1) I2 = skimage.color.rgb2gray(I2) # Detect Features in Both Images # locs1 is just the detected corners of I1 locs1 = corner_detection(I1, sigma) locs2 = corner_detection(I2, sigma) # Obtain descriptors for the computed feature locations # We use the breif descriptor to give the patch descriptions (patch of pixel width = 9) # for the corners(keypoints) which we obtained from corner_description # desc is the binary string (len(string)=256 and 256bits) # which serves as the feature descriptor desc1, locs1 = computeBrief(I1, locs1) desc2, locs2 = computeBrief(I2, locs2) # Match features using the descriptors matches = briefMatch(desc1, desc2, ratio) print(f'Computed {matches.shape[0]} matches successfully') return matches, locs1, locs2 def briefMatch(desc1,desc2,ratio): matches = skimage.feature.match_descriptors(desc1,desc2,'hamming',cross_check=True,max_ratio=ratio) return matches . Calculating the Homography Matrix . Let’s say we have two images: image1 and image2 . To Derive the A matrix we undergo the following steps: . Where h is found by taking the SVD of A and choosing the eigen vector (with least eigen value) which forms the null space of A. (Bonus) Rejecting outliers during our homography calculation . Implementation of above steps . import numpy as np import cv2 import skimage.io import skimage.color from planarH import * from opts import get_opts from matchPics import matchPics from helper import briefMatch def warpImage(opts): \"\"\" Warp template image based on homography transform Args: opts: user inputs \"\"\" image1 = cv2.imread('../data/cv_cover.jpg') image2 = cv2.imread('../data/cv_desk.png') template_img = cv2.imread('../data/hp_cover.jpg') # make sure harry_potter image is same size as CV book x,y,z = image1.shape template_img = cv2.resize(template_img, (y,x)) matches, locs1, locs2 = matchPics(image1, image2, opts) # invert the columns of locs1 and locs2 locs1[:, [1, 0]] = locs1[:, [0, 1]] locs2[:, [1, 0]] = locs2[:, [0, 1]] matched_points = create_matched_points(matches, locs1, locs2) h, inlier = computeH_ransac(matched_points[:,0:2], matched_points[:,2:], opts) print(\"homography matrix is \\n\", h) # compositeH(h, source, destination) composite_img = compositeH(h, template_img, image2) # Display images cv2.imshow(\"Composite Image :)\", composite_img) cv2.waitKey() if __name__ == \"__main__\": opts = get_opts() warpImage(opts) . RANSAC and Construction of Composite Image . from copy import deepcopy from dataclasses import replace from platform import python_branch import numpy as np import cv2 import matplotlib.pyplot as plt import skimage.color import math import random from scipy import ndimage from scipy.spatial import distance from matchPics import matchPics from helper import plotMatches from opts import get_opts from tqdm import tqdm def computeH(x1, x2): \"\"\" Computes the homography based on matching points in both images Args: x1: keypoints in image 1 x2: keypoints in image 2 Returns: H2to1: the homography matrix \"\"\" # Define a dummy H matrix A_build = [] # Define the A matrix for (Ah = 0) (A matrix size = N*2 x 9) for i in range(x1.shape[0]): row_1 = np.array([ x2[i,0], x2[i,1], 1, 0, 0, 0, -x1[i,0]*x2[i,0], -x1[i,0]*x2[i,1], -x1[i,0] ]) row_2 = np.array([ 0, 0, 0, x2[i,0], x2[i,1], 1, -x1[i,1]*x2[i,0], -x1[i,1]*x2[i,1], -x1[i,1] ]) A_build.append(row_1) A_build.append(row_2) A = np.stack(A_build, axis=0) # Do the least squares minimization to get the homography matrix # this is done as eigenvector coresponding to smallest eigen value of A`A = H matrix u, s, v = np.linalg.svd(np.matmul(A.T, A)) # here the linalg.svd gives v_transpose # but we need just V therefore we again transpose H2to1 = np.reshape(v.T[:,-1], (3,3)) return H2to1 def computeH_norm(x1, x2): #Q2.2.2 \"\"\" Compute the normalized coordinates and also the homography matrix using computeH Args: x1 (Mx2): the matched locations of corners in img1 x2 (Mx2): the matched locations of corners in img2 Returns: H2to1: Hmography matrix after denormalization \"\"\" # Q2.2.2 # Compute the centroid of the points centroid_img_1 = np.sum(x1, axis=0)/x1.shape[0] centroid_img_2 = np.sum(x2, axis=0)/x2.shape[0] # print(f'centroid of img1 is {centroid_img_1} \\n centroid of img2 is {centroid_img_2}') # Shift the origin of the points to the centroid # let origin for img1 be centroid_img_1 and similarly for img2 #? Now translate every point such that centroid is at [0,0] moved_x1 = x1 - centroid_img_1 moved_x2 = x2 - centroid_img_2 current_max_dist_img1 = np.max(np.linalg.norm(moved_x1),axis=0) current_max_dist_img2 = np.max(np.linalg.norm(moved_x2),axis=0) # moved and scaled image 1 points scale1 = np.sqrt(2) / (current_max_dist_img1) scale2 = np.sqrt(2) / (current_max_dist_img2) moved_scaled_x1 = moved_x1 * scale1 moved_scaled_x2 = moved_x2 * scale2 # Similarity transform 1 #? We construct the transformation matrix to be 3x3 as it has to be same shape of Homography t1 = np.diag([scale1, scale1, 1]) t1[0:2,2] = -scale1*centroid_img_1 # Similarity transform 2 t2 = np.diag([scale2, scale2, 1]) t2[0:2,2] = -scale2*centroid_img_2 # Compute homography H = computeH(moved_scaled_x1, moved_scaled_x2) # Denormalization H2to1 = np.matmul(np.linalg.inv(t1), np.matmul(H, t2)) return H2to1 def create_matched_points(matches, locs1, locs2): \"\"\" Match the corners in img1 and img2 according to the BRIEF matched points Args: matches (Mx2): Vector containing the index of locs1 and locs2 which matches locs1 (Nx2): Vector containing corner positions for img1 locs2 (Nx2): Vector containing corner positions for img2 Returns: _type_: _description_ \"\"\" matched_pts = [] for i in range(matches.shape[0]): matched_pts.append(np.array([locs1[matches[i,0],0], locs1[matches[i,0],1], locs2[matches[i,1],0], locs2[matches[i,1],1]])) # remove the first dummy value and return matched_points = np.stack(matched_pts, axis=0) return matched_points def computeH_ransac(locs1, locs2, opts): \"\"\" Every iteration we init a Homography matrix using 4 corresponding points and calculate number of inliers. Finally use the Homography matrix which had max number of inliers (and these inliers as well) to find the final Homography matrix Args: locs1: location of matched points in image1 locs2: location of matched points in image2 opts: user inputs used for distance tolerance in ransac Returns: bestH2to1 : The homography matrix with max number of inliers final_inliers : Final list of inliers considered for homography \"\"\" #Q2.2.3 #Compute the best fitting homography given a list of matching points max_iters = opts.max_iters # the number of iterations to run RANSAC for inlier_tol = opts.inlier_tol # the tolerance value for considering a point to be an inlier # define size of both locs1 and locs2 num_rows = locs1.shape[0] # define a container for keeping track of inlier counts final_inlier_count = 0 final_distance_error = 10000 #? Create a boolean vector of length N where 1 = inlier and 0 = outlier print(\"Computing RANSAC\") for i in range(max_iters): test_locs1 = deepcopy(locs1) test_locs2 = deepcopy(locs2) # chose a random sample of 4 points to find H rand_index = [] rand_index = random.sample(range(int(locs1.shape[0])),k=4) rand_points_1 = [] rand_points_2 = [] for j in rand_index: rand_points_1.append(locs1[j,:]) rand_points_2.append(locs2[j,:]) test_locs1 = np.delete(test_locs1, rand_index, axis=0) test_locs2 = np.delete(test_locs2, rand_index, axis=0) correspondence_points_1 = np.vstack(rand_points_1) correspondence_points_2 = np.vstack(rand_points_2) ref_H = computeH_norm(correspondence_points_1, correspondence_points_2) inliers, inlier_count, distance_error, error_state = compute_inliers(ref_H, test_locs1, test_locs2, inlier_tol) if error_state == 1: continue if (inlier_count &gt; final_inlier_count) and (distance_error &lt; final_distance_error): final_inlier_count = inlier_count final_inliers = inliers final_corresp_points_1 = correspondence_points_1 final_corresp_points_2 = correspondence_points_2 final_distance_error = distance_error final_test_locs1 = test_locs1 final_test_locs2 = test_locs2 if final_distance_error != 10000: # print(\"original point count is\", locs1.shape[0]) # print(\"final inlier count is\", final_inlier_count) # print(\"final inlier's cumulative distance error is\", final_distance_error) delete_indexes = np.where(final_inliers==0) final_locs_1 = np.delete(final_test_locs1, delete_indexes, axis=0) final_locs_2 = np.delete(final_test_locs2, delete_indexes, axis=0) final_locs_1 = np.vstack((final_locs_1, final_corresp_points_1)) final_locs_2 = np.vstack((final_locs_2, final_corresp_points_2)) bestH2to1 = computeH_norm(final_locs_1, final_locs_2) return bestH2to1, final_inliers else: bestH2to1 = computeH_norm(correspondence_points_1, correspondence_points_2) return bestH2to1, 0 def compute_inliers(h, x1, x2, tol): \"\"\" Compute the number of inliers for a given homography matrix Args: h: Homography matrix x1 : matched points in image 1 x2 : matched points in image 2 tol: tolerance value to check for inliers Returns: inliers : indexes of x1 or x2 which are inliers inlier_count : number of total inliers dist_error_sum : Cumulative sum of errors in reprojection error calc flag : flag to indicate if H was invertible or not \"\"\" # take H inv to map points in x1 to x2 try: H = np.linalg.inv(h) except: return [1,1,1], 1, 1, 1 x2_extd = np.append(x2, np.ones((x2.shape[0],1)), axis=1) x1_extd = (np.append(x1, np.ones((x1.shape[0],1)), axis=1)) x2_est = np.zeros((x2_extd.shape), dtype=x2_extd.dtype) for i in range(x1.shape[0]): x2_est[i,:] = H @ x1_extd[i,:] x2_est = x2_est/np.expand_dims(x2_est[:,2], axis=1) dist_error = np.linalg.norm((x2_extd-x2_est),axis=1) # print(\"dist error is\", dist_error) inliers = np.where((dist_error &lt; tol), 1, 0) inlier_count = np.count_nonzero(inliers == 1) return inliers, inlier_count, np.sum(dist_error), 0 def compositeH(H2to1, template, img): \"\"\" Create a composite image after warping the template image on top of the image using the homography Args: H2to1 : Existing(already found) homography matrix template: Harry Potter (template image) img: Base image onto which we overlay Harry Potter image Returns: composite_img: Base image with overlayed Harry Potter cover \"\"\" output_shape = (img.shape[1],img.shape[0]) # destination_img = img # source_img = template h = np.linalg.inv(H2to1) # Create mask of same size as template mask = np.ones((template.shape[0], template.shape[1]))*255 mask = np.stack((mask, mask, mask), axis=2) # Warp mask by appropriate homography warped_mask = cv2.warpPerspective(mask, h, output_shape) # Warp template by appropriate homography warped_template = cv2.warpPerspective(template, h, output_shape) # Use mask to combine the warped template and the image composite_img = np.where(warped_mask, warped_template, img) return composite_img def panorama_composite(H2to1, template, img): \"\"\" Stitch two images together to form a panorama Args: H2to1: Homography Matrix template: The pano_right image img: The pano_left image Returns: composite_img: Stitched image (panorama) \"\"\" output_shape = (img.shape[1]+240,img.shape[0]+240) # destination_img = img # source_img = template h = H2to1 img_padded = np.zeros((img.shape[0]+240,img.shape[1]+240,3), dtype=img.dtype) img_padded[0:img.shape[0], 0:img.shape[1], :] = img[:,:,:] # Create mask of same size as template mask = np.ones((template.shape[0], template.shape[1]))*255 mask = np.stack((mask, mask, mask), axis=2) # Warp mask by appropriate homography warped_mask = cv2.warpPerspective(mask, h, output_shape) # Warp template by appropriate homography cv2.imshow(\"template image\", template) cv2.waitKey() cv2.imshow(\"destination image\", img) cv2.waitKey() warped_template = cv2.warpPerspective(template, h, output_shape) cv2.imshow(\"warped template\", warped_template) cv2.waitKey() # Use mask to combine the warped template and the image composite_img = np.where(warped_mask, warped_template, img_padded) return composite_img . ",
    "url": "http://localhost:4000/planar_homography/#the-pipeline",
    "relUrl": "/planar_homography/#the-pipeline"
  },"39": {
    "doc": "Planar Homography",
    "title": "Applying Homography Estimation in the Real World",
    "content": " ",
    "url": "http://localhost:4000/planar_homography/#applying-homography-estimation-in-the-real-world",
    "relUrl": "/planar_homography/#applying-homography-estimation-in-the-real-world"
  },"40": {
    "doc": "Planar Homography",
    "title": "Basic cool applications",
    "content": "If we know how a template matches to a warped image, such as: . We can then use this homography matrix to map any plane (here a different book cover) onto our destination image . ",
    "url": "http://localhost:4000/planar_homography/#basic-cool-applications",
    "relUrl": "/planar_homography/#basic-cool-applications"
  },"41": {
    "doc": "Planar Homography",
    "title": "AR Video",
    "content": "Here we use the same book-cover homography mapping but onto a sequence of frames of a video . ",
    "url": "http://localhost:4000/planar_homography/#ar-video",
    "relUrl": "/planar_homography/#ar-video"
  },"42": {
    "doc": "Planar Homography",
    "title": "Panorama Stitching",
    "content": "During my visit to Ohiopyle, I took few pictures of the river. Let’s back to the fact that homography works well for far away scenes, where the large distance from camera to landscape makes the relative distances of objects in the landscape negligible. In such cases even small translations of the camera have a small effect on the landscape itself. However, since the scene at ohiopyle was not too far away, any translation would yield a bad homography matrix and cause shoddy stitching. Therefore I tried to mitigate this by only rotating about my hip (to ensure no translational movement) while taking the two views. The results of the stitching are shown below: . ",
    "url": "http://localhost:4000/planar_homography/#panorama-stitching",
    "relUrl": "/planar_homography/#panorama-stitching"
  },"43": {
    "doc": "Planar Homography",
    "title": "Acknowledgement and References",
    "content": "A lot of images are taken from the lecture slides during my computer vision class at CMU. These were taught by Prof. Kris Kitani and Prof. Deva Ramanan . These slides are publicly available (slides) . ",
    "url": "http://localhost:4000/planar_homography/#acknowledgement-and-references",
    "relUrl": "/planar_homography/#acknowledgement-and-references"
  },"44": {
    "doc": "Planar Homography",
    "title": "My Ohiopyle trip",
    "content": ". ",
    "url": "http://localhost:4000/planar_homography/#my-ohiopyle-trip",
    "relUrl": "/planar_homography/#my-ohiopyle-trip"
  },"45": {
    "doc": "Planar Homography",
    "title": "Helper Functions",
    "content": "The helper function in this framework is shown below: . import numpy as np import cv2 import scipy.io as sio from matplotlib import pyplot as plt import skimage.feature PATCHWIDTH = 9 def briefMatch(desc1,desc2,ratio): matches = skimage.feature.match_descriptors(desc1,desc2,'hamming',cross_check=True,max_ratio=ratio) return matches def plotMatches(im1,im2,matches,locs1,locs2): fig, ax = plt.subplots(nrows=1, ncols=1) im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY) im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY) plt.axis('off') skimage.feature.plot_matches(ax,im1,im2,locs1,locs2,matches,matches_color='r',only_matches=True) plt.show() return def makeTestPattern(patchWidth, nbits): np.random.seed(0) compareX = patchWidth*patchWidth * np.random.random((nbits,1)) compareX = np.floor(compareX).astype(int) np.random.seed(1) compareY = patchWidth*patchWidth * np.random.random((nbits,1)) compareY = np.floor(compareY).astype(int) return (compareX, compareY) def computePixel(img, idx1, idx2, width, center): halfWidth = width // 2 col1 = idx1 % width - halfWidth row1 = idx1 // width - halfWidth col2 = idx2 % width - halfWidth row2 = idx2 // width - halfWidth return 1 if img[int(center[0]+row1)][int(center[1]+col1)] &lt; img[int(center[0]+row2)][int(center[1]+col2)] else 0 def computeBrief(img, locs): patchWidth = 9 nbits = 256 compareX, compareY = makeTestPattern(patchWidth,nbits) m, n = img.shape halfWidth = patchWidth//2 locs = np.array(list(filter(lambda x: halfWidth &lt;= x[0] &lt; m-halfWidth and halfWidth &lt;= x[1] &lt; n-halfWidth, locs))) desc = np.array([list(map(lambda x: computePixel(img, x[0], x[1], patchWidth, c), zip(compareX, compareY))) for c in locs]) return desc, locs def corner_detection(img, sigma): # fast method result_img = skimage.feature.corner_fast(img, n=PATCHWIDTH, threshold=sigma) locs = skimage.feature.corner_peaks(result_img, min_distance=1) return locs def loadVid(path): # Create a VideoCapture object and read from input file # If the input is the camera, pass 0 instead of the video file name cap = cv2.VideoCapture(path) # Append frames to list frames = [] # Check if camera opened successfully if cap.isOpened()== False: print(\"Error opening video stream or file\") # Read until video is completed while(cap.isOpened()): # Capture frame-by-frame ret, frame = cap.read() if ret: #Store the resulting frame frames.append(frame) else: break # When everything done, release the video capture object cap.release() frames = np.stack(frames) return frames . ",
    "url": "http://localhost:4000/planar_homography/#helper-functions",
    "relUrl": "/planar_homography/#helper-functions"
  },"46": {
    "doc": "Planar Homography",
    "title": "Planar Homography",
    "content": ". | Before you Begin | PDFs | Some Basics on Camera Projection . | Projection of 3D to 2D image plane | Camera Matrices . | Generic Representation | Alternate notation of camera matrices | . | . | The Homography Situation . | Single View | Multiple Views | Limitations of Planar Homography | . | Implementation of Homography Estimation . | The Pipeline . | Keypoint Detection | Keypoint Descriptors | Calculating the Homography Matrix | (Bonus) Rejecting outliers during our homography calculation | Implementation of above steps | RANSAC and Construction of Composite Image | . | . | Applying Homography Estimation in the Real World . | Basic cool applications | AR Video | Panorama Stitching | . | Acknowledgement and References . | My Ohiopyle trip | . | Helper Functions | . ",
    "url": "http://localhost:4000/planar_homography/",
    "relUrl": "/planar_homography/"
  },"47": {
    "doc": "Triangulation Theory",
    "title": "Basic Method of behind estimating the rotation and traslation between 2 cameras",
    "content": " ",
    "url": "http://localhost:4000/triangulation_theory/#basic-method-of-behind-estimating-the-rotation-and-traslation-between-2-cameras",
    "relUrl": "/triangulation_theory/#basic-method-of-behind-estimating-the-rotation-and-traslation-between-2-cameras"
  },"48": {
    "doc": "Triangulation Theory",
    "title": "triangulate3D",
    "content": ". | Here we fix one camera (extrinsic matrix = Identity matrix). Now, we know correspondence points in image1 and image2. | Therefore we can find the Fundamental matrix based on these point correspondences | . Now, to estimate the 3D location of these points, we need . | Camera matrices (extrinsic*intrinsic) for both cameras M1 and M2 | Image points (x,y) which correspond to each other | Some theory | . The above theory is summarized as: . After finding the 3D points, we will reproject them back onto the image and compare them with our original correspondence points (which we either manually selected or got from some keypoint detector like ORB or BRIEF) . The formula for reprojection error in this case is: \\(\\operatorname{err}=\\sum_i\\left\\|\\mathbf{x}_{1 i}, \\widehat{\\mathbf{x}_{1 i}}\\right\\|^2+\\left\\|\\mathbf{x}_{2 i}, \\widehat{\\mathbf{x}_{2 i}}\\right\\|^2\\) . ",
    "url": "http://localhost:4000/triangulation_theory/#triangulate3d",
    "relUrl": "/triangulation_theory/#triangulate3d"
  },"49": {
    "doc": "Triangulation Theory",
    "title": "findM2",
    "content": "Previsously we saw that we need an M2 to triangulate, but we don’t have an M2 yet :/. However, since our first camera is fixed in 3D we can find the camera matrix M2 of our second camera as: . def camera2(E): U,S,V = np.linalg.svd(E) m = S[:2].mean() E = U.dot(np.array([[m,0,0], [0,m,0], [0,0,0]])).dot(V) U,S,V = np.linalg.svd(E) W = np.array([[0,-1,0], [1,0,0], [0,0,1]]) if np.linalg.det(U.dot(W).dot(V))&lt;0: W = -W M2s = np.zeros([3,4,4]) M2s[:,:,0] = np.concatenate([U.dot(W).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,1] = np.concatenate([U.dot(W).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,2] = np.concatenate([U.dot(W.T).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,3] = np.concatenate([U.dot(W.T).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) return M2s . Note: The above function gives three possible values for M2 . Now there are 2 checks we can use to find which is the right camera: . | Determinant(Rotation component of M2) = 1 (so that the rotation belongs to SO(3)) | All Z values should be positive (i.e. the 3D point should be in front of both the cameras right?) | . ",
    "url": "http://localhost:4000/triangulation_theory/#findm2",
    "relUrl": "/triangulation_theory/#findm2"
  },"50": {
    "doc": "Triangulation Theory",
    "title": "Combining the above two functions",
    "content": "Now we have point correspondences, M1 and 4 M2’s. Therefore we’ll try to triangulate points based on the correct criteria for camera orientations. Additionally we’ll also try to minimize reprojection error: . # iterate over M1(fixed) and M2(4 possibilites) by passing them to triangulate for i in range(M2.shape[2]): M2_current = M2[:,:,i] # build the C1 and C2: pts_in_3d, err = triangulate((K1 @ M1), pts1, (K2 @ M2_current), pts2) if err &lt; err_min and (np.where(pts_in_3d[:,2] &lt; 0)[0].shape[0] == 0): print(\"satisfies the error criteria\") err_min = err best_M2_i = i best_pts_3d = pts_in_3d if (best_M2_i is not None) and (best_pts_3d is not None): print(\"min err is\", err_min) # return M2, C2, w(3d points), M1, C1 return M2[:,:,best_M2_i], (K2 @ M2[:,:,best_M2_i]), best_pts_3d, M1, (K1 @ M1) # last entry is C1 . Finally we all together have our best_3d_points and the correct M2 matrix . ",
    "url": "http://localhost:4000/triangulation_theory/#combining-the-above-two-functions",
    "relUrl": "/triangulation_theory/#combining-the-above-two-functions"
  },"51": {
    "doc": "Triangulation Theory",
    "title": "Bundle Adjustment",
    "content": "We know that the error in the triangulation is basically difference between the projection of a 3D point and the actual point in 2D on the image. Now, we will move around the 3D points slightly and check in which orientation the reprojection error comes to a global minimum. The formula for the above operation is shown below: . The process we will follow now is very code specific. An explanation for only this below code is shown, where we will only be minimizing the rotation and translation (M2 matrix) error. High level procedure . | Use the 2D point correspondences to find the Fundamental Matrix (along with RANSAC to find the inlier points) | Use the inliers to find our best F (fundamental matrix) | Compute an initial guess for M2 by using our old findM2 function | Now, the above function would have given us 3D points (P_init) and an M2_init | Now, we have compiled the following: . | M1 and K1 | M2_init and K2 | F and E (E = (K2.T @ F) @ K1) | . | . Having the above content, we will need to derive our reprojection error. We will do this in the RodriguesResidual function: . RodriguesResidual: rodriguesResidual(x, K1, M1, p1, K2, p2) . | x basically contains the translation and rotation of camera2. We can therefore get M2 from x | We can find the camera matrices C1 = K1 @ M1, C2 = K2 @ M2 | . | Use the above equation to get p1’ and p2’ | Compare p1’ and p1, p2’ and p2, to get the reprojection error we need in both cameras | . Now we have a function which will give us reprojection error for a given M2 matrix. Now lets see how we’ll use this reporjection error to optimize our M2 . Optimization of M2 . Now that we have a function which will give us reprojection error for any given M2, lets minimize this error by moving around our 3D points slightly such that our reprojection error (for all points cumulative) reduces . We do this using the scipy.optimize.minimize function . # just some repackaging/preprocessing to give x to rodriguesResidual x0 = P_init.flatten() x0 = np.append(x0, r2_0.flatten()) x0 = np.append(x0, t2_0.flatten()) # optimization step x_opt, _ = scipy.optimize.minimze(rodriguesResidual, x0, args=(K1, M1, p1, K2, p2)) . Finally our x_opt i.e x_optimal will have the correct rotation and translation of camera 2 and the corrected 3D points . ",
    "url": "http://localhost:4000/triangulation_theory/#bundle-adjustment",
    "relUrl": "/triangulation_theory/#bundle-adjustment"
  },"52": {
    "doc": "Triangulation Theory",
    "title": "Triangulation Theory",
    "content": ". | Basic Method of behind estimating the rotation and traslation between 2 cameras . | triangulate3D | findM2 | Combining the above two functions | . | Bundle Adjustment . | High level procedure . | RodriguesResidual: rodriguesResidual(x, K1, M1, p1, K2, p2) | . | Optimization of M2 | . | . ",
    "url": "http://localhost:4000/triangulation_theory/",
    "relUrl": "/triangulation_theory/"
  },"53": {
    "doc": "Home",
    "title": "Computer Vision Projects and Assignments",
    "content": "Compiled list of projects and assignments during my time at CMU’s Robotics Institute . Main CV Reference . ",
    "url": "http://localhost:4000/#computer-vision-projects-and-assignments",
    "relUrl": "/#computer-vision-projects-and-assignments"
  },"54": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
}
