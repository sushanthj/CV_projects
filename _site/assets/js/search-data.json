{"0": {
    "doc": "ML Basics",
    "title": "Before you Begin",
    "content": " ",
    "url": "/docs/Deep%20Learning/Basics.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#before-you-begin"
  },"1": {
    "doc": "ML Basics",
    "title": "Linear Algebra Review",
    "content": ". ",
    "url": "/docs/Deep%20Learning/Basics.html#linear-algebra-review",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#linear-algebra-review"
  },"2": {
    "doc": "ML Basics",
    "title": "Defining Linear Boundaries",
    "content": "The most basic equation of a line is y = mx + c. This leads us to the formulation of a baseline linear function to be: . wx + b = 0 which essentially defines a line . ",
    "url": "/docs/Deep%20Learning/Basics.html#defining-linear-boundaries",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#defining-linear-boundaries"
  },"3": {
    "doc": "ML Basics",
    "title": "Perceptron",
    "content": "This basic algorithm is our intro to linear classifiers. The special part here is that it only works on sign(prediction) and not on how good the actual prediction turns out. ",
    "url": "/docs/Deep%20Learning/Basics.html#perceptron",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#perceptron"
  },"4": {
    "doc": "ML Basics",
    "title": "Algorithm",
    "content": ". ",
    "url": "/docs/Deep%20Learning/Basics.html#algorithm",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#algorithm"
  },"5": {
    "doc": "ML Basics",
    "title": "Need for Intercept",
    "content": ". ",
    "url": "/docs/Deep%20Learning/Basics.html#need-for-intercept",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#need-for-intercept"
  },"6": {
    "doc": "ML Basics",
    "title": "Summary",
    "content": ". ",
    "url": "/docs/Deep%20Learning/Basics.html#summary",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html#summary"
  },"7": {
    "doc": "ML Basics",
    "title": "ML Basics",
    "content": "{: .text-delta } . | Before you Begin . | Linear Algebra Review | Defining Linear Boundaries | . | Perceptron . | Algorithm | Need for Intercept | Summary | . | . ",
    "url": "/docs/Deep%20Learning/Basics.html",
    
    "relUrl": "/docs/Deep%20Learning/Basics.html"
  },"8": {
    "doc": "Computer Vision",
    "title": "Theory and Tools in CV",
    "content": " ",
    "url": "/docs/Vision_General#theory-and-tools-in-cv",
    
    "relUrl": "/docs/Vision_General#theory-and-tools-in-cv"
  },"9": {
    "doc": "Computer Vision",
    "title": "Computer Vision",
    "content": " ",
    "url": "/docs/Vision_General",
    
    "relUrl": "/docs/Vision_General"
  },"10": {
    "doc": "Constrained RRT",
    "title": "Why Constrained RRT?",
    "content": "As part of my autonomy project, we needed to pickup a tray and place it in a shelf. Generic RRT helps plan trajectories from say the table -&gt; shelf. However, RRT gives only joint angles which are within the robot’s config space and has no obstacle collisions. In our use-case we needed a plan (trajectory) which ensures that the tray stays level throughout the journey from the table -&gt; shelf. Formally, the constraints mentioned above are: . | Roll = 0 | Pitch = 0 | Yaw = no constraint | . ",
    "url": "/constr_rrt/#why-constrained-rrt",
    
    "relUrl": "/constr_rrt/#why-constrained-rrt"
  },"11": {
    "doc": "Constrained RRT",
    "title": "Generic RRT",
    "content": ". ",
    "url": "/constr_rrt/#generic-rrt",
    
    "relUrl": "/constr_rrt/#generic-rrt"
  },"12": {
    "doc": "Constrained RRT",
    "title": "Sampling in Generic RRT",
    "content": "Vanilla RRT uses simple joint constraints, within which it queries for random samples. The image below shows the Franka Panda robot which will be used in this project. The Franka has 8 joints which also includes the end effector. In vanilla RRT, the joints are given some basic constraints (based on design of the robot) . self.qmin=[-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973] # NOTE-does not include grippers self.qmax=[2.8973, 1.7628, 2.8973, -0.0698, 2.8973, 3.7525, 2.8973] # NOTE-does not include gripper . RRT Then works by simply sampling randomly in the limits of qmin and qmax . def SampleRobotConfig(self): q=[] for i in range(7): q.append(self.qmin[i]+(self.qmax[i]-self.qmin[i])*random.random()) return q . Let’s call new sampled joints as vertices (like nodes in a graph) and any two edges are by edges (like edges in a graph) . Checks on sampled points: . | We then check for collisions along these sampled joint angles. | Note. In other methodws like PRM (probabalistic roadmaps), the configuration space is queried beforehand and is stored to reduce search time | However, if we sample vertices that are too far away, we will have to constrain the expansion | We also need to check if one of our vertices is close enough to the goal to say we’ve reached | Note. We also introduce a goal bias by directly setting the sample config = goal config say 2% of the time. | . These checks are shown below: . def RRTQuery(): global FoundSolution global plan global rrtVertices global rrtEdges while len(rrtVertices)&lt;3000 and not FoundSolution: # TODO : Fill in the algorithm here # create a random node (x,y as a 2,1 array) qRand = mybot.SampleRobotConfig() # introduce the goal bias. (set the random node as goal with a certain prob) if np.random.uniform(0,1) &lt; thresh: qRand = qGoal idNear = FindNearest(rrtVertices, qRand) qNear = rrtVertices[idNear] qNear, qRand = np.asarray(qNear), np.asarray(qRand) # if it's above threshold, move in the direction of the new node, but only upto the # threshold (which limits max distance between two nodes) while np.linalg.norm(qRand - qNear) &gt; thresh: # qConnect = qNear + thres * unit_vector_pointing_towards_qRand qConnect = qNear + thresh * ((qRand-qNear) / np.linalg.norm(qRand-qNear)) if not mybot.DetectCollisionEdge(qConnect, qNear, pointsObs, axesObs): rrtVertices.append(qConnect) rrtEdges.append(idNear) qNear = qConnect else: break # check for collisions qConnect = qRand if not mybot.DetectCollisionEdge(qConnect, qNear, pointsObs, axesObs): # if no collision in new joint angles (qConnect), then add as a valid node and edge rrtVertices.append(qConnect) rrtEdges.append(idNear) # check if the qGoal is close to some node idNear = FindNearest(rrtVertices, qGoal) # if the qGoal is really close (&lt; 0.025) then we've pretty much reached goal! if np.linalg.norm(np.asarray(qGoal) - np.asarray(rrtVertices[idNear])) &lt; 0.025: # add the goal node as our final node rrtVertices.append(qGoal) rrtEdges.append(idNear) print(\"SOLUTION FOUND\") FoundSolution = True print(len(rrtVertices)) . ",
    "url": "/constr_rrt/#sampling-in-generic-rrt",
    
    "relUrl": "/constr_rrt/#sampling-in-generic-rrt"
  },"13": {
    "doc": "Constrained RRT",
    "title": "Constrained RRT",
    "content": "We saw above some tricks to make simple RRT work. Now, with one small modification we can also make it work in a constrained manner. ",
    "url": "/constr_rrt/#constrained-rrt",
    
    "relUrl": "/constr_rrt/#constrained-rrt"
  },"14": {
    "doc": "Constrained RRT",
    "title": "Constraining Sampled Points",
    "content": ". | To constrain the sampled points, we simply project the config space of the sampled points to the constrained config space | This projection was described by Berenson, Siddhartha S. etal | The process of projecting sample_points -&gt; valid_config_space is achieved by gradient descent | . In a simple manner, we essentially do the following: . | Define a state vector for the end effector | Define a cost function which uses certain elements in the above state vector | Minimize this cost function to obtain the valid config-space needed | . In the above picture, the cost function seeks to minimize the roll and pitch of the end effector . The final equation shows the update step (gradient descent) . ",
    "url": "/constr_rrt/#constraining-sampled-points",
    
    "relUrl": "/constr_rrt/#constraining-sampled-points"
  },"15": {
    "doc": "Constrained RRT",
    "title": "Defining constraints in code",
    "content": "Projection Function . def project_to_constrain(qRand): \"\"\" Project to make roll and pitch zero where possible. We do this by gradient descent Our cost function is C = (3.14 - roll)**2 + pitch**2 (we want to minize roll and pitch) NOTE: (3.14 - roll) since we have init roll of 3.14 \"\"\" # do forward kinematics and get the roll, pitch at qRand roll, pitch, yaw, J = get_roll_pitch_of_rand_pt(qRand) # print(f\"init roll={roll} and pitch={pitch} and yaw={yaw}\") if (abs(starting_roll-abs(roll))) &gt; rejection_threshold or \\ (abs(starting_pitch - abs(pitch)) &gt; rejection_threshold): return qRand, True count = 0 # while(((starting_roll-abs(roll))**2 + pitch**2 + (starting_yaw - abs(yaw)) &gt; cost_thresh) and count &lt; 1000): while(((starting_roll-abs(roll))**2 + (starting_pitch-abs(pitch))**2 &gt; cost_thresh) and count &lt; 1000): grad_cost_wrt_xyzrpy = np.expand_dims(np.array([0,0,0, 2*roll, 2*pitch, 0]), axis=1) gradient = J.T @ grad_cost_wrt_xyzrpy qRand = np.expand_dims(np.array(qRand), axis=1) - learning_rate * gradient qRand = np.squeeze(qRand).tolist() roll, pitch, yaw, J = get_roll_pitch_of_rand_pt(qRand) count += 1 # print(f\"final roll={roll} and pitch={pitch} and yaw={yaw}\") return qRand, False def get_roll_pitch_of_rand_pt(qRand): # do forward kinematics and get the Tcurr, J at qRand Tcurr, J = mybot.ForwardKin_for_check(qRand) last_link_rotation = np.asarray(Tcurr[joint_to_constrain])[0:3,0:3] r = Rotation.from_matrix(last_link_rotation) roll, pitch, yaw = r.as_euler('xyz') return roll, pitch, yaw, J . Introducing constraints to RRT . In addition to the steps specified in the algorithm above, I needed to tune some hyperparameters to make it work. Specifically Rejection Threshold, Learning Rate, Cost Threshold. | Even before doing gradient descent, I verify if the end effector state (specifically roll and pitch) are within 1 radian from my goal state (zero roll and zero pitch). This sped up the algorithm, possibly because it takes longer to compute the jacobian and do gradient descent for samples that are too far away from desired state. | Secondly, I needed to tune the learning rate of the gradient descent step | I also had to define a threshold within which the cost function would need to optimize wihtin (it would take forever if I wanted roll^2 + pitch^2 == 0), therefore I let gradient descent to run uptill roll^2 + pitch^2 &lt; 0.2 | . Implementation . def RRTQuery(): global FoundSolution global plan global rrtVertices global rrtEdges roll, pitch, yaw, J = get_roll_pitch_of_rand_pt(qInit) print(\"starting roll, pitch, and yaw\", roll, pitch, yaw) # making the assumption that we should find solution within 3000 iterations while len(rrtVertices)&lt;10000 and not FoundSolution: # TODO : Fill in the algorithm here # create a random node (x,y as a 2,1 array) qRand = mybot.SampleRobotConfig() # introduce the goal bias. (set the random node as goal with a certain prob) if np.random.uniform(0,1) &lt; thresh: qRand = qGoal \"\"\"Constrained RRT step\"\"\" # NOTE: now that we have a qRand, if we want this qRand to be such that the # end effector has roll and pitch as zero qRand, flag_1 = project_to_constrain(qRand) flag_2 = False for i in range(len(qRand)): if (qRand[i] &gt; mybot.qmax[i] or qRand[i] &lt; mybot.qmin[i]): flag_2 = True # flag_1 -&gt; being true denotes that we couldn't project # flag_2 -&gt; being true denotes that we got infeasible joint angles # print(flag_1, flag_2) if flag_1 or flag_2: continue \"\"\"End of Constrained RRT\"\"\" idNear = FindNearest(rrtVertices, qRand) qNear = rrtVertices[idNear] qNear, qRand = np.asarray(qNear), np.asarray(qRand) # if it's above threshold, move in the direction of the new node, but only upto the # threshold (which limits max distance between two nodes) while np.linalg.norm(qRand - qNear) &gt; thresh: # qConnect = qNear + thres * unit_vector_pointing_towards_qRand qConnect = qNear + thresh * ((qRand-qNear) / np.linalg.norm(qRand-qNear)) \"\"\"Constrained RRT step\"\"\" # NOTE: now that we have a qRand, if we want this qRand to be such that the # end effector has roll and pitch as zero qConnect, flag_1 = project_to_constrain(np.ndarray.tolist(qConnect)) flag_2 = False for i in range(len(qRand)): if (qConnect[i] &gt; mybot.qmax[i] or qRand[i] &lt; mybot.qmin[i]): flag_2 = True # flag_1 -&gt; being true denotes that we couldn't project # flag_2 -&gt; being true denotes that we got infeasible joint angles # print(flag_1, flag_2) if flag_1 or flag_2: break else: qConnect = np.asarray(qConnect) \"\"\"End of Constrained RRT\"\"\" if not mybot.DetectCollisionEdge(qConnect, qNear, pointsObs, axesObs): rrtVertices.append(qConnect) rrtEdges.append(idNear) qNear = qConnect else: break # check for collisions qConnect = qRand if not mybot.DetectCollisionEdge(qConnect, qNear, pointsObs, axesObs): # if no collision in new joint angles (qConnect), then add as a valid node and edge rrtVertices.append(qConnect) rrtEdges.append(idNear) # check if the qGoal is close to some node idNear = FindNearest(rrtVertices, qGoal) # if the qGoal is really close (&lt; 0.025) then we've pretty much reached goal! if np.linalg.norm(np.asarray(qGoal) - np.asarray(rrtVertices[idNear])) &lt; 0.025: # add the goal node as our final node rrtVertices.append(qGoal) rrtEdges.append(idNear) print(\"SOLUTION FOUND\") FoundSolution = True print(len(rrtVertices)) . ",
    "url": "/constr_rrt/#defining-constraints-in-code",
    
    "relUrl": "/constr_rrt/#defining-constraints-in-code"
  },"16": {
    "doc": "Constrained RRT",
    "title": "Testing in Simulation",
    "content": "The above code was tested using Mujoco simulator. I’ve shown comparisons between vanilla and constrained RRT . | Vanilla RRT | Constrained RRT | . | | | . ",
    "url": "/constr_rrt/#testing-in-simulation",
    
    "relUrl": "/constr_rrt/#testing-in-simulation"
  },"17": {
    "doc": "Constrained RRT",
    "title": "Testing in Real Life",
    "content": " ",
    "url": "/constr_rrt/#testing-in-real-life",
    
    "relUrl": "/constr_rrt/#testing-in-real-life"
  },"18": {
    "doc": "Constrained RRT",
    "title": "Acknowledgement",
    "content": "I’d like to thank my team-mate Zack for working beside me throughout this project . ",
    "url": "/constr_rrt/#acknowledgement",
    
    "relUrl": "/constr_rrt/#acknowledgement"
  },"19": {
    "doc": "Constrained RRT",
    "title": "Constrained RRT",
    "content": ". | Why Constrained RRT? | Generic RRT . | Sampling in Generic RRT | . | Constrained RRT . | Constraining Sampled Points | Defining constraints in code . | Projection Function | Introducing constraints to RRT | Implementation | . | Testing in Simulation | . | Testing in Real Life | Acknowledgement | . ",
    "url": "/constr_rrt/",
    
    "relUrl": "/constr_rrt/"
  },"20": {
    "doc": "ConvNext",
    "title": "ConvNext",
    "content": "This paper is best described in its abstract as ‘We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way’. What we’ll try to learn through building ConvNext is the meaning behind these design choices in terms of: . | Activation Functions | Architechture | Inductive Biases | And more so .. | . ",
    "url": "/ConvNext/",
    
    "relUrl": "/ConvNext/"
  },"21": {
    "doc": "ConvNext",
    "title": "Why I Love ConvNext",
    "content": "This is one of the few architechtures which I have re-written from scratch including the Dataloaders. I used ConvNext for Face Classification and beat 250+ students and TAs in my class on a Kaggle Competition. Github . I not only re-wrote it in a simple manner, I also had to make many design decisions in reducing the channel widths and reducing network depth to brind down the trainable params from 29 Million to just 11 million . | ConvNext | Why I Love ConvNext | Introduction . | Drawbacks of Vanilla Vision Transformers (ViTs) | Enter Hierarchial ViTs like SWIN Transformer | . | Approach . | Training Optimizations | Network Modernization . | Understanding ResNets | Macro Design | Making it more Lean (to reduce params) | ResNextify . | Why Depthwise Convolutions | . | Inverted BottleNeck and Large Kernels | Activations | . | . | Implementation . | ConvNext Block | Network Setup | Stochastic Depth and Training | . | Appendix . | Time Complexities Analyses . | Simple Matrix Multiplication | Time Complexity Analysis in Tranformers . | Comparison with RNNs | Comparisons with Separable and Non-Separable Convs | Conclusion: | . | . | . | . ",
    "url": "/ConvNext/#why-i-love-convnext",
    
    "relUrl": "/ConvNext/#why-i-love-convnext"
  },"22": {
    "doc": "ConvNext",
    "title": "Introduction",
    "content": " ",
    "url": "/ConvNext/#introduction",
    
    "relUrl": "/ConvNext/#introduction"
  },"23": {
    "doc": "ConvNext",
    "title": "Drawbacks of Vanilla Vision Transformers (ViTs)",
    "content": ". | ViTs became famous due to their ability to scale | With huge datasets they outperformed ResNets on Image Classification | However, ironically, the cost of global attention (to all tokens i.e. all image patches fed to the transformer) grows quadratically with image size | For real world images, this issue is a big problem! | . ",
    "url": "/ConvNext/#drawbacks-of-vanilla-vision-transformers-vits",
    
    "relUrl": "/ConvNext/#drawbacks-of-vanilla-vision-transformers-vits"
  },"24": {
    "doc": "ConvNext",
    "title": "Enter Hierarchial ViTs like SWIN Transformer",
    "content": ". | Instead of just global attention, introduce attention locally to a window (red boundary) | A fixed number of image patches form a window | This reduces the time complexity from being quadratic in image size for generic ViTs to now being linear w.r.t image size . | This linear time complexity w.r.t image size made ViTs tractable for all vision tasks like detection, segmentation and classification | . ",
    "url": "/ConvNext/#enter-hierarchial-vits-like-swin-transformer",
    
    "relUrl": "/ConvNext/#enter-hierarchial-vits-like-swin-transformer"
  },"25": {
    "doc": "ConvNext",
    "title": "Approach",
    "content": "The use of shifted-windows as in Swin Transformers and the learnings from the era of ViTs motivate the authors of ConvNext to begin ‘modernizing’ CNNs. They begin by taking a simple ResNet-50 model and reshaping it from the learning of ViTs. They do this in two steps: . | New Training Methods | New Network Architectures which include: . | Macro Design changes | ResNextify | Inverted Bottleneck | Larger Kernel Sizes | Layer-wise micro designs | . | . ",
    "url": "/ConvNext/#approach",
    
    "relUrl": "/ConvNext/#approach"
  },"26": {
    "doc": "ConvNext",
    "title": "Training Optimizations",
    "content": "This mainly included new optimizers, larger training epochs, and new augmentation methods. Specifically: . | AdamW over Adam | Augmentations such as: Mixup, Cutmix, RandAugment, Random Erasing | Regularization schemes including Stochastic Depth and Label Smoothing | . Stochastic depth is when we choose to keep a residual block active or inactive based on some probability (maybe bernoulli or a uniform probability distribution) as shown below: . ",
    "url": "/ConvNext/#training-optimizations",
    
    "relUrl": "/ConvNext/#training-optimizations"
  },"27": {
    "doc": "ConvNext",
    "title": "Network Modernization",
    "content": "Understanding ResNets . It’s beneficial to first see how a resnet works. Firstly note that we have two variants of the ResNet block . | Simple Block (used in ResNet34) | BottleNeck Block (used in all other ResNets) | . The overall architechture of Resnet is captured in the below diagrams: . Where the final 1000x1 vector is for the 1000 ImageNet classes. Also note the number of repeating ResNet block in each layer (50-layer or ResNet50 being referred below): . | Conv2_x has 3 blocks | Conv3_x has 4 blocks | . Overall we have (3,4,6,3) as ‘stage compute ratio’ as defined by authors. Macro Design . We saw (3,4,6,3) as ‘stage compute ratio’ in ResNet50 as explained previously. In Swin-Transformer the same block distribution was (1,1,9,1). Hence, ConvNext tries to follow the same and uses (3,3,9,3) as the block distributions. Making it more Lean (to reduce params) . However, I had to cut down on this to reduce parameter limit and changed the ratios to (6,5,4,4). This was chosen after a few ablations but also higher numbers for the initial blocks were chosen to allow for an optimization on the number of channels at input/output of each ConvNext stage. Specifically: . # number of channels at input/output of each res_blocks # Updated Config self.channel_list = [50, 175, 250, 400] # Original Config # self.channel_list = [96, 192, 384, 768] # number of repeats for each res_block # Updated Config self.block_repeat_list = [6,5,4,4] # Original Config # self.block_repeat_list = [3,3,9,3] . As you can see, to maintain the relative number of channels at each stage (at least keep it monotonically increasing as in the original config), I had to increase the initial block_repeats where the channel size is small and decrease the block_repeats when channel size was larger . ResNextify . ResNext utilized group convolution in the 3x3 conv layer of bottleneck blocks. What is group convolution? . The authors of ConvNext decided to use a special case of group convolution where the number of groups equals number of channels. That is literally just Depthwise Seperable Convs!!! . Why Depthwise Convolutions . Depthwise Seperable Conv has two stages: Depthwise Conv (KxKx1 filters) &amp; Pointwise Conv (1x1xC filters) . The simple answer is the computational complexitites: . | Depth-wise Separable = O(n**2*d + n*d**2) -&gt; as per Attention is all you need | Generic Convolution = O(n**2 * d**2) -&gt; Think of n = filter size spatial, d = filter size depth (num channels) | Reference : MobileNet | Reference : Attention is All You Need | . However, while those numbers may seem weird, for a more practical example you can view this post. Bottomline, MobileNet shows that Depthwise Seperable Conv has much lesser FLOPs than conventional convolution layers. There is also some super cool intuition on how Depthwise + Pointwise Conv is similar to Self Attention! . We note that depthwise convolution is similar to the weighted sum operation in self-attention, which operates on a per-channel basis, i.e., only mixing information in the spatial dimension. That’s a little painful to understand. But first read this nice blog to understand the process of self-attention. In that blog, we have a values matrix had shape values.shape: torch.Size([6, 28]). We then compute the attention weights for the second word (second token). NOTE: Computing attention_weights for second token means using q_2 matrix and multiplying it with k_1, k_2, k_3, k_4, k_5, k_6 and then some softmax. But intuition is that second word was the query or our anchor and we wanted to see how all other words are close/far from second word. So, if second word is our query, and we got attention weights attention_weights_2, which has shape 1x6. We then multiply this attention_weights_2 with the values matrix which comprises ALL words/tokens and has shape 6x28 . So finally, the attention_weights_2 @ values yields a 28x1 size context vector which is just one output of the self-attention head. But, the intuition of Depthwise seperable convolution that comes here is that: . In the above picture, we see that attention of word_1 get’s multiplied with word 1’s positional embedding and there is no cross over. That’s the best understanding I could infer from the statement. Inverted BottleNeck and Large Kernels . Inverted bottlenecks were made famous long back by MobileNetV2 and that design stuck even with Transformers . | ResNet Bottleneck | Proposed ConvNext Bottleneck | . | | | . Another aspect to note is that depthwise conv was moved up. The reasoning for this is as follows: . | We want to emulate the Swin-T’s large kernel size. We only have one spatial conv layer (that too is depthwise (d3x3)) | If we do d3x3 as in the middle design of ConvNext Bottleneck that’ll be d3x3 with 384 channels | Instead if we move it up earlier, the spatial d3x3 Conv happens with 96 channels, similarly the pointwise 1x1 conv happens across 384 channels | So the 1x1 does the heavy lifting but it’s fast, the d3x3 satisfies the large kernel size requirement with low number of channels | . To push this further, we increase d3x3 to d7x7 to exactly match the Swin Transformer: . Activations . This was very well written in the ConvNext paper, I’m directly quoting it here: Consider a Transformer block with key/query/value linear embedding layers, the projection layer, and two linear layers in an MLP block. There is only one activation function present in the MLP block. In comparison, it is common practice to append an activation function to each convolutional layer, including the 1 × 1 convs. Here we examine how performance changes when we stick to the same strategy. As depicted in Figure 4, we eliminate all GELU layers from the residual block except for one between two 1 × 1 layers, replicating the style of a Transformer block. This process improves the result by 0.7% to 81.3%, practically matching the performance of Swin-T. So we essentially: . | Replaced ReLU with GELU | Reduced the number of activations | They also reduced the number of normalizations (mentioned later) | . ",
    "url": "/ConvNext/#network-modernization",
    
    "relUrl": "/ConvNext/#network-modernization"
  },"28": {
    "doc": "ConvNext",
    "title": "Implementation",
    "content": " ",
    "url": "/ConvNext/#implementation",
    
    "relUrl": "/ConvNext/#implementation"
  },"29": {
    "doc": "ConvNext",
    "title": "ConvNext Block",
    "content": "class ConvNextBlock(torch.nn.Module): \"\"\" Refer : https://browse.arxiv.org/pdf/2201.03545v2.pdf for detailed architechture \"\"\" def __init__(self, num_ch, expansion_factor, drop_prob=0.0): # num_ch = number of channels at first and third layer of block # There'll be an expansion in the second layer given by expansion_factor super().__init__() \"\"\" NOTE: To perform depthwise conv we use the param (groups=num_ch) to create a separate filter for each input channel \"\"\" self.main_block = torch.nn.Sequential( # 1st conv layer (deptwise) torch.nn.Conv2d(in_channels=num_ch, out_channels=num_ch, kernel_size=7, padding=3, groups=num_ch), torch.nn.BatchNorm2d(num_ch), # 2nd conv layer torch.nn.Conv2d(in_channels=num_ch, out_channels=num_ch*expansion_factor, kernel_size=1, stride=1), # 1x1 pointwise convs implemented as Linear Layer torch.nn.GELU(), # 3rd conv layer torch.nn.Conv2d(in_channels=num_ch*expansion_factor, out_channels=num_ch, kernel_size=1, stride=1) ) for layer in self.main_block: if isinstance(layer, torch.nn.Conv2d): init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std']) init.constant_(layer.bias, 0) # define the drop_path layer if drop_prob &gt; 0.0: self.drop_residual_path = DropPath(drop_prob) else: self.drop_residual_path = torch.nn.Identity() def forward(self, x): input = x.clone() x = self.main_block(x) # sum the main and shortcut connection x = input + self.drop_residual_path(x) return x . ",
    "url": "/ConvNext/#convnext-block",
    
    "relUrl": "/ConvNext/#convnext-block"
  },"30": {
    "doc": "ConvNext",
    "title": "Network Setup",
    "content": "class Network(torch.nn.Module): \"\"\" ConvNext \"\"\" def __init__(self, num_classes=7001, drop_rate=0.5, expand_factor=4): super().__init__() self.backbone_out_channels = 400 self.num_classes = num_classes # number of channels at input/output of each res_blocks self.channel_list = [50, 175, 250, 400] # self.channel_list = [96, 192, 384, 768] # number of repeats for each res_block self.block_repeat_list = [6,5,4,4] # self.block_repeat_list = [3,3,9,3] # define number of stages from above self.num_stages = len(self.block_repeat_list) self.drop_path_probabilities = [i.item() for i in torch.linspace(0, drop_rate, sum(self.channel_list))] ############## DEFINE RES BLOCK AND AUX LAYERS ######################## # # Define the Stem (the first layer which takes input images) self.stem = torch.nn.Sequential( torch.nn.Conv2d(in_channels=3, out_channels=self.channel_list[0], kernel_size=4, stride=4), torch.nn.BatchNorm2d(self.channel_list[0]), ) # truncated normal initialization for layer in self.stem: if isinstance(layer, torch.nn.Conv2d): init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std']) init.constant_(layer.bias, 0) # # Store the LayerNorm and Downsampling layer when switching btw 2 types of res_blocks # self.block_to_block_ln_and_downsample = [] self.block_to_block_ln_and_downsample = [self.stem] for i in range(self.num_stages - 1): inter_downsample = torch.nn.Sequential( torch.nn.BatchNorm2d(self.channel_list[i]), torch.nn.Conv2d(in_channels=self.channel_list[i], out_channels=self.channel_list[i+1], kernel_size=2, stride=2) ) self.block_to_block_ln_and_downsample.append(inter_downsample) # Store the Res_block stages (eg. 3xres_2, 3xres_3, ...) self.res_block_stages = torch.nn.ModuleList() for i in range(self.num_stages): res_block_layer = [] for j in range(self.block_repeat_list[i]): res_block_layer.append(ConvNextBlock(num_ch=self.channel_list[i], expansion_factor=expand_factor, drop_prob=self.drop_path_probabilities[i+j])) # append the repeated res_blocks as one layer # *res_block_layer means we add individual elements of the res_block_layer list self.res_block_stages.append(torch.nn.Sequential(*res_block_layer)) # truncated normal initialization for res_block_stage in self.res_block_stages: for layer in res_block_stage: if isinstance(layer, torch.nn.Conv2d): init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std']) init.constant_(layer.bias, 0) ##################################################################### self.backbone = torch.nn.Sequential( # essentially stem (replace with stem if it works) self.block_to_block_ln_and_downsample[0], # res_1 block self.res_block_stages[0], self.block_to_block_ln_and_downsample[1], # res_2 block self.res_block_stages[1], self.block_to_block_ln_and_downsample[2], # res_3 block self.res_block_stages[2], self.block_to_block_ln_and_downsample[3], # res_4 block self.res_block_stages[3], torch.nn.AdaptiveAvgPool2d((1,1)), torch.nn.Flatten(), ) self.cls_layer = torch.nn.Sequential( torch.nn.Linear(self.backbone_out_channels, self.num_classes)) # truncated normal initialization for layer in self.cls_layer: if isinstance(layer, torch.nn.Linear): init.trunc_normal_(layer.weight, mean=config['truncated_normal_mean'], std=config['truncated_normal_std']) init.constant_(layer.bias, 0) def forward(self, x, return_feats=False): \"\"\" What is return_feats? It essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. \"\"\" feats = self.backbone(x) out = self.cls_layer(feats) if return_feats: return feats else: return out model = Network().to(DEVICE) summary(model, (3, 224, 224)) . ",
    "url": "/ConvNext/#network-setup",
    
    "relUrl": "/ConvNext/#network-setup"
  },"31": {
    "doc": "ConvNext",
    "title": "Stochastic Depth and Training",
    "content": "class DropPath(torch.nn.Module): \"\"\" Stochastic Depth (we drop the non-shortcut path inside residual blocks with some probability p) \"\"\" def __init__(self, drop_probability = 0.0): super().__init__() self.drop_prob = drop_probability def forward(self, x): # if drop prob is zero or in inference mode, skip this if np.isclose(self.drop_prob, 0.0, atol=1e-9) or not self.training: return x # find output shape (eg. if input = 4D tensor, output = (1,1,1,1)) # output_shape = (x.shape[0],) + (1,) * (x.ndim - 1) output_shape = (x.shape[0],1,1,1) # create mask of output shape and of input type on same device keep_mask = torch.empty(output_shape, dtype=x.dtype, device=DEVICE).bernoulli_((1-self.drop_prob)) # Alternative: random_tensor = x.new_empty(shape).bernoulli_(keep_prob) # NOTE: all methods like bernoulli_ with the underscore suffix means they # are inplace operations keep_mask.div_((1-self.drop_prob)) return x*keep_mask criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # multi class classification, hence CELoss and not BCELoss optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], betas=(0.9, 0.999), weight_decay=0.05) gamma = 0.6 milestones = [10,20,40,60,80] # scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.9, total_iters=5) scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma) # scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[20, 51]) scaler = torch.cuda.amp.GradScaler() . ",
    "url": "/ConvNext/#stochastic-depth-and-training",
    
    "relUrl": "/ConvNext/#stochastic-depth-and-training"
  },"32": {
    "doc": "ConvNext",
    "title": "Appendix",
    "content": " ",
    "url": "/ConvNext/#appendix",
    
    "relUrl": "/ConvNext/#appendix"
  },"33": {
    "doc": "ConvNext",
    "title": "Time Complexities Analyses",
    "content": "Simple Matrix Multiplication . In general if we are multiplying two matrices A (of size {N,D}) and B (of size {D,D}) then A@B will involve three nested loops, specifically: . | For each of the N rows in A . | We perform D dot products . | Which each involves D multiplictions | . | . | . Hence, overall time complexity = N * D * D = N * D**2 . Time Complexity Analysis in Tranformers . The transformers are seq2seq models with desired output (during training) is just the right shifted inputs. For example if input is ‘I am superman’ and we are building a word2word prediciton language model given input I the desired output is am and that makes: . | OurInput = &lt;SOS&gt; I am Superman | Desired output = I am Superman &lt;EOS&gt; | . Consider we have N words which we project in embedding layer where each word gets projected to a vector of shape D, then a sentence of N words will get projected to a shape of N x D (just a matrix where num_rows = num_words and num_cols = projection_size) . Then self attention in scaled-dot-product form: . Will have the following time comlexity . | Linearly transforming the rows of X to compute the query Q, key K, and value V matrices, each of which has shape (N, D). This is accomplished by post-multiplying X with 3 learned matrices of shape (D, D), amounting to a computational complexity of O(N D^2). | Computing the layer output, specified in above equation of the paper as SoftMax(Q @ Kt / sqrt(d)) V, where the softmax is computed over each row. Computing Q @ Kt has complexity O(N^2 D), and post-multiplying the resultant with V has complexity O(N^2 D) as well. | . Overall the time complexity would be O(N^2.D + N.D^2) . NOTE: In the paper, they say it takes only O(N^2 D) for Self Attention, but this excludes the calculation of Q,K,V . Comparison with RNNs . RNNs have a hidden state neuron which is connected across the time series as shown below: . The hidden neuron computation is simply: h(t)​ = f(U x(t)​ + W h(t−1)​) . Hence, they are modelled as O(n * d*2) *(as it’s an MLP with matrix multiplication, see Appendix) with O(n) sequential operations . Comparisons with Separable and Non-Separable Convs . | Depth-wise Separable = O(n**2*d + n*d**2) = Self Attention + Feed Forward MLP | Generic Convolution = O(n**2 * d**2) | . Conclusion: . The authors of Attention is All You Need therefore claim that Self Attention (O(N**2*D) or truly O(N**2*D + N*D**2)) is parallelizable and faster than the next best option -&gt; i.e. Depthwise Separable Convolution (O(N**2*D + N*D**2)) . Considering the true calculation of Scaled Dot Product Attention, it seems to be the same as Depthwise Separable Convolution. ",
    "url": "/ConvNext/#time-complexities-analyses",
    
    "relUrl": "/ConvNext/#time-complexities-analyses"
  },"34": {
    "doc": "Deep Learning Starter",
    "title": "Before you Begin",
    "content": "My Primary Reference is my 11-785 Intro to Deep Learning at CMU . Ref: CS231N Ref: CS229 Ref: CS231N Videos . ",
    "url": "/docs/Deep%20Learning/DL.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#before-you-begin"
  },"35": {
    "doc": "Deep Learning Starter",
    "title": "Era before DL",
    "content": "An RI Seminar from 2013 of Deva Ramanan shows what was then the State-of-the-Art methods in vision for object detection. I really like starting here as Deva had explained this transition from detection through classification of small sub-parts of a human (Deformable Parts Model) —&gt; to the advent of current Deep Learning . ",
    "url": "/docs/Deep%20Learning/DL.html#era-before-dl",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#era-before-dl"
  },"36": {
    "doc": "Deep Learning Starter",
    "title": "Deformable Parts Model",
    "content": "Naieve Way . | We have just one template which is found by taking the histogram-of-graidents (HOG) . | HOG is just a feature descriptor, just like SIFT and FAST are keypoint descriptors, but HOG is on a more global level . | Use HOG template across image to find possible matches . | . Issues with HOG . Now if we only used HOG templates for each class, it would have to capture the long tail of all possible human poses as shown below: . Deformable Templates . Therefore, to mitigate this issue of trying to capture all effects of the long tail, we can instead only have templates for each part. | Define a template for a few parts separately | Define certain learnable weights (think of it like springs whose lengths have to be learnt) | Train the weights for these springs over a small dataset | Develop other possible deformation modes using eigen vectors | . Conditional Independence . | Here, we don’t have to think of it as K parts with L possible locations making it L^K configs | Instead, we just construct as a dynamic programming model | This will make use of the assumption that given a torso, the relationship of where the leg is w.r.t. the torso, i.e. the torso-leg relationship is independent of the torso-head relationship. (This is called a spatial markov property) | This conditional independece will help do inference in linear time | . Actual Training/Inference Steps: . | I.e we’ll convolve each template across the image and get like a heat map for each of these templates | Now that we have heat-maps, we’ll relate them by the following formula (think of it like our objective function): | This formula says that we calculate local scores (phi) of each template (how well a head/face/hand) was matched in an image as well as the contextual information of how far is a head location from an arm location (psi) . The above formula can be thought of in the Dynamic Programming perspective where we have a graph with nodes of all possible head locations, all possible torso locations and we just need to find the least energy path . | This process shown in the computation graph is done actually on the images in the following manner: . Here, the steps are: . | We find the hotspots for the torso over the whole image (middle heat map above) | Now, within a radius we want to find the possible location of the head (we can do this by taking the maxpool of a 2x2 location around the torso in a given radius) | Therefore to do the above step we just take the heatmap of the head (first heat map above) and do a maxpool. Then we shift this to match the ideal location of where the head should be w.r.t the torso | By doing the above step of maxpool and shift, we have found one least energy path in the computational graph. Therefore we have found for every torso in middle heat map, the score of every possible head it could connect to. Then we do the same process to compute the score of every possible torso and by chaining every possible best head location the legs can connect to | . | . | We can also run the test images over a few different models (these models would test the images for multiple deformation modes (like maybe 1/2 affine deformations, rotations etc)) . | Now, we can model the training of the above architechture as an SVM with hingle loss as: . | The main advantage in this method is that we won’t have to create too huge a sample set of negative samples (remember we have a huge number of possible locations where each feature like hand or face can occur in the image) . | Think of it, there will be a lot of images in the world which in a small window can look can look like a wrist. Therefore, huge negative sample set | However, in this case you will only care about a writst that is detected easily but it is also co-located near a face that was detected (we only care about the context here) See table below: \\ | . | Hard negatives without context | Hard negatives in context | . | | | . | . ",
    "url": "/docs/Deep%20Learning/DL.html#deformable-parts-model",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#deformable-parts-model"
  },"37": {
    "doc": "Deep Learning Starter",
    "title": "Connecting Deformable Parts Model to Deep Learning",
    "content": "From what we saw above, to ideally detect a human we had to know both the local part-wise detections and the global orientations. If you want to improve further, we could do even sub-part detections and have more hierarchy. Therefore, one can see the need for hierarchial information to accurately detect objects. —&gt; this is a nice motivation for hierarchial structure of deep networks as: . | The first layer will see large features | As we go into deeper layers we will see more sub-part wise features | And finally we use all this information to guess where an object might be located in the image | . TODO: This hierarchy of features may not be what’s happening. Explain why! . ",
    "url": "/docs/Deep%20Learning/DL.html#connecting-deformable-parts-model-to-deep-learning",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#connecting-deformable-parts-model-to-deep-learning"
  },"38": {
    "doc": "Deep Learning Starter",
    "title": "Getting into DL",
    "content": "The first thing Deep Networks do is that they blur the line between extracting features and actually doing classification on these features. (This happens throughout the network and not only in the last layer of a network as people would commonly say) . ",
    "url": "/docs/Deep%20Learning/DL.html#getting-into-dl",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#getting-into-dl"
  },"39": {
    "doc": "Deep Learning Starter",
    "title": "Thinking of a feature extractor network as a big patch descriptor",
    "content": "Now, having said that, we could still use a network to extract some features and give an encoding how SIFT gives a 128 number encoding for an image patch (here the encoding will be larger because the number of neurons in the final layer is low, and the outputs of these neurons will also be low) maybe 500 number long encoding for the whole image . ",
    "url": "/docs/Deep%20Learning/DL.html#thinking-of-a-feature-extractor-network-as-a-big-patch-descriptor",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#thinking-of-a-feature-extractor-network-as-a-big-patch-descriptor"
  },"40": {
    "doc": "Deep Learning Starter",
    "title": "Simple Math on Fully Connected Networks",
    "content": ". ",
    "url": "/docs/Deep%20Learning/DL.html#simple-math-on-fully-connected-networks",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#simple-math-on-fully-connected-networks"
  },"41": {
    "doc": "Deep Learning Starter",
    "title": "Convolutional Neural Nets",
    "content": ". We’re going to claim that CNNs are just a special case of MLPs . ",
    "url": "/docs/Deep%20Learning/DL.html#convolutional-neural-nets",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#convolutional-neural-nets"
  },"42": {
    "doc": "Deep Learning Starter",
    "title": "Difference between FCNs and CNNs",
    "content": ". Ans. Because each neuron in the hidden layer is connected to every matrix in the input layer, The size required would be 1M * 1M = 1e12 weights. Which is too much! . How do we fix this huge weight params issue? . | Sparsity through local receptive fields: Think of it as a feature detector (like edge detectors) which only looks at a 10x10 region This will effectively make number of weights required as 1M * 10*10 = 100M . | Weight Sharing: If we set the weights of all the above 10x10 receptive fields as the same, then we’ll need just 100 weights . | . Now, you can just call this an MLP (Multi-Layer Perceptron) with sparsity and weight sharing! . Q. Now, if the weights are shared for each receptive field, wouldn’t that make each receptive field learn the same kind of features? How can we improve this? . Ans. | Instead of using the one 10x10 receptive field which has 100 weights and looks over the whole image, we can instead have 10 different 10x10 receptive fields (each has 100 weights) which look over the image in the same way. i.e. we will have 10 convolutional filters . | And the total number of weights will still be low (100 * 10 = 1000 weights) . | . ",
    "url": "/docs/Deep%20Learning/DL.html#difference-between-fcns-and-cnns",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#difference-between-fcns-and-cnns"
  },"43": {
    "doc": "Deep Learning Starter",
    "title": "Convolutional Layers",
    "content": "Now, let’s define how exactly these convolutional filters work! . Firstly, remember that even in PyTorch we define the shape of an image as NCHW . | N = number of images in one batch | C = number of channels | H,W = height and width of the image | . Note. Usually the filters to start with a small receptive field (that’s why 3x3) and as the network grows deeper, even if we continue using 3x3, because of the ‘downsampling’ nature of the conv operations, we will end up increasing our receptive field. This is also similar story to how the deformable parts model with smaller templates was better than having 1000s of large templates . Therefore, it wouldn’t make sense to start off with a really large filter! However it is now common practice to have 5x5 filters in the first layer, and then 3x3 filters in all deeper layers. This is just emperical… . Now, if we add a bias term and as we discussed above add multiple filters (like a filter bank), we get the following image: . ",
    "url": "/docs/Deep%20Learning/DL.html#convolutional-layers",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#convolutional-layers"
  },"44": {
    "doc": "Deep Learning Starter",
    "title": "Shift Invariance",
    "content": ". | If we zero pad an image in the first layer, the neuron in layer2 would see that the blue triangle is closer to the image boundary. So you may think it starts associating that feature to a particular location in the image. | However, as we go deeper and deeper into the network, that same blue triangle will be more an more towards the center. The rest of the image will be all zeros . | In this case, the neurons see zeros in most places | Therefore it can be said that padding actually does not allow the neurons to continuously learn any positional (aka spatial) dependence for the features. This is what helps generate shift invariance! | . ",
    "url": "/docs/Deep%20Learning/DL.html#shift-invariance",
    
    "relUrl": "/docs/Deep%20Learning/DL.html#shift-invariance"
  },"45": {
    "doc": "Deep Learning Starter",
    "title": "Deep Learning Starter",
    "content": ". | Before you Begin | Era before DL . | Deformable Parts Model . | Naieve Way | Issues with HOG | Deformable Templates | Conditional Independence | Actual Training/Inference Steps: | . | . | Connecting Deformable Parts Model to Deep Learning | Getting into DL . | Thinking of a feature extractor network as a big patch descriptor | Simple Math on Fully Connected Networks | . | Convolutional Neural Nets . | Difference between FCNs and CNNs . | Q. Now, if the weights are shared for each receptive field, wouldn’t that make each receptive field learn the same kind of features? How can we improve this? | . | Convolutional Layers | Shift Invariance | . | . ",
    "url": "/docs/Deep%20Learning/DL.html",
    
    "relUrl": "/docs/Deep%20Learning/DL.html"
  },"46": {
    "doc": "Deep Learning",
    "title": "Deep Learning",
    "content": "Introduction and Overview of DL as done in 11-785 IDL and 16720 Computer Vision class . ",
    "url": "/docs/Deep%20Learning",
    
    "relUrl": "/docs/Deep Learning"
  },"47": {
    "doc": "EKF",
    "title": "Extended Kalman Filter",
    "content": "We’ll be using EKF to solve Non-linear online SLAM. Note. Full SLAM = no marginalization (i.e. we optimize over all robot states). Online SLAM is what we do in EKF where we only care about the previous robot state and marginalize out all the older states. Here, marginalization is just a way of integrating probability density functions to get a certainty in pose estimates of past poses. This works fine if the pose estimates were good to begin with. This also cannot handle anything like loop closures. Detailed write-up . ",
    "url": "/docs/SLAM/EKF.html#extended-kalman-filter",
    
    "relUrl": "/docs/SLAM/EKF.html#extended-kalman-filter"
  },"48": {
    "doc": "EKF",
    "title": "Introduction",
    "content": "Here we’ll use landmarks already known to us from the dataset (landmark poses) in 2D space. Hence our localization would also be in 2D. The below image shows the robot poses and the landmarks (in green). Since we localize in 2D our robot state space would also need to be in 2D. The robot’s pose can therefore be just x, y, and yaw. Similar to the particle filter, here too we will have two steps in the algorithm: . | Prediction Step - uses motion model | Update Step - uses sensor model | . However, the update step in this case will depend on which landmarks we observe and will only update the pose estimates of the robot and those landmarks alone. The sensor model uses a laser rangefinder to give the landmark position in robot frame (which is later converted into global coordinates) . ",
    "url": "/docs/SLAM/EKF.html#introduction",
    
    "relUrl": "/docs/SLAM/EKF.html#introduction"
  },"49": {
    "doc": "EKF",
    "title": "Predicting Robot State",
    "content": "Since we localize both landmarks and the robot pose, the state vector must contain both information. The above state vector captures robot pose and landmark position in global coordinates. Robot pose has three variables (x,y,theta) and landmark position has two variables (x,y) for each landmark. This results in a large state vector which is one of the drawbacks of EKF. ",
    "url": "/docs/SLAM/EKF.html#predicting-robot-state",
    
    "relUrl": "/docs/SLAM/EKF.html#predicting-robot-state"
  },"50": {
    "doc": "EKF",
    "title": "Motion Model - Used to Predict where robot will be in next timestep",
    "content": "This is simply a non-linear state space equation: . Refresher . | Linear: f(x) = Ax + b | Non_Linear: f(x) = g(x) . | Here we do Non-Linear with some noise: f(x) = g(x) + noise | Here we’ll call f(x) as p(x) becuase we’re getting robot pose | . Core Idea . Previously, we used an odometry motion model in particle filters. This model is used here only for comparison and explanation and we will be using something slightly different. Generally there are two forms of expressions for motion models: . | Closed form calculation | Sampling | . | Calculates the robot’s pose as a probability using the previous state as a prior | Calculates multiple poses where the robot might land based on the noise in model | . | | . In Particle Filters, we needed estimates of the where each particle would land and we therefore had to use sampling. However, for EKF we need estimates of how noisy the estimated robot pose would be. Hence, we will kinda be using a closed form type. 2D Robot Motion Model . | The robot will be constrained to only move in one axis (x-axis) shown as d_t in the robot’s body frame. However, it will be allowed to rotate along it’s central axis by alpha. | We will also add noise to motion in all three axis as e_x, e_y, and e_alpha | . Using Motion Model in Prediction . Here we mathematically define the prediction step used in EKF . Noise Free Prediction . Prediction with Uncertainty . We will now represent the prediction step as a non-linear function g(x,u) with some added noise . ",
    "url": "/docs/SLAM/EKF.html#motion-model---used-to-predict-where-robot-will-be-in-next-timestep",
    
    "relUrl": "/docs/SLAM/EKF.html#motion-model---used-to-predict-where-robot-will-be-in-next-timestep"
  },"51": {
    "doc": "EKF",
    "title": "Predicting Landmark State",
    "content": "Given the range r and bearing β readings from a laser rangefinder, we can estimate the location of landmarks in the global frame given a known robot state. We will use this as our measurement prediction model h(p_t, β, r) . Therefore the landmark predictions are mathematically defined as: . The uncertainty is modelled separately as white noise (normally distributed with zero mean) . ",
    "url": "/docs/SLAM/EKF.html#predicting-landmark-state",
    
    "relUrl": "/docs/SLAM/EKF.html#predicting-landmark-state"
  },"52": {
    "doc": "EKF",
    "title": "EKF Algorithm",
    "content": "The main function of the EKF algorithm helps decrease the uncertainty in our state vector (robot and landmark poses). Firstly, lets understand how the uncertainty in poses is captured. Remember the motion model and sensor model. We saw that noise was included in both cases. The motion model noise is called control noise or process noise and the sensor model noise is called measurement noise . Since both control and measurement noise affects the robot state, it is easy to maintain a combined covariance matrix P . The below equation shows how the state vector relates to the mean and covariance matrices. In the EKF algorithm we will update parts of the above covariance matrix in different steps: . | The above image shows mean and covariance. We find the mean using the non-linear functions of the motion and sensor model | Given Control Reading : we update the Σ_xx primarily plus the first row and first column elements since they depend on x_t | Given Sensor Reading : we update the whole covariance matrix (assuming we see all landmarks in each sensor reading) | . ",
    "url": "/docs/SLAM/EKF.html#ekf-algorithm",
    
    "relUrl": "/docs/SLAM/EKF.html#ekf-algorithm"
  },"53": {
    "doc": "EKF",
    "title": "Algorithm Overview",
    "content": ". ",
    "url": "/docs/SLAM/EKF.html#algorithm-overview",
    
    "relUrl": "/docs/SLAM/EKF.html#algorithm-overview"
  },"54": {
    "doc": "EKF",
    "title": "Setup and Initialization - (Required to be completed before EKF)",
    "content": "Previously we saw the following image of what our state vector and covariance matrix would look . Note: state_vector = µ . covariance = Σ . The same structre is followed in code where we will have X as our state vector and P as our covariance matrix . # Generate variance from standard deviation sig_x2 = sig_x**2 sig_y2 = sig_y**2 sig_alpha2 = sig_alpha**2 sig_beta2 = sig_beta**2 sig_r2 = sig_r**2 # Open data file and read the initial measurements data_file = open(\"../data/data.txt\") line = data_file.readline() fields = re.split('[\\t ]', line)[:-1] \"\"\" The data file is extracted as arr and is given to init_landmarks below The data file has 2 types of entries: 1. landmarks [β_1 r_1 β_2 r_2 · · · ], where β_i , r_i correspond to landmark 2. control inputs in form of [d, alpha] (d = translation along x-axis) \"\"\" arr = np.array([float(field) for field in fields]) measure = np.expand_dims(arr, axis=1) t = 1 # Setup control and measurement covariance control_cov = np.diag([sig_x2, sig_y2, sig_alpha2]) measure_cov = np.diag([sig_beta2, sig_r2]) # Setup the initial pose vector and pose uncertainty # pose vector is initialized to zero pose = np.zeros((3, 1)) pose_cov = np.diag([0.02**2, 0.02**2, 0.1**2]) \"\"\" measure = all landmarks measure_cov = known sensor covariance pose = initialized to (0,0) pose_cov = how much we trust motion model = fixed \"\"\" k, landmark, landmark_cov = init_landmarks(measure, measure_cov, pose, pose_cov) # basically H_t in for-loop of pg 204 print(\"Orig K is\", k) # Setup state vector X by stacking pose and landmark states # X = [x_t, y_t, thetha_t, landmark1(range), landmark1(bearing), landmark2(range)...] X = np.vstack((pose, landmark)) # Setup covariance matrix P by expanding pose and landmark covariances \"\"\" - The covariance matrix for a state vector = [x,y,thetha] would be 3x3 - However, since we also add landmarks into the state vector, we need to add that as well - Since there are 2*k landmarks, we create a new matrix encapsulating pose_cov and landmark_cov - this new cov matrix (constructed by np.block) is: [[pose_cov, 0 ], [ 0, landmark_cov]] \"\"\" P = np.block([[pose_cov, np.zeros((3, 2 * k))], [np.zeros((2 * k, 3)), landmark_cov]]) print(\"Init covariance \\n\", np.round(P, 2)) . Landmark Initialization - Finding Landmark Pose Covariance . In the above code of setting up the state vector and covariance matrix, we see the following lines: . k, landmark, landmark_cov = init_landmarks(measure, measure_cov, pose, pose_cov) # basically H_t in for-loop of pg 204 # of probabalistic robotics # covariance matrix P = np.block([[pose_cov, np.zeros((3, 2 * k))], [np.zeros((2 * k, 3)), landmark_cov]]) . Here we see that the function init_landmarks is giving us the landmark_cov and k (k = no. of landmarks). Let’s take a look at this function to see it’s working. We will need to find the covariance of the landmarks by using the Sensor Model . def init_landmarks(init_measure, init_measure_cov, init_pose, init_pose_cov): ''' NOTE: Here we predict where the landmark may be, based on bearing and range sensor readings 1. Number of landmarks (k) 2. landmark states (just position (2k,1)) which will get stacked onto robot pose 3. Covariance of landmark pose estimations (see theory) input1 init_measure : Initial measurements of form (beta0, l0, beta1,...) (2k,1) input2 init_measure_cov: Initial covariance matrix (2, 2) per landmark given parameters. input3 init_pose : Initial pose vector of shape (3, 1). input4 init_pose_cov : Initial pose covariance of shape (3, 3) given parameters. return1 k : Number of landmarks. return2 landmarks : Numpy array of shape (2k, 1) for the state. return3 landmarks_cov : Numpy array of shape (2k, 2k) for the uncertainty. ''' k = init_measure.shape[0] // 2 landmark = np.zeros((2*k, 1)) landmark_cov = np.zeros((2*k, 2*k)) x_t = init_pose[0][0] y_t = init_pose[1][0] theta_t = init_pose[2][0] # to find the covaraince of all landmark poses, we need to iterate over each landmark # and start filling in landmark_cov array initialized above. (we'll fill in diagonal # components only) for l_i in range(k): # l_i is the i'th landmark # init_measure.shape = (2k,1) beta = init_measure[l_i*2][0] l_range = init_measure[l_i*2 + 1][0] # need to find landmark location in global coords (l_x, l_y) to find H_l l_x = x_t + (float(l_range * np.cos(beta+theta_t))) l_y = y_t + (float(l_range * np.sin(beta+theta_t))) landmark[l_i*2][0] = l_x landmark[l_i*2+1][0] = l_y # Note, L here is the derivative of (l_x,l_y) vector (sensor model) w.r.t beta and theta # G_l is the derivative of same sensor model w.r.t state varialbes (x,y,theta) L = np.array([[float(-l_range* np.sin(beta+theta_t)), float(np.cos(beta+theta_t))], [float(l_range* np.cos(beta+theta_t)), float(np.sin(beta+theta_t))]]) # G_l represents the robot pose aspect of landmark measurement # therefore when measuring covariance, it will use robot pose covariance G_l = np.array([[1, 0, float(-l_range * np.sin(theta_t + beta))], [0, 1, float(l_range * np.cos(theta_t + beta))]]) # See theory, L below was derived w.r.t to measurement. Therefore, # during covariance calculation it will use measurement_covariance # Similarly, G defined w.r.t state variables (x,y,theta) therefore uses pose_covariance pred_landmark_cov = (G_l @ init_pose_cov @ G_l.T) + (L @ init_measure_cov @ L.T) assert(pred_landmark_cov.shape == (2,2)) landmark_cov[l_i*2:l_i*2+2, l_i*2:l_i*2+2] = pred_landmark_cov return k, landmark, landmark_cov . ",
    "url": "/docs/SLAM/EKF.html#setup-and-initialization---required-to-be-completed-before-ekf",
    
    "relUrl": "/docs/SLAM/EKF.html#setup-and-initialization---required-to-be-completed-before-ekf"
  },"55": {
    "doc": "EKF",
    "title": "Main Loop of Algorithm",
    "content": "We’ll follow the structure mentioned below: . ├── EKF Main Loop ├── Prediction Step Theory ├── Prediction Step Code | ├── Update Step Theory └── Update Step Code . Once we have setup our state vector and covariance matrix, we start the series of prediction and update steps. This is done in code as follows: . # Core loop: sequentially process controls and measurements for line in data_file: fields = re.split('[\\t ]', line)[:-1] arr = np.array([float(field) for field in fields]) ########### Prediction (aka Control step) ############ if arr.shape[0] == 2: print(f'{t}: Predict step') d, alpha = arr[0], arr[1] control = np.array([[d], [alpha]]) X_pre, P_pre = predict(X, P, control, control_cov, k) ########### Measurement (aka Update step) ############ else: print(f'{t}: Update step') measure = np.expand_dims(arr, axis=1) X, P = update(X_pre, P_pre, measure, measure_cov, k) last_X = X t += 1 . Prediction Step . After we initialize the state vector and covariance matrix, we can then proceed to the next time step in our data file. At this stage we will be making predictions and updating the state vector. Finding the Covariance . | Step 4 of the EKF Algorithm is the predicted covariance based on some odometry reading. | However, this predicted covariance has two parts. Robot pose covariance and Landmark covariance | Landmark covariance was initialized in the previous step. Hence, only the robot pose covariance and any covariance associated with robot pose will be updated. This translates to only the first row and first column of the covariance matrix getting updated. See cov matrix for why first row and column | . Prediction Step Code . Now, we will only be predicting the robot pose (landmarks are initialized once and only updated in sensor measurements i.e. update step) . def predict(X, P, control, control_cov, k): ''' NOTE: Here we predict only the robot's new state (new state's mean and covariance) \\param X State vector of shape (3 + 2k, 1) stacking pose and landmarks. \\param P Covariance matrix of shape (3 + 2k, 3 + 2k) for X. \\param control Control signal of shape (2, 1) in the polar space that moves the robot. \\param control_cov Control covariance shape (3, 3) in the (x, y, theta) space. \\param k Number of landmarks. \\return X_pre Predicted X state of shape (3 + 2k, 1). \\return P_pre Predicted P covariance of shape (3 + 2k, 3 + 2k). ''' # TODO: Predict new position (mean) using control inputs (only geometrical, no cov here) theta_curr = X[2][0] d_t = control[0][0] # control input in robot's local frame's x-axis alpha_t = control[1][0] P_pred = deepcopy(P) pos_cov = deepcopy(P[0:3,0:3]) X_pred = np.zeros(shape=X.shape) # update only robot pose (not landmark pose) X_pred[0][0] += float(d_t*np.cos(theta_curr)) X_pred[1][0] += float(d_t*np.sin(theta_curr)) X_pred[2][0] += float(alpha_t) X_pred = X_pred + X # TODO: Predict new uncertainity (covariance) using motion model noise, find G_t and R_t # NOTE: G_t needs to be mulitplied with P viz of shape (3 + 2k, 3+ 2k), because it has # pose and measurement cov. IN THIS STEP OF PREDICTION WE ONLY UPDATE POSE COV # Therefore G_t and R_t can be 3x3 (3 variables in state vector) G_t = np.array([[1, 0, float(-d_t * np.sin(theta_curr))], [0, 1, float(d_t * np.cos(theta_curr))], [0, 0, 1 ]]) rotation_matrix_z = np.array([[float(np.cos(theta_curr)), -float(np.sin(theta_curr)), 0], [float(np.sin(theta_curr)), float(np.cos(theta_curr)), 0], [ 0, 0, 1]]) pose_pred_cov = (G_t @ pos_cov @ G_t.T) + \\ (rotation_matrix_z @ control_cov @ rotation_matrix_z.T) # update just the new predicted covariance in robot pose, measurement pose is left untouched P_pred[0:3,0:3] = pose_pred_cov return X_pred, P_pred . Update Step - A Comparison Step . The prediction step helps us get an esitmate of where the robot and landmarks should be. Using this estimate of where the robot and landmarks are located, we check the difference between what sensor reading we expect to get at these estimated poses (predicted sensor reading) and where the robot actually is (actual sensor reading) . The method to predict a sensor reading given an estimated robot and landmark pose is by defining a measurement model. Measurement Model Definition . Goal: Given robot state, predict what will be the sensor reading . Using state vector p_t (contains robot pose and landmark) for the j’th landmark, the bearing β and range r estimate for the j’th landmark is predicted as h(p_t , j) . Where h(p_t , j) = measurement model . Now, since the measurement model is non-linear, we will do the ‘E’ part of ‘EKF’. i.e. we will find the 1st order Taylor expansion of the non-linear measurement function: . This jacobian will comprise of two parts: . | | | All put together | . Final Comparison Step . The comparison step happens in the final parts of the algorithm shown below: . Here we see that we compare the predicted covariance Σ with H_t. Specifically the second equation above is clear in it’s purpose and is explained below: . | µ = µ̄ + K(z - ẑ) | The RHS part of this equation shows how we compare the real sensor reading z and the predicted sensor reading ẑ | The variable K then acts as a scaling factor only | . Update Step in Code . def update(X_pre, P_pre, measure, measure_cov, k): ''' NOTE: Using predicted landmark &amp; robot pose, we emulate our sensor reading using the sensor model \\param X_pre Predicted state vector of shape (3 + 2k, 1) from the predict step. \\param P_pre Predicted covariance matrix of shape (3 + 2k, 3 + 2k) from the predict step. \\param measure Measurement signal of shape (2k, 1). \\param measure_cov Measurement covariance of shape (2, 2) per landmark given the parameters. \\param k Number of landmarks. \\return X Updated X state of shape (3 + 2k, 1). \\return P Updated P covariance of shape (3 + 2k, 3 + 2k). Since we have a measurement, we will have to update both pose and measure covariances, i.e. the entire P_pre will be updated. Here we use the H_p and H_l described in the theory section. H_l and H_p will be combined to form H_t (the term in the EKF Algorithm in Probablistic Robotics). This H_t term will be defined for each landmark and stored in a massive matrix Q viz measurement covariance will need to be added to the H_t of each landmark, therefore it too will also be stored in a huge diagonal matrix ''' # Q needs to be added to (Ht @ P_pre @ (Ht.T)) = (2*k, 2*k), therefore must be same shape Q = np.zeros(shape=(2*k, 2*k)) # stack all predicted measurements into one large vector z_t = np.zeros(shape=(2*k, 1)) # H_t as discussed above will be a large diagonal matrix where we'll stack H_p and H_l # side-by-side horizontally (making H_t 2x5 for each landmark). This will then be stacked # vertically, but again as a diagonal matrix. # H_t.T will also be multiplied with P_pre (3+2k, 3+2k). Therefore this needs to # also have 3+2k columns therefore the other dimensions should be 2k rows since # (H_p concat with H_l) = 2x5. Therefore, final H_t shape = 2k,3+2k H_t = np.zeros(shape=(2*k, 3+(2*k))) # iterate through every measurement, assuming every measurement captures every landmark num_measurements = k for i in range(num_measurements): # since we have a predicted pose already X_pre[0:3] we'll use that as our # linearization point # define the predicted pose of robot and landmark in global frame pos_x = X_pre[0][0] # robot pose_x in global frame pos_y = X_pre[1][0] # robot pose_y in global frame pos_theta = X_pre[2][0] # bearing in global frame l_x = X_pre[3+i*2][0] # landmark i in global frame l_y = X_pre[4+i*2][0] # landmark i in global frame # convert predicted poses to local frame l_x_offset = l_x - pos_x l_y_offset = l_y - pos_y # use predicted pose of robot and landmark to get predicted measurements i_bearing = warp2pi(np.arctan2(l_y_offset, l_x_offset) - pos_theta) # bearing of i-th l i_range = math.sqrt(l_x_offset**2 + l_y_offset**2) # range of i-th landmark z_t[2*i][0] = i_bearing z_t[2*i+1][0] = i_range # Jacobian of measurement function (h(β,r) in theory) w.r.t pose (x,y,theta) # Note here we define h(β,r), whereas in theory it is h(r,β), hence rows are interchanged H_p = np.array([[(-l_x_offset/i_range) , (-l_y_offset/i_range), 0], [(l_y_offset/(i_range**2)), (-l_x_offset/(i_range**2)), -1],], dtype=np.float64) # Note here we define h(β,r) H_l = np.array([[(l_x_offset/i_range) , (l_y_offset/i_range) ], [(-l_y_offset/(i_range**2)), (l_x_offset/(i_range**2))]]) # See theory how H_t is constructed. H_p goes only along the first three columns H_t[2*i : 2*i+2, 0:3] = H_p H_t[2*i : 2*i+2, 3+2*i : 5+2*i] = H_l Q[i*2:i*2+2, i*2:i*2+2] = measure_cov # Now after obtaining H_t and Q_t, find Kalman gain K # K = (3+2k, 3+2k) @ (3+2k, 2k) @ (2k, 2k) = (3+2k, 2k) K = P_pre @ H_t.T @ np.linalg.inv((H_t @ P_pre @ H_t.T) + Q) # Update pose(mean) and noise(covariance) using K X_updated = np.zeros(shape=X_pre.shape) X_updated = X_pre + (K @ (measure - z_t)) # (measure - z_t) = (actual - prediction) P_updated = (np.eye(2*k+3) - (K @ H_t)) @ P_pre return X_updated, P_updated . ",
    "url": "/docs/SLAM/EKF.html#main-loop-of-algorithm",
    
    "relUrl": "/docs/SLAM/EKF.html#main-loop-of-algorithm"
  },"56": {
    "doc": "EKF",
    "title": "Overview of Implementation",
    "content": ". Visual Interpretation . . ",
    "url": "/docs/SLAM/EKF.html#overview-of-implementation",
    
    "relUrl": "/docs/SLAM/EKF.html#overview-of-implementation"
  },"57": {
    "doc": "EKF",
    "title": "Detailed Notes and Derivation",
    "content": "Detailed Derivation . ",
    "url": "/docs/SLAM/EKF.html#detailed-notes-and-derivation",
    
    "relUrl": "/docs/SLAM/EKF.html#detailed-notes-and-derivation"
  },"58": {
    "doc": "EKF",
    "title": "EKF",
    "content": "{: .text-delta } . | Extended Kalman Filter | Introduction | Predicting Robot State . | Motion Model - Used to Predict where robot will be in next timestep . | Refresher | Core Idea | 2D Robot Motion Model | Using Motion Model in Prediction . | Noise Free Prediction | Prediction with Uncertainty | . | . | . | Predicting Landmark State | EKF Algorithm . | Algorithm Overview | Setup and Initialization - (Required to be completed before EKF) . | Landmark Initialization - Finding Landmark Pose Covariance | . | Main Loop of Algorithm . | Prediction Step . | Finding the Covariance | . | Prediction Step Code | Update Step - A Comparison Step | Measurement Model Definition | Final Comparison Step | Update Step in Code | . | Overview of Implementation . | Visual Interpretation | . | Detailed Notes and Derivation | . | . ",
    "url": "/docs/SLAM/EKF.html",
    
    "relUrl": "/docs/SLAM/EKF.html"
  },"59": {
    "doc": "Linear Algebra in Eigen",
    "title": "Background",
    "content": "Eigen is the numpy equivalent in C++. Here we look at some basic linear algebra computations using eigen. Credits: . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#background",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#background"
  },"60": {
    "doc": "Linear Algebra in Eigen",
    "title": "Installation",
    "content": "To install eigen3 we can use the apt repository. | sudo apt update | sudo apt install libeigen3-dev | Verify your installation by doing dpkg -L libeigen3-dev | . Then to use in code you simply need to include the following header file and work within the following namespace: . #include &lt;eigen3/Eigen/Dense&gt; using namespace Eigen; . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#installation",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#installation"
  },"61": {
    "doc": "Linear Algebra in Eigen",
    "title": "Declaring and Defining Matrices",
    "content": " ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#declaring-and-defining-matrices",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#declaring-and-defining-matrices"
  },"62": {
    "doc": "Linear Algebra in Eigen",
    "title": "Basics",
    "content": "Here we’ll define a 3x3 matrix in two equivalent ways: . #include &lt;iostream&gt; #include &lt;eigen3/Eigen/Dense&gt; using namespace std; using namespace Eigen; int main() { // define 3x3 matrix -explicit declaration Matrix &lt;float, 3, 3&gt; matrixA; matrixA.setZero(); cout &lt;&lt; matrixA &lt;&lt;endl; // define 3x3 matrix -typedef declaration Matrix3f matrixA1; matrixA1.setZero(); cout &lt;&lt;\"\\n\"&lt;&lt;matrixA1&lt;&lt;endl; // Dynamic Allocation -explicit declaration Matrix &lt;float, Dynamic, Dynamic&gt; matrixB; // Dynamic Allocation -typedef declaration // 'X' denotes that the memory is to be dynamic MatrixXf matrixB1; // constructor method to declare matrix MatrixXd matrixC(10,10); // print any matrix in eigen is just piping to cout cout &lt;&lt; endl &lt;&lt; matrixC &lt;&lt; endl; // resize any dynamic matrix MatrixXd matrixD1; matrixD1.resize(3, 3); matrixD1.setZero(); cout &lt;&lt; endl &lt;&lt; matrixD1 &lt;&lt; endl; return 0; } . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#basics",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#basics"
  },"63": {
    "doc": "Linear Algebra in Eigen",
    "title": "Easier to Remember and Use",
    "content": "int main() { // directly init a matrix of zeros MatrixXf A; A = MatrixXf::Zero(3, 3); cout &lt;&lt; \"\\n \\n\"&lt;&lt; A &lt;&lt; endl; // directly init a matrix of ones MatrixXf B; B = MatrixXf::Ones(3, 3); cout &lt;&lt; \"\\n \\n\"&lt;&lt; B &lt;&lt; endl; // directly init a matrix filled with a constant value MatrixXf C; C = MatrixXf::Constant(3, 3, 1.2); cout &lt;&lt; \"\\n \\n\"&lt;&lt; C &lt;&lt; endl; // directly init identity (eye matrix) MatrixXd D; D = MatrixXd::Identity(3, 3); cout &lt;&lt; \"\\n \\n\" &lt;&lt; D &lt;&lt; endl; MatrixXd E; E.setIdentity(3, 3); cout &lt;&lt; \"\\n \\n\" &lt;&lt; E &lt;&lt; endl; } . Common Bug in above operations . int main() { MatrixXd V; V &lt;&lt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116; cout &lt;&lt; V &lt;&lt; endl; } . | If you try to run the above code it will compile. However, in execution it will segfault. | The reason will be that we did not allocate memory for the matrix V. | . We can fix this by doing the following: . int main() { MatrixXd V; // option 1 V.resize(4,4); // option 2 V = MatrixXd::Zero(4, 4); // best option MatrixXd V(4,4); V &lt;&lt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116; cout &lt;&lt; V &lt;&lt; endl; } . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#easier-to-remember-and-use",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#easier-to-remember-and-use"
  },"64": {
    "doc": "Linear Algebra in Eigen",
    "title": "Explicitly Defining Matrix Entries",
    "content": "We already saw this above, but once we have defined the right shape of the matrix, we can then define it’s entries as shown below: . Note: Entries are interpreted in row-major order . MatrixXd V; V.resize(4,4); V &lt;&lt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116; . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#explicitly-defining-matrix-entries",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#explicitly-defining-matrix-entries"
  },"65": {
    "doc": "Linear Algebra in Eigen",
    "title": "Slicing Matrices",
    "content": "int main() { MatrixXd V = MatrixXd::Zero(4,4); V &lt;&lt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116; cout &lt;&lt; V &lt;&lt; endl; MatrixXd Vblock = V.block(0, 0, 2, 2); cout &lt;&lt; \"\\n \\n\" &lt;&lt; Vblock &lt;&lt; endl; } . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#slicing-matrices",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#slicing-matrices"
  },"66": {
    "doc": "Linear Algebra in Eigen",
    "title": "Extracting Individual Rows and Columns + Find Shape",
    "content": "int main() { MatrixXd V = MatrixXd::Zero(4,4); V &lt;&lt; 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116; MatrixXd row1 = V.row(0); MatrixXd col1 = V.col(0); cout &lt;&lt; row1 &lt;&lt; endl; cout &lt;&lt; col1 &lt;&lt; endl; } . The above is useful in finding the shape of any given matrix (like numpy.shape) . #include &lt;iostream&gt; #include &lt;eigen3/Eigen/Dense&gt; using namespace Eigen; int main() { MatrixXd matrix(3, 4); // Example matrix with 3 rows and 4 columns int numRows = matrix.rows(); int numCols = matrix.cols(); std::cout &lt;&lt; \"Number of rows: \" &lt;&lt; numRows &lt;&lt; std::endl; std::cout &lt;&lt; \"Number of columns: \" &lt;&lt; numCols &lt;&lt; std::endl; return 0; } . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#extracting-individual-rows-and-columns--find-shape",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#extracting-individual-rows-and-columns--find-shape"
  },"67": {
    "doc": "Linear Algebra in Eigen",
    "title": "Matrix Math",
    "content": "Addition . int main() { MatrixXd A1(2, 2); MatrixXd B1(2, 2); A1 &lt;&lt; 1, 2, 3, 4; B1 &lt;&lt; 3, 4, 5, 6; MatrixXd C1 = A1 + B1; cout &lt;&lt; \" \\n\\n The sum of A1 and B1 is\\n\\n\" &lt;&lt; C1 &lt;&lt; endl; } . Matrix Multiplication - Dot Prod and Scalar . Unlike numpy, here the * operator serves as default matrix multiplication . int main() { MatrixXd A1(2, 2); MatrixXd B1(2, 2); A1 &lt;&lt; 1, 2, 3, 4; B1 &lt;&lt; 3, 4, 5, 6; // Dot Product MatrixXd C1 = A1 * B1; // Multiplication by a scalar MatrixXd C2 = 2 * A1; } . Transpose . Don’t do this . int main() { A1 = A1.transpose(); } . Can do this . // the correct and safe way to do the matrix transpose is the following A1.transposeInPlace(); // we can use a transpose operator in expressions R1 = A1.transpose() + B1; . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html#matrix-math",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html#matrix-math"
  },"68": {
    "doc": "Linear Algebra in Eigen",
    "title": "Linear Algebra in Eigen",
    "content": "{: .text-delta } . | Background . | Installation | . | Declaring and Defining Matrices . | Basics | Easier to Remember and Use . | Common Bug in above operations | . | Explicitly Defining Matrix Entries | Slicing Matrices | Extracting Individual Rows and Columns + Find Shape | Matrix Math . | Addition | Matrix Multiplication - Dot Prod and Scalar | Transpose . | Don’t do this | Can do this | . | . | . | . ",
    "url": "/docs/Vision%20with%20C++/Eigen.html",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen.html"
  },"69": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Introduction",
    "content": ". | Here we’ll try to use Eigen alongside opencv to do some basic computer vision | We’ll emulate the eightpoint algorithm used to find the fundamental matrix in multiview geometry | . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#introduction",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#introduction"
  },"70": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Basic Integration of Eigen in C++",
    "content": "I found that installing opencv is quite straightforward with the steps delineated below: . OpenCV on Linux . However, installing Eigen is a bit more tricky. I followed the below steps: . sudo apt update sudo apt install libeigen3-dev dpkg -S libeigen3-dev # only to verify if it's been installed right . However, if you use the following command, you can import eigen simply as: #include Eigen . sudo ln -sf eigen3/Eigen Eigen . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#basic-integration-of-eigen-in-c",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#basic-integration-of-eigen-in-c"
  },"71": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Example Code",
    "content": " ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#example-code",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#example-code"
  },"72": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "C++ File:",
    "content": "#include &lt;iostream&gt; #include &lt;eigen3/Eigen/Dense&gt; #include &lt;opencv2/opencv.hpp&gt; // #include &lt;inlucde/supp_two.hpp&gt; using namespace std; using namespace Eigen; int main() { MatrixXf K1(3,3); MatrixXf K2(3,3); K1 &lt;&lt; 1.5204e+03, 0.0000e+00, 3.0232e+02, 0.0000e+00, 1.5259e+03, 2.4687e+02, 0.0000e+00, 0.0000e+00, 1.0000e+00; K2 &lt;&lt; 1.5204e+03, 0.0000e+00, 3.0232e+02, 0.0000e+00, 1.5259e+03, 2.4687e+02, 0.0000e+00, 0.0000e+00, 1.0000e+00; cout &lt;&lt; \"the expected fundamental matrix should be\" &lt;&lt; endl; cv::Mat im1; cv::Mat im2; im1 = cv::imread(\"/home/sush/CMU/Assignment_Sem_1/CV_A/Assignment_4/code/data/im1.png\", 1); im2 = cv::imread(\"/home/sush/CMU/Assignment_Sem_1/CV_A/Assignment_4/code/data/im2.png\", 1); /* cv::namedWindow(\"Display Image\", cv::WINDOW_AUTOSIZE); cv::imshow(\"Display Image\", im1); cv::waitKey(0); */ Eigen::MatrixXi pts1(10, 2); Eigen::MatrixXi pts2(10, 2); pts1 &lt;&lt; 157, 231, 309, 284, 157, 225, 149, 330, 196, 316, 302, 273, 159, 324, 158, 137, 234, 340, 240, 261; pts2 &lt;&lt; 157, 211, 311, 279, 157, 203, 149, 334, 197, 318, 305, 268, 160, 327, 157, 140, 237, 346, 240, 258; cout &lt;&lt; pts1 &lt;&lt; endl; cout &lt;&lt; pts2 &lt;&lt; endl; return 0; } . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#c-file",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#c-file"
  },"73": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Header File - (place in include directory in the same folder as the .cpp file)",
    "content": "#ifndef SUPP_TWO_HPP #define SUPP_TWO_HPP #include &lt;eigen3/Eigen/Dense&gt; // Define and initialize the matrix extern Eigen::MatrixXi pts1(110, 2); extern Eigen::MatrixXi pts2(110, 2); #endif . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#header-file---place-in-include-directory-in-the-same-folder-as-the-cpp-file",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#header-file---place-in-include-directory-in-the-same-folder-as-the-cpp-file"
  },"74": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "CMakeLists.txt",
    "content": "cmake_minimum_required(VERSION 2.8) project( eightpt ) find_package( OpenCV REQUIRED ) include_directories( ${OpenCV_INCLUDE_DIRS}) add_executable( eightpt q2_1_in_cpp.cpp ) target_link_libraries( eightpt ${OpenCV_LIBS} ) . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#cmakeliststxt",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#cmakeliststxt"
  },"75": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Execution",
    "content": "Run cmake . and make in the same level as the .cpp file . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html#execution",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html#execution"
  },"76": {
    "doc": "Eigen, OpenCV, and Images",
    "title": "Eigen, OpenCV, and Images",
    "content": "{: .text-delta } . | Introduction | Basic Integration of Eigen in C++ | Example Code . | C++ File: | Header File - (place in include directory in the same folder as the .cpp file) | CMakeLists.txt | Execution | . | . ",
    "url": "/docs/Vision%20with%20C++/Eigen_applied.html",
    
    "relUrl": "/docs/Vision%20with%20C++/Eigen_applied.html"
  },"77": {
    "doc": "Expectation and Covariance",
    "title": "Expectation",
    "content": "It is a basically a weighted average (which is for discrete variables) for a continuous distribution . | If a random variable can have discrete outcomes, the probability of each outcome is weighted and an average is taken | In the continuous distribution sense, this becomes an integral each event of a random variable (x) and it’s probability (p(x)) | . The above image shows some examples of how alpha(constant) and x(random varible) are computed However, there are a few basic properties to understand for all the Kalman Filters and Particle Filters we will study: . | E[alpha + x] = alpha + E[x] | E[x,y] (called Joint Expectation) | (called Conditional Expectation) | E[x + y] = E[x] + E[y] (derived below) | | . ",
    "url": "/docs/SLAM/Expectation_and_cov.html#expectation",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#expectation"
  },"78": {
    "doc": "Expectation and Covariance",
    "title": "Correlation and Uncorrelation",
    "content": "Uncorrelation and Joint Expectation . Above we mentioned Joint Expectation as E([x,y]. Now the only way we know x and y are uncorrelated here is if E[x,y] = = E[x]*E[y] (as x and y are clearly independent random variables) . However, the inverse is not valid (i.e. if we only know they are uncorrelated, we cannot state independece like above equation) . Example to show that uncorrelation does not mean independence . ",
    "url": "/docs/SLAM/Expectation_and_cov.html#correlation-and-uncorrelation",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#correlation-and-uncorrelation"
  },"79": {
    "doc": "Expectation and Covariance",
    "title": "Connecting Variance to Expectation (I think it’s important!)",
    "content": ". ",
    "url": "/docs/SLAM/Expectation_and_cov.html#connecting-variance-to-expectation-i-think-its-important",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#connecting-variance-to-expectation-i-think-its-important"
  },"80": {
    "doc": "Expectation and Covariance",
    "title": "Covariances and Thinking Vectors",
    "content": "If we have a simple vector equation of the form: . Then for an equation of the form: y = Ax + b , we can find the covariance in terms of vector x as: . ",
    "url": "/docs/SLAM/Expectation_and_cov.html#covariances-and-thinking-vectors",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#covariances-and-thinking-vectors"
  },"81": {
    "doc": "Expectation and Covariance",
    "title": "Projecting Multivariate covariance",
    "content": "If z = f(x,y), then the covariance of z can be expressed as: . ",
    "url": "/docs/SLAM/Expectation_and_cov.html#projecting-multivariate-covariance",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#projecting-multivariate-covariance"
  },"82": {
    "doc": "Expectation and Covariance",
    "title": "Important Learning",
    "content": ". ",
    "url": "/docs/SLAM/Expectation_and_cov.html#important-learning",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#important-learning"
  },"83": {
    "doc": "Expectation and Covariance",
    "title": "Properties of Covariance Matrix",
    "content": ". Properties of PSD . Note: In the second property below, he means if A is positive semi definite and B is positive definite only then the sum will be positive definite. (At least one of them should be PSD and other PD) . ",
    "url": "/docs/SLAM/Expectation_and_cov.html#properties-of-covariance-matrix",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#properties-of-covariance-matrix"
  },"84": {
    "doc": "Expectation and Covariance",
    "title": "Correltation Coefficient",
    "content": ". ",
    "url": "/docs/SLAM/Expectation_and_cov.html#correltation-coefficient",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html#correltation-coefficient"
  },"85": {
    "doc": "Expectation and Covariance",
    "title": "Expectation and Covariance",
    "content": "{: .text-delta } . | Expectation . | Correlation and Uncorrelation . | Uncorrelation and Joint Expectation | Example to show that uncorrelation does not mean independence | . | . | Connecting Variance to Expectation (I think it’s important!) | Covariances and Thinking Vectors . | Projecting Multivariate covariance | Important Learning | Properties of Covariance Matrix . | Properties of PSD | . | . | Correltation Coefficient | . Once we understand how probability density functions (PDFs) work, we can extend this to understand expectation . ",
    "url": "/docs/SLAM/Expectation_and_cov.html",
    
    "relUrl": "/docs/SLAM/Expectation_and_cov.html"
  },"86": {
    "doc": "MLPs (IDL1)",
    "title": "Before you Begin",
    "content": "Ref: 11-785 . ",
    "url": "/docs/Deep%20Learning/IDL1.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#before-you-begin"
  },"87": {
    "doc": "MLPs (IDL1)",
    "title": "Multi-Layer Perceptrons Basics",
    "content": "These are machines that can model any function in the world! For now, let’s start with simple functions like boolean gates and build our way up. The basic working is shown below: . ",
    "url": "/docs/Deep%20Learning/IDL1.html#multi-layer-perceptrons-basics",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#multi-layer-perceptrons-basics"
  },"88": {
    "doc": "MLPs (IDL1)",
    "title": "Perceptron as a boolean gate",
    "content": ". | Each perceptron seen above is a an addition gate | The sum is computed, and the threshold value is given by the number inside the circle | Therefore, the number dictates what type of gate it functions as | . Recap types of gates: . Andrej Reference . | Add gate | Max gate | Multiply gate | . XOR Gate . These gates are activated only if the inputs are (1,0) or (0,1). These are bit tricky and need to be modelled with a network of perceptrons: . Therefore, it can be seen that combining MLPs in such a manner, one can say that MLPs are universal boolean functions . We can also claim that any boolean function can be modelled with just 1 hidden layer . Reason: . ",
    "url": "/docs/Deep%20Learning/IDL1.html#perceptron-as-a-boolean-gate",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#perceptron-as-a-boolean-gate"
  },"89": {
    "doc": "MLPs (IDL1)",
    "title": "Why do we need depth?",
    "content": "Let’s take a slightly difficult case (say an XOR) . However, if we model the same with XORs depthwise, we get: . ",
    "url": "/docs/Deep%20Learning/IDL1.html#why-do-we-need-depth",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#why-do-we-need-depth"
  },"90": {
    "doc": "MLPs (IDL1)",
    "title": "Perceptrons as Linear Classifiers",
    "content": "If we have 2 boolean inputs, we can have 4 combinations: . | (0,0) | (0,1) | (1,0) | (1,1) | . Now, using an OR gate, NOT Y gate, XOR gate we can model some basic classifiers: . Note. clearly the XOR needs to boundaries (we call these decision boundaries) Therefore, we say that the XOR cannot be modelled with just one perceptron . Complex Decision Boundaries . If we create multiple decision boundaries, we can do the following: . | Find output of each decision boundary (i.e. does my point lie to the left or right of decision boundary) | The above step happens in the hidden layer | Then we can cumulate these decision boundary inputs | From below fig. notice that only if sum == 5, the final neuron fires | . This way, we can model complex geometries, even complex ones like: . Another case for depth . Now, consider the above double pentagon figure. What if we were to do it using just one layer? . We would have to approximate it using cylindrical regions (basically polygons with large number of sides, say 1000 sides) . We can then use this cylinder decision boundary (multiples of them) to sort of make up our double pentagon as shown below: . But as seen above, the major drawback is that the first layer will have an infinite number of neurons! . Now, comparing our depthwise vs spanwise solutions: . ",
    "url": "/docs/Deep%20Learning/IDL1.html#perceptrons-as-linear-classifiers",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#perceptrons-as-linear-classifiers"
  },"91": {
    "doc": "MLPs (IDL1)",
    "title": "Sufficiency of Architecture",
    "content": ". | A network arch is sufficient (i.e. sufficiently braod and sufficiently deep) it can represent any function. | Conversely if a network is not sufficient, it can miss out on information, and this lack of information can be propagated deeper causing major loss of information . In the above image, if the red lines our the first layer, the information passed to the second layer is that we are in those tiny diamond regions. However, we have no idea where we are in those diamonds. (This is loss of info to the next layer!) . To mitigate this loss, instead of doing hard thresholding, we can use softer decision boundaries as shown below: . | . ",
    "url": "/docs/Deep%20Learning/IDL1.html#sufficiency-of-architecture",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#sufficiency-of-architecture"
  },"92": {
    "doc": "MLPs (IDL1)",
    "title": "Further on MLPs",
    "content": "Include bias as an input for simplifying downstream computations . | Bias as a separate term | Bias included in input | . | | | . This also helps in simplifying the (z = Wx + b) equation from being affine to a linear form of (z = Wx) . ",
    "url": "/docs/Deep%20Learning/IDL1.html#further-on-mlps",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#further-on-mlps"
  },"93": {
    "doc": "MLPs (IDL1)",
    "title": "Proceeding from simple boolean functions",
    "content": ". | We cannot handcraft our network like how we did for the double pentagon | Therefore, we need a learnable method | Also, most real functions are very complex and don’t have nice visualizations like the double pentagon | Therefore, we also need a way of learning such complex functions with only few samples and not relying on continuous data | We do this by a sampling approach, where we calculate the error for every sample in our training data | . ",
    "url": "/docs/Deep%20Learning/IDL1.html#proceeding-from-simple-boolean-functions",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#proceeding-from-simple-boolean-functions"
  },"94": {
    "doc": "MLPs (IDL1)",
    "title": "The Perceptron algorithm",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL1.html#the-perceptron-algorithm",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#the-perceptron-algorithm"
  },"95": {
    "doc": "MLPs (IDL1)",
    "title": "Why is the perceptron algorithm not good?",
    "content": "The primary issue is that the simple perceptron is flat and non-differentiable. Data is never fully clean . We mostly never have nicely linearly separable data . The solution: Differentiable activations . Now, making this activation differentiable has two benefits: . | Let’s us know if our changes is having a positive or negative effect on prediction | It allows us to do backprop! | . ",
    "url": "/docs/Deep%20Learning/IDL1.html#why-is-the-perceptron-algorithm-not-good",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#why-is-the-perceptron-algorithm-not-good"
  },"96": {
    "doc": "MLPs (IDL1)",
    "title": "Thinking about Derivatives",
    "content": ". | Instead of thinking of derivatives as dy/dx where if we have y and x as vectors, dividing them would not make much sense, instead we define it as y' = alpha*x', where alpha is now a vector and alpha*x’ can be though of as a dot product. Therefore, this alpha will now define the vector which when dot product with x gives the direction of the fastest increase in y. | Adavantage of doing it as y' = alpha*x' now is that for a multivariate form like above, we can write the alpha vector as a partial derivate of y with x. | Now, we can clearly see how the gradient gives the direction of fastest increase in in the function. Therefore, if we want to minimize, we go in the direction exactly opposite to the gradient. | . ",
    "url": "/docs/Deep%20Learning/IDL1.html#thinking-about-derivatives",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html#thinking-about-derivatives"
  },"97": {
    "doc": "MLPs (IDL1)",
    "title": "MLPs (IDL1)",
    "content": "{: .text-delta } . | Before you Begin | Multi-Layer Perceptrons Basics . | Perceptron as a boolean gate . | Recap types of gates: | XOR Gate | . | Why do we need depth? | Perceptrons as Linear Classifiers . | Complex Decision Boundaries | Another case for depth | . | Sufficiency of Architecture | . | Further on MLPs . | Include bias as an input for simplifying downstream computations | Proceeding from simple boolean functions | The Perceptron algorithm | Why is the perceptron algorithm not good? . | The primary issue is that the simple perceptron is flat and non-differentiable. | Data is never fully clean | The solution: Differentiable activations | . | . | Thinking about Derivatives | . ",
    "url": "/docs/Deep%20Learning/IDL1.html",
    
    "relUrl": "/docs/Deep%20Learning/IDL1.html"
  },"98": {
    "doc": "Classifiers (IDL2)",
    "title": "Before you Begin",
    "content": "Ref: 11-785 . ",
    "url": "/docs/Deep%20Learning/IDL2.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#before-you-begin"
  },"99": {
    "doc": "Classifiers (IDL2)",
    "title": "Binary Classifiers and Cross Entropy Loss",
    "content": "This is usually used for classification. For a binary case, it is given by the following formula: . Notice that the equation uses the term divergence here. This is actually the background term for ‘loss’ in our situation. Divergence tells us how off we are from the correct solution. Note. divergence is not direction dependent, it just tells how far away we are from the desirable output. More formally, loss = average divergence of our output w.r.t the ground truth . Therefore, in a binary setting, if we use softmax as our activation function -&gt; we get the class probablity score as the output. In the binary case we get (y, 1-y) as our output. | let y = output of softmax for each class (we have 2 classes) | let d = ground truth | Now, plugging in (y, 1-y) into the Cross Entropy loss formula above | We see that when y = 0 and when y = 1, since log(0) = -infinity and log(1) = 0 | Therefore, if d=0 and y=1, we get infinity, if d=1 and y=0 we also get infinity | . Now it’s also interesting to observe the derivative of cross entropy loss function. The derivative is shown below: . Now, notice the following cases: . | Case 1: When d=1 and y=1, plugging into the above formula, we get derivative(CE_loss) = -1 | Case 2: When d=0 and y=0, plugging into above forumula we get derivative(CE_loss) = 1 | Note, if you assumed that if output(y) = desired(d) would have zero gradient, you’re wrong! | The above two cases are plotted below | | Case 1 | Case 1 and Case 2 | . | | | . | . However, instead of cross entropy loss, if we were to use a simple L2 error (sum of sqaured diffs (quadratic function)) we would get a bowl shaped instead like: . An extract from the Xavier Intitialization paper shows this more accurately . From the above picture, one can see that the cross entropy loss surface (black) is much steeper than the quadratic surface (red) . ",
    "url": "/docs/Deep%20Learning/IDL2.html#binary-classifiers-and-cross-entropy-loss",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#binary-classifiers-and-cross-entropy-loss"
  },"100": {
    "doc": "Classifiers (IDL2)",
    "title": "Why is Cross Entropy better than L2?",
    "content": "Ans. The L2 is a quadratic loss function, which is smooth bowl. Now from the above picture, you can see that doing gradient descent on L2 would take so much longer than using it on the steeper curve of the cross entropy loss! . ",
    "url": "/docs/Deep%20Learning/IDL2.html#why-is-cross-entropy-better-than-l2",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#why-is-cross-entropy-better-than-l2"
  },"101": {
    "doc": "Classifiers (IDL2)",
    "title": "Multi-Class Cross Entropy",
    "content": ". Here we only have y_i (i.e. one class which we’re looking for in our loss function) Therefore, the derivative will look different as seen above . The problem with the above definition of CE (cross entropy) is that derivative of loss for all other classes is zero. Which isn’t desirable for fast convergence. Therefore, we slightly modify the labels ‘d’ as shown below: . ",
    "url": "/docs/Deep%20Learning/IDL2.html#multi-class-cross-entropy",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#multi-class-cross-entropy"
  },"102": {
    "doc": "Classifiers (IDL2)",
    "title": "Label Smoothening",
    "content": "Here we change our target label to (1-(K-1)*e) instead of just being 1 . ",
    "url": "/docs/Deep%20Learning/IDL2.html#label-smoothening",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#label-smoothening"
  },"103": {
    "doc": "Classifiers (IDL2)",
    "title": "Simple 2 layer network (beautiful diagram!)",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL2.html#simple-2-layer-network-beautiful-diagram",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#simple-2-layer-network-beautiful-diagram"
  },"104": {
    "doc": "Classifiers (IDL2)",
    "title": "Backprop",
    "content": ". | Derivative w.r.t to the loss was already computer in previous section. | Now, derivative w.r.t the activation function is shown below . Example: Sigmoid Derivative . Example: Tanh Derivative . Example: Logistic Derivative . Note. tanh is a scaled and shifted version of sigmoid. This is shown below by just rearranging some terms: . | Computing derivate w.r.t one weight (one weight connects one neuron in layer N-1 to another neuron in layer 2) . | Computing the derivative w.r.t y (y = output of activation function) Here, one neuron will have effect on all the neurons in the next layer. This is why we need to sum the derivates of z (z = wx + b of next layer) w.r.t y(from previous layer) (This is explained better in the Scalar vs Vector activations) . | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#backprop",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#backprop"
  },"105": {
    "doc": "Classifiers (IDL2)",
    "title": "Special Cases",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL2.html#special-cases",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#special-cases"
  },"106": {
    "doc": "Classifiers (IDL2)",
    "title": "Scalar vs Vector activations",
    "content": "We assumed activation to be neuron specific. However, this may not be the case! . Also, the backprop gets bit murky as well . | Scalar Activation | Vector Activation | . | | | . Note. The important aspect to remember is that for a vector activation, the derivative of divergence w.r.t any input (input to activation func) is a sum of partial derivative on every neuron of activation function as seen in picture above . Example of a vector activation: softmax activation . ",
    "url": "/docs/Deep%20Learning/IDL2.html#scalar-vs-vector-activations",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#scalar-vs-vector-activations"
  },"107": {
    "doc": "Classifiers (IDL2)",
    "title": "Sub-gradients",
    "content": "For RELU’s, the origin is not smooth and the gradient cannot be computed. Instead we use a sub-gradient which is shown as multiple lines in the figure below. However, we just use the sub-gradient line which is parallel to the x-axis and define the gradient as = 1 at origin . ",
    "url": "/docs/Deep%20Learning/IDL2.html#sub-gradients",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#sub-gradients"
  },"108": {
    "doc": "Classifiers (IDL2)",
    "title": "Training Process",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL2.html#training-process",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#training-process"
  },"109": {
    "doc": "Classifiers (IDL2)",
    "title": "Vector Formulation",
    "content": "Forward Pass . In the below picture, the first row of weights vector represents all of the weights going to the first neuron. Backward Pass . Now, if z = output of affine function (wx + b) and f(z) = output of activation Having vectorized activations will cause the below backprop through y = f(z) . The Jacobian will therefore be the multivariant form of gradient, giving us the direction in which incresing delta(z) will cause the max increase in delta(y) . Rule of thumb: the derivative of [y(scalar) = f(z(matrix))] = matrix.T shape Extension: the derivative of scalar(row_vector) = column vector . Special Cases . ",
    "url": "/docs/Deep%20Learning/IDL2.html#vector-formulation",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#vector-formulation"
  },"110": {
    "doc": "Classifiers (IDL2)",
    "title": "Backward pass summary",
    "content": ". | For the first gradient, we calculate the fastest increase in Y which gets the fastest increase in loss, where loss is given as . The derivative therefore becomes . Where Y = column vector, therefore derivative of Y w.r.t Divergence(loss) = delta(Y)*Div shown in picture above. This grad(y) is a row vector! . | Now we compute derivative through affine variable z = wx+b, then we do . | Now, derivative w.r.t previous Y (Y(n-1)) will be . | Remember, as we go back we just post multiply by: . | A jacobian if it’s an activation layer (vector activation) | A weight matrix for an affine layer (scalar activation) | . | Now, two more things tbd are derivatives on weights and biases! | Since bias should be the same size as our vector activation (z) therefore the defivative w.r.t the bias is . | Similarly, the derivative of the weights is given by which will have the same shape as the weight matrix . | Remember, all the shapes of the derivatives should match the shapes of the weights or the bias itself . | | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#backward-pass-summary",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#backward-pass-summary"
  },"111": {
    "doc": "Classifiers (IDL2)",
    "title": "Loss Surface",
    "content": ". | The common hypothesis is that in large networks there are lot more saddle points than global minima. | A saddle point is defined as one where moving in one direction increases loss and moving in other direction decreases. I.e. depending on which direciton you’re looking at you can be at a minima or maxima. Also the slope at saddle points is zero (therefore, you’ll get stuck with gradient descent) | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#loss-surface",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#loss-surface"
  },"112": {
    "doc": "Classifiers (IDL2)",
    "title": "Issues with Convergence",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL2.html#issues-with-convergence",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#issues-with-convergence"
  },"113": {
    "doc": "Classifiers (IDL2)",
    "title": "Convergence for a Convex Problem",
    "content": "Consider a simple quadratic case . Note. Optimizing w.r.t the second order is called Newton’s method . Multivariate Convex . Now, the A matrix will introduce different slopes in different axes since it’s multivariate. To mitigate this we do . Now, you see that the A matrix has been removed (think of it as having become identity) . Then as we saw in our derivation above, the optimal step size will then become inverse(A) = inverse(I) = 1. Therefore, the optimal step size is now the same in all dimensions . The math for the above steps . Points to note . | In the simple scalar quadratic space the optimal step size was one value | In the multivariate space, we need to scale and then find the optimal step | However, after scaling we can still achieve single step move to global minima even in multivariate space, we just need to find the inverse of a matrix | . General Case of Convex Functions (function has higher order, i.e. not a quadratic) . | Even in such cases we can find Taylor expansions and just truncate upto second order | In such cases it’ll just be an approximation, but let’s live with that | | Here we see that the second derivative is replaced by a Hessian | Therefore in this case, the optimum step size would be the inverse(Hessian) | The normalized and optimal update step in gradient descent form is shown below | . Issues with the above process . Solutions . . In the momentum method, the first term in RHS is the scaled term of previous weight update being addes to the current update step. | Big red vector = previous update step | Blue vector = 2nd term of RHS above | Small red vector = scaled version of big red vector | black vector = final update (LHS term) | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#convergence-for-a-convex-problem",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#convergence-for-a-convex-problem"
  },"114": {
    "doc": "Classifiers (IDL2)",
    "title": "SGD vs Batch GD",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL2.html#sgd-vs-batch-gd",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#sgd-vs-batch-gd"
  },"115": {
    "doc": "Classifiers (IDL2)",
    "title": "SGD",
    "content": "In SGD we update after every training instance. The caveat for convergence is that the increments should be small and not too large. The increments should also shrink so that we don’t keep shifting around the decision boundary too much due to to just one training instance. If we define epsilon to be the margin by which we need to be within to have ‘converged’, then using the above optimal learning rate of (1/k) where k = no. of layers, we see that after one iteration we should be within (1/k)*desired_range. Therefore if we only need to be epsilon*desired range, we can reach it in O(1/epsilon) . ",
    "url": "/docs/Deep%20Learning/IDL2.html#sgd",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#sgd"
  },"116": {
    "doc": "Classifiers (IDL2)",
    "title": "Batch GD",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL2.html#batch-gd",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#batch-gd"
  },"117": {
    "doc": "Classifiers (IDL2)",
    "title": "Problems with SGG",
    "content": ". | If our job was to minimize the shaded area in the below picture (shaded area = divergence) then, we would want to push the red line up or down (blue = ground truth) | If we look at the curve at it’s current location, we would want to move the red curve down drastically. | In the below picture, we would want to push up our red curve drastically | Therefore, the problem becomes that the estimated loss and subsequent update has too high a variance! | However, despite all this SGD is fast since it works only on 1 sample at a time | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#problems-with-sgg",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#problems-with-sgg"
  },"118": {
    "doc": "Classifiers (IDL2)",
    "title": "Middle Ground Solution: Mini Batches",
    "content": "Here we compute the average loss over a mini-batch and use this averaged loss for update . But how does the variance of mini-batch compare to that of full-batch gradient descent? . | Variance of mini-batch GD where b = batch size is: . | Variace of full-batch GD will be (1/N) instead of (1/b) . | Now, if we have 1000 training samples, it can be seen that 1/100 is small enough that it won’t make that much of a difference if it’s 1/100 or 1/1000. This is why mini-batching works, i.e. even with 100 samples we capture almost the same variance as we would if we took all training samples into consideration . | . ",
    "url": "/docs/Deep%20Learning/IDL2.html#middle-ground-solution-mini-batches",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html#middle-ground-solution-mini-batches"
  },"119": {
    "doc": "Classifiers (IDL2)",
    "title": "Classifiers (IDL2)",
    "content": "{: .text-delta } . | Before you Begin | Binary Classifiers and Cross Entropy Loss . | Why is Cross Entropy better than L2? | . | Multi-Class Cross Entropy . | Label Smoothening | . | Simple 2 layer network (beautiful diagram!) . | Backprop | . | Special Cases . | Scalar vs Vector activations . | Example of a vector activation: softmax activation | . | Sub-gradients | . | Training Process . | Vector Formulation . | Forward Pass | Backward Pass . | Special Cases | . | . | Backward pass summary | Loss Surface | . | Issues with Convergence . | Convergence for a Convex Problem . | Multivariate Convex . | The math for the above steps | . | General Case of Convex Functions (function has higher order, i.e. not a quadratic) . | Issues with the above process | Solutions | . | . | . | SGD vs Batch GD . | SGD | Batch GD | Problems with SGG | Middle Ground Solution: Mini Batches | . | . ",
    "url": "/docs/Deep%20Learning/IDL2.html",
    
    "relUrl": "/docs/Deep%20Learning/IDL2.html"
  },"120": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Before you Begin",
    "content": "Ref: 11-785 . ",
    "url": "/docs/Deep%20Learning/IDL3.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#before-you-begin"
  },"121": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Improving over momentum update",
    "content": "Previously we saw how the derivatives change in subsequent steps (as we did in simple momentum) and take a step considering the weighted average of the current and prior step (actual implementation was a running average) . Now, we’ll consider the way in which these derivatives change (this is called second moment) which takes care of the variance in graident shifts. The second moment can be implemented as shown below. As seen in our prior image, since we had high variation along y and low variation along x, we will do: . ",
    "url": "/docs/Deep%20Learning/IDL3.html#improving-over-momentum-update",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#improving-over-momentum-update"
  },"122": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Commonly Used methods which use Second Moment",
    "content": "RMS Prop . Here, let’s do a running average like simple momentum, but do it on the second derivative of the gradient. The gamma value is just a weighting factor between prior step’s gradient (k-1) and (1-gamma) is the weight applied to the current step’s gradient: . Now, the way we will include this is our update will be to normalize the learning rate using this second second moement: . Just for comparison, this is how the update step for simple momentum only scaled the preious step’s weight magnitude and did not touch learning rate. ADAM (RMSprop with momentum) . The reason first and second moments are scaled by the weighting factor is to ensure that in the beginning of training, we don’t let sigma and gamma terms to dominate (it’ll slow us down) . ",
    "url": "/docs/Deep%20Learning/IDL3.html#commonly-used-methods-which-use-second-moment",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#commonly-used-methods-which-use-second-moment"
  },"123": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Batch Normalization",
    "content": "Problem with covarite shifts . Solution to covariate shifts . ",
    "url": "/docs/Deep%20Learning/IDL3.html#batch-normalization",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#batch-normalization"
  },"124": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Batch Norm Theory",
    "content": ". | We do this covariate shifts typically at the at location of the affine sum (Wx + b) | | | The above step (first yellow box) will cause all training instances to have mean = 0 and variance = 1 | Now, we move the entire data to a separate appropriate location (second yellow box) as defined by gamma and beta. | How do we get this gamma and beta? . | Ans. They are only learnt, we don’t define or derive them (initialize them to 0 or 1 and let them be learned) | | . | . Q. Why is batch_norm applied before the activation function? Ans. It’s debatable. But if it’s used after activation some activations may get reveresed maybe? . Note. Understand vocab: Difference between Normalization and Standardization . Now, its nice to see data having low variance. However, the real issue arises when we try to do backprop. ",
    "url": "/docs/Deep%20Learning/IDL3.html#batch-norm-theory",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#batch-norm-theory"
  },"125": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Backprop through Batch Norm",
    "content": "Conventional backprop happens by taking a derivative of the divergence function as shown below: . However, after batch norm, it gets tricky since our divergence will now depend on not only the mini-batch (training samples of mini-batch), but will now also depend on the mean and variance of the entire mini-batch (since our mini-batch was scaled and shifted according to the mean and variance) . Derivation . The derivation is shown below: . ",
    "url": "/docs/Deep%20Learning/IDL3.html#backprop-through-batch-norm",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#backprop-through-batch-norm"
  },"126": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Batch Norm in Test Time",
    "content": "Here also we need some estimate of variance as to where this test image belongs to. We do so by using a running average over the training batches. ",
    "url": "/docs/Deep%20Learning/IDL3.html#batch-norm-in-test-time",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#batch-norm-in-test-time"
  },"127": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Overfitting",
    "content": ". We essentially need a way to smoothen the above curve such that it fills in the gap nicely. There are several ways of doing this, but the most common ones are: . ",
    "url": "/docs/Deep%20Learning/IDL3.html#overfitting",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#overfitting"
  },"128": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Smoothness through weight manipulation",
    "content": "Think of the sigmoid function . Now, if the value of our input (x) increases a lot, the curve changes from a nice smooth curve to something a lot more steep: . (here w = our input (x)) . Therefore simply constraining the weight to be low will ensure the perceptron output is smooth. ",
    "url": "/docs/Deep%20Learning/IDL3.html#smoothness-through-weight-manipulation",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#smoothness-through-weight-manipulation"
  },"129": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Smoothness through weight constraints (Regularization)",
    "content": "This is basically regularization (where we ensure model is penalized for large weights) . Now, this is also easy to backprop as shown below: . ",
    "url": "/docs/Deep%20Learning/IDL3.html#smoothness-through-weight-constraints-regularization",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#smoothness-through-weight-constraints-regularization"
  },"130": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Smoothness through network structure",
    "content": ". | As we saw in the MLP section on why depth matters, each layer of an MLP imposes constraints, i.e. each layer creates some decision boundary. | See why we need depth . | In the picture below, after the first layer, we know that our input is in either a pentagon region of a triangle region (but we don’t know where inside it!) . | . | Therefore, deeper models have a natural tendency to restricting shapes they can model and this gives the natural smoothness required. | . Example for further clarity . In the above example, the earlier layers have really bad fit shapes. As we go deeper the smoothness naturally increased. ",
    "url": "/docs/Deep%20Learning/IDL3.html#smoothness-through-network-structure",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#smoothness-through-network-structure"
  },"131": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Dropout",
    "content": ". | During Train time, each neuron is active such that: (number_of_instances_neuron_is_active/total_number_of_instances) = alpha . i.e. if the chance of a neuron being active is say 0.7 (then alpha = 0.7) . | . | By following above steps, The effective network is different for different sets of inputs. Additionally, the graidents are also updated differently . | Like any Bernoulli Distribution, each event has 2 outcomes. Therefore a statistical interpretation would yield the below picture: . | . | I think it also serves as a form of augmentation, where instead of blacking out certain parts of the image, we make the object recognizable only at certain receptive fields . | Dropout also has the tendency of removing redundancies in learning. i.e. the network learns a cat even if it doesn’t have a tail, or if it doens’t have pointy ears . | . Implementing Dropout during Training . The dropout is added onto the activation layer as an additional (like an if condition) constraint. It is shown below . Now, we will use this alpha value in our test time everywhere . Implementing Dropout during Inference . | We could add alpha (the bernoulli factor) to the activation of every neuron (just like train time) | Or we could mulltiply every weight with alpha (we will effectively be blocking out connections instead of neurons) | Instead of applying alpha as chance of a neuron being active during train time, use inverse of alpha. Then during test time, we just don’t use alpha at all!! | . ",
    "url": "/docs/Deep%20Learning/IDL3.html#dropout",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#dropout"
  },"132": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Augmentation",
    "content": ". | Mosaicing | Flipping | Rotating | Blurring | Warp (Distort the image) | . ",
    "url": "/docs/Deep%20Learning/IDL3.html#augmentation",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#augmentation"
  },"133": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Other Tricks",
    "content": ". | Normalize the input (covariate shifts in next section) | Xavier Initialization | . ",
    "url": "/docs/Deep%20Learning/IDL3.html#other-tricks",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html#other-tricks"
  },"134": {
    "doc": "Optimizers and Regularizers (IDL3)",
    "title": "Optimizers and Regularizers (IDL3)",
    "content": "{: .text-delta } . | Before you Begin | Improving over momentum update . | Commonly Used methods which use Second Moment . | RMS Prop | ADAM (RMSprop with momentum) | . | . | Batch Normalization . | Problem with covarite shifts | Solution to covariate shifts | Batch Norm Theory | Backprop through Batch Norm . | Derivation | . | Batch Norm in Test Time | . | Overfitting . | Smoothness through weight manipulation | Smoothness through weight constraints (Regularization) | Smoothness through network structure . | Example for further clarity | . | Dropout . | Implementing Dropout during Training | Implementing Dropout during Inference | . | . | Augmentation | Other Tricks | . ",
    "url": "/docs/Deep%20Learning/IDL3.html",
    
    "relUrl": "/docs/Deep%20Learning/IDL3.html"
  },"135": {
    "doc": "Intro to CNNs",
    "title": "Before you Begin",
    "content": "Ref: 11-785 . ",
    "url": "/docs/Deep%20Learning/IDL4.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#before-you-begin"
  },"136": {
    "doc": "Intro to CNNs",
    "title": "Simple method of achieving shift invariance",
    "content": "Assume we have a simple MLP which was trained to identify a flower. Now, if we run the MLP blindly on the image, we will NOT have shift invariance. A simple solution would be to scan the image at different positions and take the region which gave the max output of an activation (say largest softmax class score). ",
    "url": "/docs/Deep%20Learning/IDL4.html#simple-method-of-achieving-shift-invariance",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#simple-method-of-achieving-shift-invariance"
  },"137": {
    "doc": "Intro to CNNs",
    "title": "Important Backprop Theory",
    "content": ". | Now, to train this, the initial 3 layers are seen to have the same weights (shared weights) | Therefore treat it like a vector function where each and every window affects the final classification head. Now, if we want to backprop through such a function which depends on each and every window, we need to sum over the activations. | Similaraly, the update step will also be such that the updated weights effect each and every one of the input weights equally as shown below: | . ",
    "url": "/docs/Deep%20Learning/IDL4.html#important-backprop-theory",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#important-backprop-theory"
  },"138": {
    "doc": "Intro to CNNs",
    "title": "Summary",
    "content": ". Q. In which layer would you expect to see something that looks like a flower? \\ . Ans. Deeper Layers . Q. Why do we need to distribute the scanning and not have one level of neurons scan the entire window at the same time? . Ans. It reduces the number of learnable parameters . However, to get a better understanding I strongly suggest going through the slides and video links attached below to get a better understanding. | In the references below, understand what the K,N,D,L terms represent | If we consider a frequency spectrum of a voice recording, then considering some timestep we get an input vector of size ‘LxD’ where L = length of recording used and D = height here . | Now, let’s consider a case like this, where input vector of size L (here size 8) are feeding into layer1 which has N1 neurons (4 neurons in below picture) | We’ll use this in a more generic scanning case as shwon below . | From the above picture we calculate that each input vector has 8 timesteps (L) | Each vector has a dimensionality D (think height of the frequency plot image) | Therefore, number of weights connecting the input layer of size LD with 1st layer of size N1 leads to the first term LDN1 | The next term is simply dependent on number of neurons in layer1 and layer2 = N1*N2 | . | . Now that we’ve seen the number of parameters in non-distributed scanning, let’s compare it to an example of non-distributed scanning as shown below: . Clearly, the distributed scanning has more shared computations which gives it fewer parameters. We also have many identical weights as shown below: . | In the above image, only the ones circled in greeen have unique parameters (weights) and the remaining are shared (thereby saving computation) | We also have a notion where saved computation has more gains over having more weights | . Now, if we think of the same logic in Image terms, it’s just changing the dimensions of the vectors (say K changes to a 2D patch which gets flattened to K^2) . Distributed vs Undistributed scanning for images . Note. Sometimes there’s a (K + 1) term. That’s just for the bias I’m assuming. Finally, by doing distributed scanning we see the quantifiable effect as shown below: . Main Intuition . ",
    "url": "/docs/Deep%20Learning/IDL4.html#summary",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#summary"
  },"139": {
    "doc": "Intro to CNNs",
    "title": "Nice take on Max Pooling (Why is it needed?)",
    "content": "When we scan the image, if we find that one pixel which should belong to a petal has been shifted, we somehow have to account for it. A nice solution is to not be too local focused as to where the activation occured, but to just take the max of a small window. By taking the max of the small window say 4x4, we’re effectively not caring as to where the activation in that window occured, as long as it is within 4x4 . Nice consequence of above logic . | A little jitter in images can be expected due to irregularity of objects in the real world. | However, in the speech world, jitter would mess up any phonetics that convey meaning | As a result, in Speech recog there isn’t much max-pooling | . Note. The max pool occurs for each channel (unlike the conv filters which across all channels of the image). Therefore, the output of a maxpool will retain the number of channels. ",
    "url": "/docs/Deep%20Learning/IDL4.html#nice-take-on-max-pooling-why-is-it-needed",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#nice-take-on-max-pooling-why-is-it-needed"
  },"140": {
    "doc": "Intro to CNNs",
    "title": "Convolutional NNs",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL4.html#convolutional-nns",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#convolutional-nns"
  },"141": {
    "doc": "Intro to CNNs",
    "title": "Number of Parameters in a Conv Layer",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL4.html#number-of-parameters-in-a-conv-layer",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#number-of-parameters-in-a-conv-layer"
  },"142": {
    "doc": "Intro to CNNs",
    "title": "Types of Filters",
    "content": ". | Typically the first layer people have used large filter sizes of 5x5 (emperically proven to provide better results in feature extraction) | Most lower levels have smaller filters of 3x3 | Now, there also exists a 3x3 filter. What is that? It’s just a single perceptron | . More on 1x1 Convolution . | Here too we find element-wise products | Then as usual we apply a ReLU | . You can think of it as a single neuron layer which takes a vector input of 32 and has 32 weights which gets multiplied by the input. These then go through an activation like ReLU as well. It’s a fully connected network (single layer perceptron) which takes 32 vector input and outputs 1 number. (Add DEVA’s content here too !!!) . ",
    "url": "/docs/Deep%20Learning/IDL4.html#types-of-filters",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#types-of-filters"
  },"143": {
    "doc": "Intro to CNNs",
    "title": "Importatnt to Remember",
    "content": ". | Number of Parameters in Conv Layer | While it may good to lose information during max-pooling, since it’s primarily to account for jitter and noise and it’s okay to loose that information. However, we also saw in the MLP decision boundaries case, that deeper layers with complete information from previous layer, can learn complex shapes. (Just imagine the MLP if it lost some information from the input layer how our final learnt shape would be?) | . ",
    "url": "/docs/Deep%20Learning/IDL4.html#importatnt-to-remember",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html#importatnt-to-remember"
  },"144": {
    "doc": "Intro to CNNs",
    "title": "Intro to CNNs",
    "content": "{: .text-delta } . | Before you Begin | Simple method of achieving shift invariance . | Important Backprop Theory | Summary . | Q. In which layer would you expect to see something that looks like a flower? \\ | Q. Why do we need to distribute the scanning and not have one level of neurons scan the entire window at the same time? . | Distributed vs Undistributed scanning for images | Main Intuition | . | . | Nice take on Max Pooling (Why is it needed?) . | Nice consequence of above logic | . | . | Convolutional NNs . | Number of Parameters in a Conv Layer | Types of Filters . | More on 1x1 Convolution | . | . | Importatnt to Remember | . ",
    "url": "/docs/Deep%20Learning/IDL4.html",
    
    "relUrl": "/docs/Deep%20Learning/IDL4.html"
  },"145": {
    "doc": "Lessons Learnt 1",
    "title": "Before you Begin",
    "content": "Ref: 11-785 . This course has become quite intense to try and document. I’ll be posting highlights here which are my biggest takeaways . ",
    "url": "/docs/Deep%20Learning/IDL5.html#before-you-begin",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#before-you-begin"
  },"146": {
    "doc": "Lessons Learnt 1",
    "title": "CNN Architechtures Studied",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL5.html#cnn-architechtures-studied",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#cnn-architechtures-studied"
  },"147": {
    "doc": "Lessons Learnt 1",
    "title": "RESNET",
    "content": "Convolutional Neural Nets are really good at extracting features. My understanding on why we use RESNETs: . | It prevents overfitting and limits the non-linearity to the necessary amount by allowing for gradients to skip neurons on the backward pass | It solves the issue of vanishing gradients | . A simple network would have following form: . Now, if we add a skip connection, we get the following structure: . Note. The new activation is g(z + a) . ",
    "url": "/docs/Deep%20Learning/IDL5.html#resnet",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#resnet"
  },"148": {
    "doc": "Lessons Learnt 1",
    "title": "Numbers and Math",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL5.html#numbers-and-math",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#numbers-and-math"
  },"149": {
    "doc": "Lessons Learnt 1",
    "title": "Shared Parameter Network (Thinking wise: Neurons -&gt; Filters)",
    "content": "We previously saw that the number of params of a CNN were much lesser than an MLP. A better understand of why this would be from the below diagram . | The above shows an MLP scanning 1D data. now, the MLP in the raw form above can be seen taking 8 inputs at time. | Additionally, each layer of the MLP is seen to have neurons which do the same work. Therefore, such neurons can be set to hold the same shared parameters (same color) | . If we think of only the most minimalist structure of the MLP which is required (discarding any duplicates nuerons), we can make it a scanning MLP of the below structure: . From the above image the equivalent CNN will have the following structure: . The first hidden layer has 4 filters of kernel-width 2 and stride 2; the second layer has 3 filters of kernel-width 2 and stride 2; the third layer has 2 filters of kernel-width 2 and stride 2 . Also, such a CNN which is moving over 1D input (mostly time) is called a Time Delay Neural Network (TDNN) . Implications . | Lower nubmer of params | Due to shared params, if 4 filters in the lower layer feeds 1 filter above, then the gradient at the higher filter will be equally rerouted (not split!) amongst the 4 lower layer fiters. See the below Andrej’s explanation for a backprop refresher: . | . ",
    "url": "/docs/Deep%20Learning/IDL5.html#shared-parameter-network-thinking-wise-neurons---filters",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#shared-parameter-network-thinking-wise-neurons---filters"
  },"150": {
    "doc": "Lessons Learnt 1",
    "title": "Kernal, Stride, Padding and Output Shape Calculation",
    "content": "The pytorch website has a bad looking equation, I prefer the simple one below . output_shape = (input_shape + 2*padding - kernel)/(stride) + 1 . ",
    "url": "/docs/Deep%20Learning/IDL5.html#kernal-stride-padding-and-output-shape-calculation",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#kernal-stride-padding-and-output-shape-calculation"
  },"151": {
    "doc": "Lessons Learnt 1",
    "title": "Divergence and Loss Functions",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL5.html#divergence-and-loss-functions",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#divergence-and-loss-functions"
  },"152": {
    "doc": "Lessons Learnt 1",
    "title": "L2 Loss",
    "content": ". | The 1/2 as you can see is a scaling factor which makes derivative clean | . Intuition behind the derivative . | d = desired value / label = 1 | y = network output = 1.5 | In the above case, increasing y will increase the loss | Hence, the derivative (think of it like slope here) will be positive (i.e rise/run is positive) | The magnitude of this derivative will also be dependent on error (y - d) | Hence, the derivative formula above = (y - d) and will be positive when y &gt; d | . To do gradient descent, we will therefore go in the negative direction of this derivative by doing: . new_value_of_weights = current_value_of_weights - (derivative_of_divergence * step_size) . ",
    "url": "/docs/Deep%20Learning/IDL5.html#l2-loss",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#l2-loss"
  },"153": {
    "doc": "Lessons Learnt 1",
    "title": "KL Divergence and CE Loss",
    "content": ". Try to think about why we have the log term in this divergence? . Intuition on Loss Function . | In binary classification, we can either be fully correct (d=0 and y=0) or fully wrong(d=0 and y=1) | Now, ideally we don’t want to do anything when we are fully correct | ‘don’t want to do anything’ is just english for make the penalty zero | When we are fully wrong, we want a harsh penalty on the network and ask it to correct itself | ‘harsh penalty’ is english for make the penalty infinite . | Let’s plug in the values and see if the above intuition is applied. Let d = 1 and Y = 0 | Plugging in values, Div = - 1 * log(0), and log(0) = - infinity, therefore Div = + infinity | In the above line, note that because log(0) is negative infinity, that’s why we multiply by -1 in the begging | . Intuition behind Derivative . | d = desired, y = actual output | If d = 1 and y = 0.9, then increasing y will decrease the loss | As you can see above, since increase in y will make loss go lower, the slope would be negative, i.e. derivative will be negative | The above intuition is verified from the equation . | Now if d = 0 and y = 0.9, then increasing y will increase loss. Hence, derivative is positive | . ",
    "url": "/docs/Deep%20Learning/IDL5.html#kl-divergence-and-ce-loss",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#kl-divergence-and-ce-loss"
  },"154": {
    "doc": "Lessons Learnt 1",
    "title": "Why KL over L2?",
    "content": " ",
    "url": "/docs/Deep%20Learning/IDL5.html#why-kl-over-l2",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#why-kl-over-l2"
  },"155": {
    "doc": "Lessons Learnt 1",
    "title": "Activation Functions",
    "content": ". ",
    "url": "/docs/Deep%20Learning/IDL5.html#activation-functions",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#activation-functions"
  },"156": {
    "doc": "Lessons Learnt 1",
    "title": "What is Inductive Bias? How does ReLU have it?",
    "content": "Inductive bias is the inherent, baked-in, bias that some activation functions like ReLU has which has started becoming evident. This is what gave rise to GELU as mentioned in the ConvNext Paper and why it’s gained prominance in vision transformers and modern CNNs. When is it bad to have inductive bias? . | The inductive bias of ReLU is that it prioritizes smooth linear boundaries which is useful when working with very high dimensional data (as it prevents overfitting I assume). | However, if we work on low-dimensional models, which can be simple cases like trying to reconstruct a singular image, or even super useful networks like NERFs | In this RI Seminar by Prof. Simon Lucey he talks about the case where a simple network with ReLU activations is used in an encoder-decoder like fashion to reconstruct one image. | . | The above seems to fail and the reasoning is stated to be the inductive bias of ReLU | . Here’s an example of what happens when we remove the inductive bias by replacing ReLU with another activation function (Gaussians here) which are said to have higher bandwidth (i.e. can represent higher number of frequencies (think of a neural net as giving a signal as an output)) . When is it good to have this inductive bias? . For large networks like vision transformers which are trained on many classes, and a ton of data which should not be overfit, in those cases the inductive bias of ReLU is okay . Bottom Line: Do not use ReLU in combination with any form of position encoding (like in NERFs) . What is the probabalistic interpretation of Sigmoid? . ",
    "url": "/docs/Deep%20Learning/IDL5.html#what-is-inductive-bias-how-does-relu-have-it",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html#what-is-inductive-bias-how-does-relu-have-it"
  },"157": {
    "doc": "Lessons Learnt 1",
    "title": "Lessons Learnt 1",
    "content": "{: .text-delta } . | Before you Begin | CNN Architechtures Studied . | RESNET | . | Numbers and Math . | Shared Parameter Network (Thinking wise: Neurons -&gt; Filters) . | Implications | . | Kernal, Stride, Padding and Output Shape Calculation | . | Divergence and Loss Functions . | L2 Loss . | Intuition behind the derivative | . | KL Divergence and CE Loss . | Intuition on Loss Function | Intuition behind Derivative | . | Why KL over L2? | . | Activation Functions . | What is Inductive Bias? How does ReLU have it? . | When is it bad to have inductive bias? | When is it good to have this inductive bias? | What is the probabalistic interpretation of Sigmoid? | . | . | . ",
    "url": "/docs/Deep%20Learning/IDL5.html",
    
    "relUrl": "/docs/Deep%20Learning/IDL5.html"
  },"158": {
    "doc": "Building this Page",
    "title": "Bulding the Webpage",
    "content": "For Jekyll reference see just_the_docs . The following pages are built in order to understand Computer Vision and Machine Learning . To deploy on heroku follow the steps in the link below (and use the gem files, rake files and proc files in this repo for reference) . The following files will need to be copied from this repo: . | config.ru | Rakefile | Procfile | static.json | config.yaml (modify this file as per requirement) | Gemfile | . And only if necessary: . | Gemfile.lock | remove _sites from .gitignore | . Run bundle once to intialize Run bundle exec jekyll serve Go to the specified webpage by the above command . After copying these files (or their necessary contents), install heroku cli and do heroku login: . curl https://cli-assets.heroku.com/install.sh | sh heroku login . Then directly start with heroku create as per the below link and the other steps necessary (git push heroku master) . Deploy jekyll on heroku . Finally, go to heroku page -&gt; settings -&gt; change the name of the app and find the url . ",
    "url": "/intro/#bulding-the-webpage",
    
    "relUrl": "/intro/#bulding-the-webpage"
  },"159": {
    "doc": "Building this Page",
    "title": "To better your experience of writing in code",
    "content": "Download the following extensions in vscode: . | Markdown All in one | code runner (see youtube video on how to setup vscode for C++) | . ",
    "url": "/intro/#to-better-your-experience-of-writing-in-code",
    
    "relUrl": "/intro/#to-better-your-experience-of-writing-in-code"
  },"160": {
    "doc": "Building this Page",
    "title": "Shortcuts in general pour toi",
    "content": ". | Once Markdown all in one is installed, you can do ctrl+shift+v to see preview of markdown immediately | To run any C++ file it’s just ctrl+shift+n | If you want to bold any specific text in markdown just select the text by holding down ctrl+shift and using arrow keys to select the required text. Then once text is selected just do ctrl+b to bolden and ctrl+i to italicize . | click on tab after using - for normal bullet pointing to get sub-points | . | To get numbered list continuously, in-between two headings 1. and 2. all content should be indented with 4 spaces in the markdown script . | To shift between windows in ubuntu, just do windows_key+shift+right/left_arrow | To minimize or unmaximize any window in hold down alt and press space, then choose to minimize | To then maximize or move window to right half/left half of screen, windows_key+shift+right/left_arrow | . ",
    "url": "/intro/#shortcuts-in-general-pour-toi",
    
    "relUrl": "/intro/#shortcuts-in-general-pour-toi"
  },"161": {
    "doc": "Building this Page",
    "title": "Building this Page",
    "content": " ",
    "url": "/intro/",
    
    "relUrl": "/intro/"
  },"162": {
    "doc": "MRSD Capstone Project",
    "title": "Introduction",
    "content": "I’m currently pursuing my Master’s in Robotic Systems Development at Carnegie Mellon University. Our program is unique in that we emulate the systems engineering process which usually runs in a robotics company. This process runs parallel to our capstone project where we build a robot, secure funding and work within a budget, adhere to a timeline, and deliver on key performance requirements. I’ve been documenting my progress on the project work on a separate website which is accessed by my peers occasionally. ",
    "url": "/mrsd_proj/#introduction",
    
    "relUrl": "/mrsd_proj/#introduction"
  },"163": {
    "doc": "MRSD Capstone Project",
    "title": "Websites",
    "content": "Personal Project Website . Official Website . ",
    "url": "/mrsd_proj/#websites",
    
    "relUrl": "/mrsd_proj/#websites"
  },"164": {
    "doc": "MRSD Capstone Project",
    "title": "MRSD Capstone Project",
    "content": " ",
    "url": "/mrsd_proj/",
    
    "relUrl": "/mrsd_proj/"
  },"165": {
    "doc": "Volume Rendering and NERFs",
    "title": "Introduction",
    "content": "NERF gave us a whole new way of approaching general computer vision tasks like Novel View Synthesis (NVS), 3D Reconstruction, etc. in a more physics informed way. The NeRF is basically an MLP that learns the 3D density and 5D light field of a given scene from image observations and corresponding perspective transforms. Q. What’s 5D Light Field? . Ans. Choosing any ray (i.e. choosing any starting origin and direction) in the scene, we should be able to find the total radiance coming from that ray. Q. What is Radiance? . Ans. Measure of Radiant Energy (joules) per unit time, per unit area, per unit solid angle AKA: The amount of irradiance per unit solid angle . Since the formulation of rendering is crucial to understanding NERFs we’ll do that first. ",
    "url": "/docs/Computer%20Vision/NERF.html#introduction",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#introduction"
  },"166": {
    "doc": "Volume Rendering and NERFs",
    "title": "Part 1 : Volume Rendering",
    "content": " ",
    "url": "/docs/Computer%20Vision/NERF.html#part-1--volume-rendering",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#part-1--volume-rendering"
  },"167": {
    "doc": "Volume Rendering and NERFs",
    "title": "Understanding Absorption and Trasmittance",
    "content": "Absorption Emission Model . Let’s consider an infinitesimal (small) volume through which we have a light ray travelling. Now, this volume can do two things: . | Absorb some intensity of incoming light | Emit some light of it’s own | . For both cases, we see the factor σ this is the absorption coefficient of the volume. Furthermore, we see that both incoming light and light produced in this volume will be affected by this σ . Absorption-only Model . Modelling only absoroption through a non-homogenous volume, we derive the relationship between incoming radiation and outgoing radiation as follows: . Note in above figure, x0 is where the light ray enters the volume. We assume the volume is perfectly oriented in the ray’s direction ω. Then ωz would be the length of the volume along the ω unit vector. That’s why the final radiance output is L(x0 + ωz, ω) . As you can observe, we have a new term here called Transmittance. The intuitive meaning of transmittance is the proportion of incoming light that eventually leaves the volume (gets transmitted). Think of it like absorption is a coefficient (say 0.2) meaning that 20% of all incoming light is absorbed. For transmittance (say 0.8) the intuition would be that 80% of all light is let through the medium. Why The Importance on Transmittance . Trasmittance has some nice properties which simple absorption would not have. Specifically: . | Monotonic Function | Multiplicativity | . In the below picture, we see that even though σ might vary in the volume, the transmittance is always a monotonic function: . Now, previously we saw Transmittance for a non-homogenous medium. It can be easily adapted to a homogenous medium as well as shown below: . Now, above we see that it’s basically an exponential. The multiplicativity property of transmittance is due to the this multiplicativity of exponentials . Using our transmittance terminology above, we finnaly get for absorption only transmittance: . Emission-Absorption Transmittance . As a recap of what was done above, let’s see the basic absorption model . In the above picture note the following: . | The transmittance in vacuum is 1 | Only the cloud has some transmittance value less than 1 | Therefore T(x,x_z) = T_cloud in the above scenario | . Now, lets make some assumptions to go from absorption only model to absorption-emission model: . | Let’s divide the cloud into small sections (small volumes) | Let each volume not only absorb (have transmittance &lt; 1) but also be able to emit light | The final radiation at the eye will be a combination of emission and absorption | . The context above is baked into the picture below: . Ray Marching . Now, the issue with the emission-absorption model is that the integral cannot be solved numerically without some simplifications. We will make the following simplifications: . | Discretize the space into small volumes | Let each small volume have it’s own σ | Our final radiation at the eye/camera will be the summation of each of these small volumes | . . Finally, we see that computing Transmittance is recursive, where the i+1’th segment’s transmittance (T_i+1) = T(i) * T(small volume of i+1) . Practice Transmittance Calculations . Implementing Ray Marching . This will be done in two steps: . 1. Discretize the space into small volumes (just define the sampling points) . import torch from ray_utils import RayBundle from pytorch3d.renderer.cameras import CamerasBase # Sampler which implements stratified (uniform) point sampling along rays class StratifiedRaysampler(torch.nn.Module): def __init__( self, cfg ): super().__init__() self.n_pts_per_ray = cfg.n_pts_per_ray self.min_depth = cfg.min_depth self.max_depth = cfg.max_depth def forward( self, ray_bundle, ): # Compute z values for self.n_pts_per_ray points uniformly sampled between [near, far] z_vals = torch.linspace(self.min_depth, self.max_depth, self.n_pts_per_ray, device=ray_bundle.origins.device) # Sample points from z values \"\"\" NOTE: if image_plane_points.shape = torch.Size([65536, 3]), then rays_origin.shape = torch.Size([65536, 3]) and sample_lenths.shape = torch.Size([65536, 1, 3]) \"\"\" origins_expanded = ray_bundle.origins.unsqueeze(1) # Shape: (N, 1, 3) origins_expanded = origins_expanded.expand(-1, self.n_pts_per_ray, -1) # Shape: (N, D, 3) directions_expanded = ray_bundle.directions.unsqueeze(1) # Shape: (N, 1, 3) directions_expanded = directions_expanded.expand(-1, self.n_pts_per_ray, -1) # Shape: (N, D, 3) z_vals_expanded = z_vals.expand(ray_bundle.origins.shape[0], -1).unsqueeze(-1) # Shape: (1, D, 1) new_sample_points = origins_expanded + z_vals_expanded * directions_expanded # Return return ray_bundle._replace( sample_points=new_sample_points, sample_lengths=z_vals_expanded * torch.ones_like(new_sample_points[..., :1]), # shape = (N, D, 1) ) . 2. Get the density and color of each small volume (This step is Done by the NERF MLP.) . 3. Aggregate the density and color of each small volume to get the final color at the origin of each ray . NOTE: . During training, density is not explicitly trained for. Instead, we check what the final color of a ray should be and compare with what the NERF MLP is telling us. This comparison gives us our loss which will update ray marching. Because we don’t optimize for density directly, that’s why the NeRF Depth output is bad. That’s what prompted people to move to Neural SDFs so that geometry would be better optimized. ",
    "url": "/docs/Computer%20Vision/NERF.html#understanding-absorption-and-trasmittance",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#understanding-absorption-and-trasmittance"
  },"168": {
    "doc": "Volume Rendering and NERFs",
    "title": "Part 2 : NERF Pipeline",
    "content": " ",
    "url": "/docs/Computer%20Vision/NERF.html#part-2--nerf-pipeline",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#part-2--nerf-pipeline"
  },"169": {
    "doc": "Volume Rendering and NERFs",
    "title": "Overview:",
    "content": ". | Define a set of rays (origin, direction): return RayBundle ( rays_origin, rays_d, sample_lengths=torch.zeros_like(rays_origin).unsqueeze(1), sample_points=torch.zeros_like(rays_origin).unsqueeze(1), ) . | Give rays (origin, direction) to StratifiedRaysampler to get the sample points along the rays . | i.e. Do the discretization step of Ray Marching # Sample points along the ray cur_ray_bundle = StratifiedSampler(cur_ray_bundle) . | . | Pass the sample points to the NERF MLP to get the density and color of each sample point . | Note: These sample points are anywhere along the rays (think of it like random camera positions and randomly close to the object) and we want to predict the density and color predictions = NeRF_MLP(cur_ray_bundle) . | . | . The next two steps are a bit involved . | Aggregate the density and color of each sample point to get the final color at the origin of each ray. (the final color of all rays forms the IMAGE!) | Compare the final colors (Image) with the GT image to get the loss | . Check out the forward function in the Volume Renderer class below reference: . Reference . predicted_density_for_all_samples_for_all_rays_in_chunk = NERF_MLP_output['density'] # shape = (self._chunk_size*n_pts, 1) : The density value of that discrete volume predicted_colors_for_all_samples_for_all_rays_in_chunk = NERF_MLP_output['feature'] # shape = (self._chunk_size*n_pts, 3) : Emittance for each discrete volume for RGB channels # Compute length of each ray segment # NOTE: cur_ray_bundle.sample_lengths.shape = (self._chunk_size, n_pts, n_pts) depth_values = cur_ray_bundle.sample_lengths[..., 0] # depth_values.shape = (self._chunk_size, n_pts) # deltas are the distance between each sample deltas = torch.cat( ( depth_values[..., 1:] - depth_values[..., :-1], 1e10 * torch.ones_like(depth_values[..., :1]), ), dim=-1, )[..., None] # Compute aggregation weights (weights = overall transmittance for all rays in the chunk) weights = self._compute_weights( deltas.view(-1, n_pts, 1), # shape = (self._chunk_size, n_pts, 1) predicted_density_for_all_samples_for_all_rays_in_chunk.view(-1, n_pts, 1) # shape = (self._chunk_size, n_pts, 1) ) # TODO (1.5): Render (color) features using weights # weights.shape = (self._chunk_size, n_pts, 1) # color.shape = (self._chunk_size*n_pts, 3) color_of_all_rays = self._aggregate(weights, predicted_colors_for_all_samples_for_all_rays_in_chunk.view(-1, n_pts, 3)) # feature = RGB color # TODO (1.5): Render depth map # depth_values.shape = (self._chunk_size, n_pts) depth_of_all_rays = self._aggregate(weights, depth_values.view(-1, n_pts, 1)) # Return cur_out = { 'feature': color_of_all_rays, 'depth': depth_of_all_rays, } # shape = (N, 3) for feature and (N, 1) for depth . The function compute_weights will find the overall transmittance for each ray and compute_aggregate will use this transmittance to find either color or depth for each ray . def _compute_weights( self, deltas, rays_density: torch.Tensor, eps: float = 1e-10): \"\"\" Args: deltas : distance between each sample (self._chunk_size, n_pts, 1) rays_density (torch.Tensor): (self._chunk_size, n_pts, 1) predicting density values of each sample (from NERF MLP) eps (float, optional): Defaults to 1e-10. Returns: _type_: _description_ \"\"\" # TODO (1.5): Compute transmittance using the equation described in the README num_rays, num_sample_points, _ = deltas.shape transmittances = [] transmittances.append(torch.ones((num_rays, 1)).to(deltas.device)) # first transmittance is 1 #! Find the transmittance for each discrete volume = T(x, x_i) for i in range(1, num_sample_points): # recursive formula for transmittance transmittances.append(transmittances[i-1] * torch.exp(-rays_density[:, i-1] * deltas[:, i-1] + eps)) #! Find = T(x, x_t) * (1 - e^{−σ(x) * δx}) transmittances_stacked = torch.stack(transmittances, dim=1) # the below line implements the T(x, x_t) * (1 - e^{−σ(x) * δx}) part of the equation =&gt; we'll call this 'weights' return transmittances_stacked * (1 - torch.exp(-rays_density*deltas+eps)) # -&gt; weights def _aggregate( self, weights: torch.Tensor, rays_feature: torch.Tensor): \"\"\" Args: weights (torch.Tensor): (self._chunk_size, n_pts, 1) (Overall Transmittance for each ray) rays_feature (torch.Tensor): (self._chunk_size*n_pts, 3) feature = color/depth Returns: feature : Final Attribute (color or depth) for each ray \"\"\" # TODO (1.5): Aggregate (weighted sum of) features using weights feature = torch.sum((weights*rays_feature), dim=1) return feature . Basically, compute_weights finds the T(x, x_t) * (1 - e^{−σ(x) * δx}) part of the equation below: . And _aggreate finds the L(x,ω) which can be color or depth for each ray . ",
    "url": "/docs/Computer%20Vision/NERF.html#overview",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#overview"
  },"170": {
    "doc": "Volume Rendering and NERFs",
    "title": "NeRF Training Loop (Simple)",
    "content": "for iteration, batch in t_range: image, camera, camera_idx = batch[0].values() image = image.cuda().unsqueeze(0) camera = camera.cuda() # Sample rays xy_grid = get_random_pixels_from_image( cfg.training.batch_size, cfg.data.image_size, camera ) ray_bundle = get_rays_from_pixels( xy_grid, cfg.data.image_size, camera ) rgb_gt = sample_images_at_xy(image, xy_grid) # Run model forward out = model(ray_bundle) # TODO (Q3.1): Calculate loss loss = criterion(out['feature'], rgb_gt) # Take the training step. optimizer.zero_grad() loss.backward() optimizer.step() . ",
    "url": "/docs/Computer%20Vision/NERF.html#nerf-training-loop-simple",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html#nerf-training-loop-simple"
  },"171": {
    "doc": "Volume Rendering and NERFs",
    "title": "Volume Rendering and NERFs",
    "content": " ",
    "url": "/docs/Computer%20Vision/NERF.html",
    
    "relUrl": "/docs/Computer%20Vision/NERF.html"
  },"172": {
    "doc": "Least Squares SLAM",
    "title": "Background",
    "content": "In EKF we used the jacobian of the measurement model in the update step. However, upon deeper inspection we see that this matrix is very sparse (spare = lots of zeros) . | It would therefore be most efficient if we could utilize this sparsity. | Additionally, EKF was sort of a local solver in that all prior states were marginalized out. Hence, we could not improve our estimates of our previous robot states even if we got more and more measurements. | . ",
    "url": "/docs/SLAM/Non_linear_slam.html#background",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#background"
  },"173": {
    "doc": "Least Squares SLAM",
    "title": "Learning Goals",
    "content": ". | Learn to optimize operations by exploiting sparsity | Do Full SLAM using the Least Squares Formulation (EKF was only online SLAM where prior states were marginalized out) | . ",
    "url": "/docs/SLAM/Non_linear_slam.html#learning-goals",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#learning-goals"
  },"174": {
    "doc": "Least Squares SLAM",
    "title": "Introduction",
    "content": "Similar to the EKF problem, we setup our state vector to comprise both landmark and robot states. However, here we will make use the Factor Graph formulation which consists of the following components: . | Factors (edges) . | Odometry measurements | Landmark measurements | . | States (nodes) . | Robot poses | Landmark poses | . | . Here we’re already given the data of the all the factors and states present in the factor graph. We will use this data to minimize the variance in the predicted values of each measurement (odometry measurements or landmark measurements) between every two connected states on the factor graph. This minimization will be crafted in a least squares minimization form. The high level procedure to do so is shown below . Finally, the factor graph and least squares equivalence is seen below: . Detailed Derivation Linear Least Sq. Detailed Derivation Non-Linear Least Sq. We will be starting with 2D Linear SLAM and then moving onto 2D Non-Linear SLAM . ",
    "url": "/docs/SLAM/Non_linear_slam.html#introduction",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#introduction"
  },"175": {
    "doc": "Least Squares SLAM",
    "title": "2D Linear Least Squares SLAM",
    "content": "Here we will look to best fit our measurements (z) to our observed robot states (θ). Specifically, we will try to optimize over both the robot states and sensor measurements and try to reach the best middle ground. Factor Graphs or Pose Graphs give us the best method of doing such global optimizations. This global optimization is first defined in the below manner in terms of increasing probability p(z | θ) . Why Least Squares? . The reason we will formulate the above maximization equations as a least squares problem is becuase we get mulitple landmark measurements as our robot moves, ending up with more observations than variables (states in state vector). Hence its not possible to extract an exact solution which fits mathematically. Therefore, our next easiest guess would be a least squares formulation. Think of it like simultaneous equations, if we have 3 variables, we need only 3 equations to solve for the variables. However, if we have 4 equations, it’s overdetermined. Similaraly, here we have only few robot states, but a lot of observations constraining those states. These observations can be odometry readings or sensor readings. ",
    "url": "/docs/SLAM/Non_linear_slam.html#2d-linear-least-squares-slam",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#2d-linear-least-squares-slam"
  },"176": {
    "doc": "Least Squares SLAM",
    "title": "Derivation",
    "content": "The exact derivation can be seen in this link. The abbreviated version is shown below. Now, let’s assume that our measurement function h(x) is linear (Even if it isn’t linear, we approximate the non-linear function by taking the first order taylor expansion later on) . Here we don’t solve directly for Ax = b because of noise in the measurements. Instead, we do the following: . The above equation will lead to: . Intuition for our final solution . ",
    "url": "/docs/SLAM/Non_linear_slam.html#derivation",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#derivation"
  },"177": {
    "doc": "Least Squares SLAM",
    "title": "Measurement Funtion h(x)",
    "content": "Odometry Measurement Function . Landmark Measurment Function . Shapes of the A and b matrices . During the derivation we saw that . | | . | Now, the A matrix is seen to be the same size as the jacobian. We define the jacobian below (it’s equated to A, but it’s yet to be scaled by Σ^-0.5) | b is the same size as the number of measurements z. Hence, h_0 should also be a vector or the same size | . | We see that the rows of the jacobian are p, u1, u2, d1, d2 .. | Since we’re working in 2D, each of these measurements will have an x and y component | p is technically not a measurement, but is the starting pose of the robot | Hence we can define the number of rows of A to be equal to (n_odom + 1) * 2 + n_obs * 2 . | We also see that the b vector has no. of rows = no. of measurements | Therefore size of b = n_poses * 2 + n_landmarks * 2 | . ",
    "url": "/docs/SLAM/Non_linear_slam.html#measurement-funtion-hx",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#measurement-funtion-hx"
  },"178": {
    "doc": "Least Squares SLAM",
    "title": "2D Linear Least Squares in Code",
    "content": "Fully Define the A and b matrices (setup step) . def create_linear_system(odoms, observations, sigma_odom, sigma_observation, n_poses, n_landmarks): ''' \\param odoms Odometry measurements between i and i+1 in the global coordinate system. Shape: (n_odom, 2). \\param observations Landmark measurements between pose i and landmark j in the global coordinate system. Shape: (n_obs, 4). \\param sigma_odom Shared covariance matrix of odometry measurements. Shape: (2, 2). \\param sigma_observation Shared covariance matrix of landmark measurements. Shape: (2, 2). \\return A (M, N) Jacobian matrix. \\return b (M, ) Residual vector. where M = (n_odom + 1) * 2 + n_obs * 2, total rows of measurements. N = n_poses * 2 + n_landmarks * 2, length of the state vector. ''' n_odom = len(odoms) n_obs = len(observations) M = (n_odom + 1) * 2 + n_obs * 2 N = n_poses * 2 + n_landmarks * 2 A = np.zeros((M, N)) b = np.zeros((M, )) # Prepare Sigma^{-1/2}. sqrt_inv_odom = np.linalg.inv(scipy.linalg.sqrtm(sigma_odom)) sqrt_inv_obs = np.linalg.inv(scipy.linalg.sqrtm(sigma_observation)) # TODO: First fill in the prior to anchor the 1st pose at (0, 0) # The prior is just a reference frame, it also has some uncertainty, but no measurement # Hence the measurement function which estimates the prior is just a identity function # i.e h_p(r_t) = r_t. Since no measurements exist, the b matrix will have only zeros (already the case) # Here we also define the uncertainty in prior is same as odom uncertainty A[0:2, 0:2] = sqrt_inv_odom @ np.eye(2) # no need to update b (already zeros) # TODO: Then fill in odometry measurements \"\"\" The A matrix structure is shown in the theory section. Along the rows, it has: - predicted prior (of size 1) - predicted odom measurements (of size n_odom) - predicted landmark measurements (of size n_obs) We will also follow the same order \"\"\" H_odom = np.array([[-1,0,1,0], [0,-1,0,1]], dtype=np.float32) H_land = np.array([[-1,0,1,0], [0,-1,0,1]], dtype=np.float32) A_fill_odom = sqrt_inv_odom @ H_odom for i in range(n_odom): # declare an offset for i to include the prior term (which only occurs once along rows) j = i+1 # A[2*j : 2*j+2 , 2*j : 2*j+4] = sqrt_inv_odom @ H_odom A[2*j : 2*j+2, 2*i : 2*i+4] = A_fill_odom b[2*j : 2*j + 2] = sqrt_inv_odom @ odoms[i] # TODO: Then fill in landmark measurements A_fill_land = sqrt_inv_obs @ H_land # H_land like H_odom is also a 2x4 matrix for i in range(n_obs): # observations = (52566,4) # (pose_index, landmark_index, measurement_x, measurement_y) # Therefore we need to check which pose is associated with which landmark p_idx = int(observations[i,0]) l_idx = int(observations[i,1]) # offset to account for prior (offset only along rows) + all odom measurements above j = i + n_odom + 1 A[2*j : 2*j+2, 2*p_idx : 2*p_idx+2] = A_fill_land[0:2, 0:2] A[2*j : 2*j+2, 2*(n_poses + l_idx):2*(n_poses + l_idx)+2] = A_fill_land[0:2, 2:4] b[2*j : 2*j+2] = sqrt_inv_obs @ observations[i,2:4] # Convert matrix to sparse format which scipy can use return csr_matrix(A), b . Solving the Linear System . The previous function has access to all the odometry and landmark measurements at once. Therefore we already have fleshed out the A and b matrices. Using this, we can then solve for the same as shown below: . if __name__ == '__main__': n_poses = len(gt_traj) n_landmarks = len(gt_landmarks) odoms = data['odom'] observations = data['observations'] sigma_odom = data['sigma_odom'] sigma_landmark = data['sigma_landmark'] \"\"\" The shapes of above values for 2d_linear.npz are: odoms = (999,2) which makes sense since there are 1000 robot poses observations = (52566,4) # (pose_index, landmark_index, measurement_x, measurement_y) sigma_odom = (2,2) sigma_landmark = (2,2) \"\"\" # Build a linear system A, b = create_linear_system(odoms, observations, sigma_odom, sigma_landmark, n_poses, n_landmarks) # Solve with the selected method for method in args.method: print(f'Applying {method}') total_time = 0 total_iters = args.repeats for i in range(total_iters): start = time.time() x, R = solve(A, b, method) end = time.time() total_time += end - start print(f'{method} takes {total_time / total_iters}s on average') if R is not None: plt.spy(R) plt.show() traj, landmarks = devectorize_state(x, n_poses) # Visualize the final result plot_traj_and_landmarks(traj, landmarks, gt_traj, gt_landmarks) . As seen above, we have have multiple methods to solve the minimization function. Lookup scipy.optimize.minimize to see a simple way in which one can solve an optimization problem by defining an objective function. Here, our objective function is the Ax = b . The methods we will define below are pre-programmed to solve for such linear equations. ",
    "url": "/docs/SLAM/Non_linear_slam.html#2d-linear-least-squares-in-code",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#2d-linear-least-squares-in-code"
  },"179": {
    "doc": "Least Squares SLAM",
    "title": "Naieve Solvers",
    "content": "Pseudo-Inverse . During our derivation we saw that one simple way to solve our minimization problem is to use the psuedo-inverse: . This is done in code as shown below: . def solve_pinv(A, b): # TODO: return x s.t. Ax = b using pseudo inverse. N = A.shape[1] x = np.zeros((N, )) # Ax = b &lt;======&gt; inv(A.T @ A) @ A.T @ A @ x = A.T b x = inv(A.T @ A) @ (A.T @ b) return x, None . Why not do SVD? . RECAP . | In CV ( see Planar Homography ) we saw that our final equation to solve boiled down to the an Ax = 0 form. | Here x is found by taking the SVD of A and choosing the eigen vector (with least eigen value) which forms the null space of A. | Remember, null-space of a vector is the transformation (i.e. transformation matrix) which squeezed the vector onto a point (i.e. it reduces dimensions to zero). | In this case x is the vector and we find the corresponding transformation matrix which forms it’s null-space. This matrix then becomes our homography matrix | For a better understanding of SVD, refer to This Document . | In the SLAM problem, we can’t do SVD mainly because it would take too long to compute! | The vector here will have a million dimensions | Also, we won’t be utilizing the sparsity and letting go of an easy improvement | . Scipy Default Solver . def solve_default(A, b): from scipy.sparse.linalg import spsolve x = spsolve(A.T @ A, A.T @ b) return x, None . ",
    "url": "/docs/SLAM/Non_linear_slam.html#naieve-solvers",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#naieve-solvers"
  },"180": {
    "doc": "Least Squares SLAM",
    "title": "Matrix Factorization - Utilizing the Sparsity",
    "content": "Cholesky - LU Decomposition - Fast, little lower numerical stability . from scipy.sparse import csc_matrix, eye from scipy.sparse.linalg import inv, splu, spsolve, spsolve_triangular def solve_lu(A, b): # TODO: return x, U s.t. Ax = b, and A = LU with LU decomposition. # https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html # Better ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.SuperLU.html N = A.shape[1] x = np.zeros((N, )) lu = splu(A.T @ A, permc_spec='NATURAL') x = lu.solve(A.T @ b) U = eye(N) U = lu.U.A return x, U def solve_lu_colamd(A, b): # TODO: return x, U s.t. Ax = b, and Permutation_rows A Permutration_cols = LU with reordered LU decomposition. # https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.splu.html N = A.shape[1] x = np.zeros((N, )) lu = splu(A.T @ A, permc_spec='COLAMD') x = lu.solve(A.T @ b) U = eye(N) U = lu.U.A return x, U . Above we see an lu_colamd method defined separately: . Column Ordering . ChatGPT explanation . Sparse LU factorization is a technique used to solve systems of linear equations involving sparse matrices. The process involves decomposing the original matrix into a lower triangular matrix (L), an upper triangular matrix (U), and a permutation matrix (P) such that A = PLU. The resulting LU factorization can then be used to efficiently solve multiple linear systems with the same coefficient matrix. The choice of column ordering can significantly affect the performance of the LU factorization algorithm. The COLAMD algorithm is an approximate minimum degree ordering method that aims to minimize the fill-in, which refers to the number of additional non-zero entries introduced during the LU factorization process. By reducing the fill-in, the COLAMD ordering can lead to faster factorization and improved computational efficiency. My Understading . The colamd method rearranges (permutes) the A matrix to avoid accumulation of data in the last few columns of one of the triangular matrices. This rearragement as seen in helps in the following manner: . | The algorithm reorders the columns of the matrix so that the non-zero entries are clustered together | This can reduce the number of fill-in elements created during the factorization process. | Therefore reordering can significantly reduce the computational and memory requirements of the LU decomposition. | . Some graphical examples which were computed for this dataset is shown below: . QR Factorization - Slower, but more stable . def solve_qr(A, b): # TODO: return x, R s.t. Ax = b, and |Ax - b|^2 = |Rx - d|^2 + |e|^2 # https://github.com/theNded/PySPQR N = A.shape[1] x = np.zeros((N, )) R = eye(N) # rz gives the upper triangular part Z, R ,E, rank = rz(A, b, permc_spec='NATURAL') x = spsolve_triangular(R,Z,lower=False) return x, R def solve_qr_colamd(A, b): # TODO: return x, R s.t. Ax = b, and |Ax - b|^2 = |R E^T x - d|^2 + |e|^2, with reordered QR decomposition (E is the permutation matrix). # https://github.com/theNded/PySPQR N = A.shape[1] x = np.zeros((N, )) R = eye(N) # rz gives the upper triangular part Z, R ,E, rank = rz(A, b, permc_spec='COLAMD') # E is symmetric and is the permutation vector s.t. QR = AE E = permutation_vector_to_matrix(E) x = spsolve_triangular(R,Z,lower=False) x = E @ x return x, R . ",
    "url": "/docs/SLAM/Non_linear_slam.html#matrix-factorization---utilizing-the-sparsity",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#matrix-factorization---utilizing-the-sparsity"
  },"181": {
    "doc": "Least Squares SLAM",
    "title": "Results of Least Sq. Optimization and Visual Inference",
    "content": "Ground Truth Trajectory . Optimization Results . Inference on the Runtime Comparisons . Therefore, it’s safe to say that the when A matrix is more dense, the colamd method makes sense and efficiency boost is observed. Therefore, for small A matrices, it might not be necessary. Additionaly Optimization Results . ",
    "url": "/docs/SLAM/Non_linear_slam.html#results-of-least-sq-optimization-and-visual-inference",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#results-of-least-sq-optimization-and-visual-inference"
  },"182": {
    "doc": "Least Squares SLAM",
    "title": "2D Non-Linear Least Squares SLAM4",
    "content": " ",
    "url": "/docs/SLAM/Non_linear_slam.html#2d-non-linear-least-squares-slam4",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#2d-non-linear-least-squares-slam4"
  },"183": {
    "doc": "Least Squares SLAM",
    "title": "Measurement Functions",
    "content": ". ",
    "url": "/docs/SLAM/Non_linear_slam.html#measurement-functions",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#measurement-functions"
  },"184": {
    "doc": "Least Squares SLAM",
    "title": "Building the Linear System from Non-Linear System",
    "content": ". ",
    "url": "/docs/SLAM/Non_linear_slam.html#building-the-linear-system-from-non-linear-system",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#building-the-linear-system-from-non-linear-system"
  },"185": {
    "doc": "Least Squares SLAM",
    "title": "Non Linear System in Code",
    "content": "def warp2pi(angle_rad): \"\"\" Warps an angle in [-pi, pi]. Used in the update step. \\param angle_rad Input angle in radius \\return angle_rad_warped Warped angle to [-\\pi, \\pi]. \"\"\" angle_rad = angle_rad - 2 * np.pi * np.floor( (angle_rad + np.pi) / (2 * np.pi)) return angle_rad def init_states(odoms, observations, n_poses, n_landmarks): ''' Initialize the state vector given odometry and observations. ''' traj = np.zeros((n_poses, 2)) landmarks = np.zeros((n_landmarks, 2)) landmarks_mask = np.zeros((n_landmarks), dtype=bool) for i in range(len(odoms)): traj[i + 1, :] = traj[i, :] + odoms[i, :] for i in range(len(observations)): pose_idx = int(observations[i, 0]) landmark_idx = int(observations[i, 1]) if not landmarks_mask[landmark_idx]: landmarks_mask[landmark_idx] = True pose = traj[pose_idx, :] theta, d = observations[i, 2:] landmarks[landmark_idx, 0] = pose[0] + d * np.cos(theta) landmarks[landmark_idx, 1] = pose[1] + d * np.sin(theta) return traj, landmarks def odometry_estimation(x, i): ''' \\param x State vector containing both the pose and landmarks \\param i Index of the pose to start from (odometry between pose i and i+1) \\return odom Odometry (\\Delta x, \\Delta y) in the shape (2, ) ''' # TODO: return odometry estimation odom = np.zeros((2, )) try: odom[0] = x[2*(i+1)] - x[2*i] odom[1] = x[2*(i+1)+1] - x[(2*i)+1] except: ipdb.set_trace() return odom def bearing_range_estimation(x, i, j, n_poses): ''' \\param x State vector containing both the pose and landmarks \\param i Index of the pose to start from \\param j Index of the landmark to be measured \\param n_poses Number of poses \\return obs Observation from pose i to landmark j (theta, d) in the shape (2, ) ''' # TODO: return bearing range estimations obs = np.zeros((2, )) # given the robot pose and landmark location, get the bearing estimate (see theory) y_dist = x[(2*n_poses)+(2*j)+1] - x[(2*i)+1] x_dist = x[(2*n_poses)+(2*j)] - x[(2*i)] obs[0] = warp2pi(np.arctan2(y_dist, x_dist)) obs[1] = np.sqrt(x_dist**2 + y_dist**2) return obs def compute_meas_obs_jacobian(x, i, j, n_poses): ''' \\param x State vector containing both the pose and landmarks \\param i Index of the pose to start from \\param j Index of the landmark to be measured \\param n_poses Number of poses \\return jacobian Derived Jacobian matrix in the shape (2, 4) ''' # TODO: return jacobian matrix jacobian = np.zeros((2, 4)) y_dist = x[(2*n_poses)+(2*j)+1] - x[(2*i)+1] x_dist = x[(2*n_poses)+(2*j)] - x[(2*i)] sensor_range = np.sqrt(x_dist**2 + y_dist**2) jacobian[0,0] = y_dist/(sensor_range**2) jacobian[0,1] = -x_dist/(sensor_range**2) jacobian[0,2] = -y_dist/(sensor_range**2) jacobian[0,3] = x_dist/(sensor_range**2) jacobian[1,0] = -x_dist/sensor_range jacobian[1,1] = -y_dist/sensor_range jacobian[1,2] = x_dist/sensor_range jacobian[1,3] = y_dist/sensor_range return jacobian def create_linear_system(x, odoms, observations, sigma_odom, sigma_observation, n_poses, n_landmarks): ''' \\param x State vector x at which we linearize the system. \\param odoms Odometry measurements between i and i+1 in the global coordinate system. Shape: (n_odom, 2). \\param observations Landmark measurements between pose i and landmark j in the global coordinate system. Shape: (n_obs, 4). \\param sigma_odom Shared covariance matrix of odometry measurements. Shape: (2, 2). \\param sigma_observation Shared covariance matrix of landmark measurements. Shape: (2, 2). \\return A (M, N) Jacobian matrix. \\return b (M, ) Residual vector. where M = (n_odom + 1) * 2 + n_obs * 2, total rows of measurements. N = n_poses * 2 + n_landmarks * 2, length of the state vector. ''' n_odom = len(odoms) n_obs = len(observations) M = (n_odom + 1) * 2 + n_obs * 2 N = n_poses * 2 + n_landmarks * 2 A = np.zeros((M, N)) b = np.zeros((M, )) sqrt_inv_odom = np.linalg.inv(scipy.linalg.sqrtm(sigma_odom)) sqrt_inv_obs = np.linalg.inv(scipy.linalg.sqrtm(sigma_observation)) # TODO: First fill in the prior to anchor the 1st pose at (0, 0) # The prior is just a reference frame, it also has some uncertainty, but no measurement # Hence the measurement function which estimates the prior is just a identity function # i.e h_p(r_t) = r_t. Since no measurements exist, the b matrix will have only zeros (already the case) # Here we also define the uncertainty in prior is same as odom uncertainty A[0:2, 0:2] = sqrt_inv_odom @ np.eye(2) # no need to update b (already zeros) H_odom = np.array([[-1,0,1,0], [0,-1,0,1]], dtype=np.float32) A_fill_odom = sqrt_inv_odom @ H_odom # TODO: Then fill in odometry measurements for i in range(n_odom): # declare an offset for i to include the prior term (which only occurs once along rows) j = i+1 A[2*j : 2*j+2, 2*i : 2*i+4] = A_fill_odom b[2*j : 2*j + 2] = sqrt_inv_odom @ (odom[i] - odometry_estimation(x,i)) # TODO: Then fill in landmark measurements for i in range(n_obs): p_idx = int(observations[i,0]) l_idx = int(observations[i,1]) Al = sqrt_inv_obs @ compute_meas_obs_jacobian(x, p_idx, l_idx, n_poses) # offset again to account for prior j = n_odom+1+i A[2*j:2*j+2, 2*p_idx:2*p_idx+2] = Al[0:2,0:2] A[2*j:2*j+2,2*(n_poses+l_idx):2*(n_poses+l_idx)+2] = Al[0:2,2:4] b[2*j:2*j+2] = sqrt_inv_obs @ warp2pi(observations[i,2:4] - bearing_range_estimation(x,p_idx,l_idx,n_poses)) return csr_matrix(A), b if __name__ == '__main__': n_poses = len(gt_traj) n_landmarks = len(gt_landmarks) odom = data['odom'] observations = data['observations'] sigma_odom = data['sigma_odom'] sigma_landmark = data['sigma_landmark'] # Initialize: non-linear optimization requires a good init. for method in args.method: print(f'Applying {method}') traj, landmarks = init_states(odom, observations, n_poses, n_landmarks) print('Before optimization') plot_traj_and_landmarks(traj, landmarks, gt_traj, gt_landmarks) # Iterative optimization x = vectorize_state(traj, landmarks) for i in range(10): A, b = create_linear_system(x, odom, observations, sigma_odom, sigma_landmark, n_poses, n_landmarks) dx, _ = solve(A, b, method) x = x + dx traj, landmarks = devectorize_state(x, n_poses) print('After optimization') plot_traj_and_landmarks(traj, landmarks, gt_traj, gt_landmarks) . ",
    "url": "/docs/SLAM/Non_linear_slam.html#non-linear-system-in-code",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#non-linear-system-in-code"
  },"186": {
    "doc": "Least Squares SLAM",
    "title": "Results",
    "content": ". ",
    "url": "/docs/SLAM/Non_linear_slam.html#results",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#results"
  },"187": {
    "doc": "Least Squares SLAM",
    "title": "Distinction between Non-Linear and Linear Optimization",
    "content": ". | The Non Linear optimization process is different in that we have a residual h 0 term which causes the optimizer to solve for the error between predicted and actual measurement. In the linear case the optimizer directly solves for x in the equation Ax = b. | Additionally, the non-linear method also requires a good initial estimate to start. This is because the optimization is iterative for the non-linear case if the initialization is bad it will take longer or not converge within the required error tolerance. | . ",
    "url": "/docs/SLAM/Non_linear_slam.html#distinction-between-non-linear-and-linear-optimization",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html#distinction-between-non-linear-and-linear-optimization"
  },"188": {
    "doc": "Least Squares SLAM",
    "title": "Least Squares SLAM",
    "content": "{: .text-delta } . | Background . | Learning Goals | . | Introduction | 2D Linear Least Squares SLAM . | Why Least Squares? | Derivation . | Intuition for our final solution | . | Measurement Funtion h(x) . | Odometry Measurement Function | Landmark Measurment Function | Shapes of the A and b matrices | . | 2D Linear Least Squares in Code . | Fully Define the A and b matrices (setup step) | Solving the Linear System | . | Naieve Solvers . | Pseudo-Inverse . | Why not do SVD? | Scipy Default Solver | . | . | Matrix Factorization - Utilizing the Sparsity . | Cholesky - LU Decomposition - Fast, little lower numerical stability . | Column Ordering | ChatGPT explanation | My Understading | . | QR Factorization - Slower, but more stable | . | Results of Least Sq. Optimization and Visual Inference . | Ground Truth Trajectory | Optimization Results | Inference on the Runtime Comparisons | Additionaly Optimization Results | . | . | 2D Non-Linear Least Squares SLAM4 . | Measurement Functions | Building the Linear System from Non-Linear System | Non Linear System in Code | Results | Distinction between Non-Linear and Linear Optimization | . | . Detailed write-up . ",
    "url": "/docs/SLAM/Non_linear_slam.html",
    
    "relUrl": "/docs/SLAM/Non_linear_slam.html"
  },"189": {
    "doc": "Numpy for CV",
    "title": "Before you Begin",
    "content": "Official Documentation . Numpy and Scipy are two resources to compute a variety of functions on matrices. Scipy is built on top of numpy and has a larger codebase of modules which we can utilize . ",
    "url": "/docs/Computer%20Vision/Numpy.html#before-you-begin",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#before-you-begin"
  },"190": {
    "doc": "Numpy for CV",
    "title": "Images and Arrays",
    "content": " ",
    "url": "/docs/Computer%20Vision/Numpy.html#images-and-arrays",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#images-and-arrays"
  },"191": {
    "doc": "Numpy for CV",
    "title": "Image Operations",
    "content": "Importing Images . In the below code we input an image and convert it into an array. Shape of an array is just it’s size . im = array(Image.open('empire.jpg')) print im.shape, im.dtype . The output would look lik this: . (800, 569, 3) uint8 (RGB image) . Converting image to Greyscale . This uses an extra library called Python Pillow . from PIL import Image, ImageOps im = array(Image.open('empire.jpg').convert('L'),'f') print im.shape, im.dtype . Plotting an image . img = np.array(Image.open('House2.jpg')) plt.figure(figsize=(8,8)) plt.imshow(img) plt.show . ",
    "url": "/docs/Computer%20Vision/Numpy.html#image-operations",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#image-operations"
  },"192": {
    "doc": "Numpy for CV",
    "title": "Array Functions and Operations",
    "content": " ",
    "url": "/docs/Computer%20Vision/Numpy.html#array-functions-and-operations",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#array-functions-and-operations"
  },"193": {
    "doc": "Numpy for CV",
    "title": "Array Nomenclature",
    "content": ". It’s important to realise that we only care about shapes of a matrix and our computation revolves around the shape tuple (1,2,3) irrespective of which is row or column. Develop a generalized version of matrix definitions!! . An image can have shape as (640,540,3). Here we need to think in the way that there are 640 rows and 540 columns and 3 RGB channels. Therefore, rows, columns, pages don’t matter much. Just think in terms of shapes. sum() function in 1D . import numpy as np arr = [20, 2, .2, 10, 4] print(\"\\nSum of arr : \", np.sum(arr)) print(\"Sum of arr(uint8) : \", np.sum(arr, dtype = np.uint8)) print(\"Sum of arr(float32) : \", np.sum(arr, dtype = np.float32)) . Output: . Sum of arr : 36.2 Sum of arr(uint8) : 36 Sum of arr(float32) : 36.2 . In 1D it just computes the sum of all elements in the array. It can also do type conversion on the go. We can extend this same logic to 2D, there too it calculates the sum of all matrix elements . sum() in 2D along axes . Axis along which we want to calculate the sum value. Otherwise, it will consider arr to be flattened(works on all the axis). axis = 0 means it calculates sum of all elements in ith column and (i=1)th column.. axis = 1 means it calculates sum of all elements in (j)th column and (j+1)th column.. arr = [[14, 17, 12, 33, 44], [15, 6, 27, 8, 19], [23, 2, 54, 1, 4,]] print(\"\\nSum of arr : \", np.sum(arr)) print(\"Sum of arr(axis = 0) : \", np.sum(arr, axis = 0)) print(\"Sum of arr(axis = 1) : \", np.sum(arr, axis = 1)) . Output would be: . Sum of arr : 279 Sum of arr(axis = 0) : [52 25 93 42 67] Sum of arr(axis = 1) : [120 75 84] . But notice how the vector of axis = 1 has been transposed to show as a row vector . We change that behaviour by adding a second argument to the sum() function: . print(\"\\nSum of arr (keepdimension is True): \\n\", np.sum(arr, axis = 1, keepdims = True)) . Output . Sum of arr (keepdimension is True): [[120] [ 75] [ 84]] . Looping over an Image and Grayscale . We can loop over individual elements in a matrix after knowing the shape of the matrix . The shape of the image is given as a tuple eg. (640, 540, 3) . | the last item of that tuple is the RGB spectrum (3 dimensions per pixel) | the first two items in the tuple is the actual size of the image | . for i in range(img.shape[1]): print() . In the above code we are looping over the rows. Therefore we are looping 640 times. Method 1 : Consider this method of converting image into greyscale: . import numpy as np import matplotlib.pyplot as plt from PIL import Image, ImageOps img = np.array(Image.open('B1.jpg')) print(img.shape) for i in range(img.shape[0]): for j in range(img.shape[1]): grey_value = 0 for k in range(img.shape[2]): grey_value += img[i,j,k] img[i,j,0] = int(grey_value/3) img2 = img[:,:,1] plt.figure(figsize=(8,8)) plt.imshow(img2) plt.show() . Also note how we removed the third (extra) dimensions using: . img2 = img[:,:,1] . This method uses averaging to find grayscale. However a slightly modified version is usually preferred: . Method 2: Accounting for Luminance Perception . import numpy as np import matplotlib.pyplot as plt from PIL import Image, ImageOps weight = [0.2989, 0.5870, 0.1140] img = np.array(Image.open('B1.jpg')) print(img.shape) for i in range(img.shape[0]): for j in range(img.shape[1]): grey_value = 0 for k in range(len(weight)): grey_value += (img[i,j,k]*weight[k]) img[i,j,0] = int(grey_value) img2 = img[:,:,1] plt.figure(figsize=(8,8)) plt.imshow(img2, cmap=plt.get_cmap(\"gray\")) plt.show() . Method 3: Simpler code using numpy.mean . from PIL import Image import numpy as np import matplotlib.pyplot as plt color_img = np.array(Image.open('B1.jpg')) / 255 img = np.mean(color_img, axis=2) plt.figure(figsize=(8,8)) plt.imshow(img, cmap=plt.get_cmap(\"gray\")) plt.show() . ",
    "url": "/docs/Computer%20Vision/Numpy.html#array-nomenclature",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#array-nomenclature"
  },"194": {
    "doc": "Numpy for CV",
    "title": "Built-in Numpy functions",
    "content": " ",
    "url": "/docs/Computer%20Vision/Numpy.html#built-in-numpy-functions",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#built-in-numpy-functions"
  },"195": {
    "doc": "Numpy for CV",
    "title": "Difference between dot, matmul, and *",
    "content": ". ",
    "url": "/docs/Computer%20Vision/Numpy.html#difference-between-dot-matmul-and-",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#difference-between-dot-matmul-and-"
  },"196": {
    "doc": "Numpy for CV",
    "title": "Plotting a pixel-wise histogram",
    "content": "img = np.array(Image.open('emma_stone.jpg')) img_flat = img.flatten() plt.hist(img_flat, bins=200, range=[0, 256]) plt.title(\"Number of pixels in each intensity value\") plt.xlabel(\"Intensity\") plt.ylabel(\"Number of pixels\") plt.show() . ",
    "url": "/docs/Computer%20Vision/Numpy.html#plotting-a-pixel-wise-histogram",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#plotting-a-pixel-wise-histogram"
  },"197": {
    "doc": "Numpy for CV",
    "title": "Reshaping Arrays",
    "content": "x = np.arange(4).reshape((2,2)) x &gt;&gt;array([[0, 1], [2, 3]]) . ",
    "url": "/docs/Computer%20Vision/Numpy.html#reshaping-arrays",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#reshaping-arrays"
  },"198": {
    "doc": "Numpy for CV",
    "title": "Transpose of a matrix",
    "content": "Simple transpose is done using the matrix.transpose() or matrix.T method (both are same). One of them is showed below: . # (refer matrix x in above example) np.transpose(x) array([[0, 2], [1, 3]]) . However the transpose function takes more arguments and this is important for 3D matrices. Note that if a 3D matrix say ‘A’ has shape (1,2,3), the result of transpose without specifying any extra argument will be (3,2,1) . x = np.ones((1, 2, 3)) np.transpose(x, (1, 0, 2)).shape &gt;&gt;(2, 1, 3) . Note. While declaring array as in np.ones(1,2,3). This can be interpreted in two ways: . | If we are printing the array in terminal we will read it as: there are 1 pages, 2 rows and 3 columns | If it’s an image, the shape will be 1 row, 2 coulmns and 3 will be for 3 RGB channels | . It’s important to realise that we only care about shapes of a matrix and our computation revolves around the shape tuple (1,2,3) irrespective of which is row or column. Develop a generalized version of matrix definitions!! . However, we will access each row/column starting from 0 as x[0,0,0] or x[1,1,1]. The second argument stands for the axes parameter. Axes are numbered as 0,1,2 . i.e. default configuration of axes is (0,1,2) for a 3D array and (0,1) for a 2D array . Therefore if we specify &lt;np.transpose(x,(1,0,2))&gt; we’re saying that we want the first two shapes interchanged. Remember that first two shapes are pages and rows. Hence, those two will interchange. ",
    "url": "/docs/Computer%20Vision/Numpy.html#transpose-of-a-matrix",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#transpose-of-a-matrix"
  },"199": {
    "doc": "Numpy for CV",
    "title": "Padding of Matrices",
    "content": "Padding is used to ensure overall image size does not reduce while run filters/convulutions on it . import numpy as np x = np.ones(3) y = np.pad(x, pad_width=1) y # Expected result # array([0., 1., 1., 1., 0.]) . ",
    "url": "/docs/Computer%20Vision/Numpy.html#padding-of-matrices",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#padding-of-matrices"
  },"200": {
    "doc": "Numpy for CV",
    "title": "newaxis method",
    "content": "ref: newaxis . This method can be used to convert a row vector to a column vector and at the same time add another dimension as shown below: . a = np.array([0,1,2]) print(a.shape) . Output: (3,) . Now lets do the newaxis modification: . c = (a[:, np.newaxis]) print(c) print(c.shape) . Output: . [[0] [1] [2]] (3, 1) . Therefore we can see that the vector has been rotated and another dimension has been added to the shape tuple . ",
    "url": "/docs/Computer%20Vision/Numpy.html#newaxis-method",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#newaxis-method"
  },"201": {
    "doc": "Numpy for CV",
    "title": "einsum",
    "content": "Refer to this document: einsum . ",
    "url": "/docs/Computer%20Vision/Numpy.html#einsum",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#einsum"
  },"202": {
    "doc": "Numpy for CV",
    "title": "Stacking rows using vstack",
    "content": "We can use this function to stack rows onto an exiting numpy array. in_arr1 = geek.array([ 1, 2, 3] ) in_arr2 = geek.array([ 4, 5, 6] ) # Stacking the two arrays vertically out_arr = geek.vstack((in_arr1, in_arr2)) print (out_arr) . Practically we can use this in a specific case. If we don’t know the number of rows we will be adding to a numpy array: . | We will define the array as a 0 row array | We then add rows as we progress using the vstack function | . word_array = np.array([]).reshape(0, maxlength) for message in messages: word_count = np.zeros((1, word_num)) for word in word_list: if word == \"yes\": word_count[0, word_dictionary[word]] += 1 word_array = np.vstack([word_array, word_count]) return word_array . ",
    "url": "/docs/Computer%20Vision/Numpy.html#stacking-rows-using-vstack",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#stacking-rows-using-vstack"
  },"203": {
    "doc": "Numpy for CV",
    "title": "Saving a numpy matrix in a text file",
    "content": "np.savetxt('./output/p06_sample_train_matrix', train_matrix[:100,:]) . ",
    "url": "/docs/Computer%20Vision/Numpy.html#saving-a-numpy-matrix-in-a-text-file",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html#saving-a-numpy-matrix-in-a-text-file"
  },"204": {
    "doc": "Numpy for CV",
    "title": "Numpy for CV",
    "content": ". | Before you Begin | Images and Arrays . | Image Operations . | Importing Images | Converting image to Greyscale | Plotting an image | . | Array Functions and Operations | Array Nomenclature . | sum() function in 1D | sum() in 2D along axes | Looping over an Image and Grayscale . | Method 1 : Consider this method of converting image into greyscale: | Method 2: Accounting for Luminance Perception | Method 3: Simpler code using numpy.mean | . | . | . | Built-in Numpy functions . | Difference between dot, matmul, and * | Plotting a pixel-wise histogram | Reshaping Arrays | Transpose of a matrix | Padding of Matrices | newaxis method | einsum | Stacking rows using vstack | Saving a numpy matrix in a text file | . | . ",
    "url": "/docs/Computer%20Vision/Numpy.html",
    
    "relUrl": "/docs/Computer%20Vision/Numpy.html"
  },"205": {
    "doc": "Optical Flow and Image Alignment",
    "title": "Writeup",
    "content": "Questionnaire . Writeup . ",
    "url": "/docs/Computer%20Vision/Optical%20Flow.html#writeup",
    
    "relUrl": "/docs/Computer%20Vision/Optical%20Flow.html#writeup"
  },"206": {
    "doc": "Optical Flow and Image Alignment",
    "title": "Optical Flow and Image Alignment",
    "content": ". ",
    "url": "/docs/Computer%20Vision/Optical%20Flow.html",
    
    "relUrl": "/docs/Computer%20Vision/Optical%20Flow.html"
  },"207": {
    "doc": "Particle Filters Theory",
    "title": "Before you Begin",
    "content": "Particle Filters are a direct application of Bayes Filters. The Short Version of Bayes Filter is shown below: . ",
    "url": "/docs/SLAM/Particle%20Filter_theory.html#before-you-begin",
    
    "relUrl": "/docs/SLAM/Particle%20Filter_theory.html#before-you-begin"
  },"208": {
    "doc": "Particle Filters Theory",
    "title": "High Level Overview",
    "content": "We will be doing the following steps: . | Initialize particles randomly across the whole map | resample at every step (selecting only the particles whose predicted laserscan matches actual laserscan) (low variance sampler) | Slowly decay the number of particles being randomly reinitialized | Use the mean of the particles’s estimated pose to get true pose | . Detailed write-up . ",
    "url": "/docs/SLAM/Particle%20Filter_theory.html#high-level-overview",
    
    "relUrl": "/docs/SLAM/Particle%20Filter_theory.html#high-level-overview"
  },"209": {
    "doc": "Particle Filters Theory",
    "title": "Particle Filters Theory",
    "content": "{: .text-delta } . | Before you Begin | High Level Overview | . ",
    "url": "/docs/SLAM/Particle%20Filter_theory.html",
    
    "relUrl": "/docs/SLAM/Particle%20Filter_theory.html"
  },"210": {
    "doc": "Planar Homography",
    "title": "Before you Begin",
    "content": "Reference Book 1 Reference Book 2 . ",
    "url": "/planar_homography/#before-you-begin",
    
    "relUrl": "/planar_homography/#before-you-begin"
  },"211": {
    "doc": "Planar Homography",
    "title": "PDFs",
    "content": "Assignment Questionnaire . My Answers . ",
    "url": "/planar_homography/#pdfs",
    
    "relUrl": "/planar_homography/#pdfs"
  },"212": {
    "doc": "Planar Homography",
    "title": "Some Basics on Camera Projection",
    "content": " ",
    "url": "/planar_homography/#some-basics-on-camera-projection",
    
    "relUrl": "/planar_homography/#some-basics-on-camera-projection"
  },"213": {
    "doc": "Planar Homography",
    "title": "Projection of 3D to 2D image plane",
    "content": "To understand how a camera views the 3D world, first we look at the projection of 3D points onto an image plane. We use basic high school physics and some similar triangle properties to derive the following formula: . Notice that the minus sign is bit irritating to work with. (Also we don’t see inverted images as the formula suggests. This is becauase our brain does the inversion in real time) . Therefore, let’s start with the below version of the formula by ignoring this inversion effect . Now, the above equation can be written in matrix form, but we’ll form one artifact in this conversion i.e. lambda . It’s clear that we can find this lambda as shown. However, why do we even need this? Ans. We want to represent the coordinates in homogenous coordinates . ",
    "url": "/planar_homography/#projection-of-3d-to-2d-image-plane",
    
    "relUrl": "/planar_homography/#projection-of-3d-to-2d-image-plane"
  },"214": {
    "doc": "Planar Homography",
    "title": "Camera Matrices",
    "content": "Generic Representation . Now, let’s add another constraint on this equation. Suppose we rotate our 3D point in space or we rotate the camera itself by a certain angle. In the world of robotics we call such transforms as a rotation matrix. Reference: Rotation Matrices . To get a good grasp of rotation matrices, I highly recommend some linear algebra brush-up using 3B1B (3 Blue 1 Brown). Specifically (watch 8th minute of this video) The rotation shown in the above video in the 8th minute is a rotation matrix in 2D. Now, adding a 3D translation (just 3 numbers which add to the x,y,z component of a 3D vector) along with a 3D rotation we get the basic projection equation . Where the two matrices are called the camera intrinsics (captures focal lengths) and the camera extrinsics (capturing rotation and translation) . This rotation (r-matrix) can also be visualized as fixing a world coordinate frame onto some plane in the 3D world (think of a it as a flat table top) and then thinking how our camera is rotated w.r.t that frame: . Now, most cameras also distort images due to lens optics or other properties inherent in building the camera itself. These are captured as shown below: . Now, adding these intrinsic and extrinsic factors, we get: . Alternate notation of camera matrices . ",
    "url": "/planar_homography/#camera-matrices",
    
    "relUrl": "/planar_homography/#camera-matrices"
  },"215": {
    "doc": "Planar Homography",
    "title": "The Homography Situation",
    "content": " ",
    "url": "/planar_homography/#the-homography-situation",
    
    "relUrl": "/planar_homography/#the-homography-situation"
  },"216": {
    "doc": "Planar Homography",
    "title": "Single View",
    "content": "Now, if we focus on only planes (table top and human holding camera situation): We can make certain simplifying assumptions. This is primarily that the 3D point we’re looking at has constant depth in it’s immediate neighbourhood. Using this we simplify our equations to: . This 3x3 m-matrix now represents the mapping of 3D points on a plane to 2D point in an image . ",
    "url": "/planar_homography/#single-view",
    
    "relUrl": "/planar_homography/#single-view"
  },"217": {
    "doc": "Planar Homography",
    "title": "Multiple Views",
    "content": "Now, by simple extension of the above logic we can derive the following: . | We have just 1 plane in the 3D world | We have two cameras looking at this plane | Each camera has it’s own 3x3 m-matrix which maps 3D plane points onto 2D image frame | Therefore if two cameras can see the same 3D point, we can find a mapping between the two cameras | This mapping between the two cameras is given by a new 3x3 matrix called the homography matrix | . ",
    "url": "/planar_homography/#multiple-views",
    
    "relUrl": "/planar_homography/#multiple-views"
  },"218": {
    "doc": "Planar Homography",
    "title": "Limitations of Planar Homography",
    "content": ". | When the scene is very far away from the camera, all objects can be said to have the same depth. This is because the relative depth distances between foreground and background will be negligible in comparison to the average scene depth. Therefore, in such cases all objects in scene can be said to lie on a plane and as proved above, can be captured by two cameras related by a homography matrix. | For nearby scenes where the variation in scene depth is more apparent, a homography mapping works well only under pure rotation. | . ",
    "url": "/planar_homography/#limitations-of-planar-homography",
    
    "relUrl": "/planar_homography/#limitations-of-planar-homography"
  },"219": {
    "doc": "Planar Homography",
    "title": "Implementation of Homography Estimation",
    "content": " ",
    "url": "/planar_homography/#implementation-of-homography-estimation",
    
    "relUrl": "/planar_homography/#implementation-of-homography-estimation"
  },"220": {
    "doc": "Planar Homography",
    "title": "The Pipeline",
    "content": "The main application of homography transforms is to find how some reference template has been warped due to movement of the camera. This is seem below as: . The applications of this are: . | image stitching (think of two images from two views as a warped version of view 0) | augmented reality (projecting some images onto a fixed/known plane in the real world) | . To perform any of the above cool applications, we first need to compute the homography between any two views. The pipeline for this would be: . | Have one reference view and another view with the camera having moved slightly | Detect some keypoints (interest points like corners/edges) in each image | Describe these keypoints in some way (maybe capture the histogram of pixel intensities in a small patch around the keypoint) | Match the keypoints in one image to another using the keypoint descriptions | Use the spatial information of these matched keypoints (i.e. the x,y coordinates of each of these keypoints) to find the Homography matrix | **Apply the homography matrix as a transformation on one of the images **to warp and match the images | . Let’s go deeper into the each of the above steps: . Keypoint Detection . | There are several methods to find keypoints in an image. Usually these keypoints are corners since other features like edges may warp or curve due to distortion and may be difficult to trace. | The common methods are Harris Corner Detector, polygon fitting, FAST detectors etc. | Here we use the FAST detector | . Keypoint Descriptors . Common descriptors include BRIEF, ORB, SIFT etc. Here we’ve used the BRIEF descriptor . The BRIEF descriptor works by creating a binary feature vector of a patch from random (x,y) point pairs. This randomness in generating point pairs ensures changes in pixel intensities are captuerd in multiple directions thereby being sensitive to a large variety of edges or corners. The BRIEF descriptor also compares these binary strings using hamming distance further reduces compute time. Due to this computational cost of calculating histograms for each filter bank it would not make sense to use filterbanks instead of BRIEF. Further, just filterbanks cannot encode patch descriptions, i.e. without any form of histograms (like SIFT), the filterbanks themselves cannot be used instead of BRIEF. Reference: BRIEF Descriptor . The implementation of keypoint detection, description and matching are shown below: . import numpy as np import cv2 import skimage.color from helper import briefMatch from helper import computeBrief from helper import corner_detection # Q2.1.4 def matchPics(I1, I2, opts): \"\"\" Match features across images Input ----- I1, I2: Source images opts: Command line args Returns ------- matches: List of indices of matched features across I1, I2 [p x 2] locs1, locs2: Pixel coordinates of matches [N x 2] \"\"\" print(\"computing image matches\") ratio = opts.ratio #'ratio for BRIEF feature descriptor' sigma = opts.sigma #'threshold for corner detection using FAST feature detector' # Convert Images to GrayScale I1 = skimage.color.rgb2gray(I1) I2 = skimage.color.rgb2gray(I2) # Detect Features in Both Images # locs1 is just the detected corners of I1 locs1 = corner_detection(I1, sigma) locs2 = corner_detection(I2, sigma) # Obtain descriptors for the computed feature locations # We use the breif descriptor to give the patch descriptions (patch of pixel width = 9) # for the corners(keypoints) which we obtained from corner_description # desc is the binary string (len(string)=256 and 256bits) # which serves as the feature descriptor desc1, locs1 = computeBrief(I1, locs1) desc2, locs2 = computeBrief(I2, locs2) # Match features using the descriptors matches = briefMatch(desc1, desc2, ratio) print(f'Computed {matches.shape[0]} matches successfully') return matches, locs1, locs2 def briefMatch(desc1,desc2,ratio): matches = skimage.feature.match_descriptors(desc1,desc2,'hamming',cross_check=True,max_ratio=ratio) return matches . Calculating the Homography Matrix . Let’s say we have two images: image1 and image2 . To Derive the A matrix we undergo the following steps: . | Where h is found by taking the SVD of A and choosing the eigen vector (with least eigen value) which forms the null space of A. | We will also normalize the correspondence points to better numerical stability of direct linear transform (DLT) estimation. Refer Normalization Document to get a better understanding of the normalization steps used below | Remember, null-space of a vector is the transformation (i.e. transformation matrix) which squeezed the vector onto a point (i.e. it reduces dimensions to zero). | In this case x is the vector and we find the corresponding transformation matrix which forms it’s null-space. This matrix then becomes our homography matrix | For a better understanding of SVD, refer to This Document | . (Bonus) RANSAC: Rejecting outliers during our homography calculation . Implementation of above steps . import numpy as np import cv2 import skimage.io import skimage.color from planarH import * from opts import get_opts from matchPics import matchPics from helper import briefMatch def warpImage(opts): \"\"\" Warp template image based on homography transform Args: opts: user inputs \"\"\" image1 = cv2.imread('../data/cv_cover.jpg') image2 = cv2.imread('../data/cv_desk.png') template_img = cv2.imread('../data/hp_cover.jpg') # make sure harry_potter image is same size as CV book x,y,z = image1.shape template_img = cv2.resize(template_img, (y,x)) matches, locs1, locs2 = matchPics(image1, image2, opts) # invert the columns of locs1 and locs2 locs1[:, [1, 0]] = locs1[:, [0, 1]] locs2[:, [1, 0]] = locs2[:, [0, 1]] matched_points = create_matched_points(matches, locs1, locs2) h, inlier = computeH_ransac(matched_points[:,0:2], matched_points[:,2:], opts) print(\"homography matrix is \\n\", h) # compositeH(h, source, destination) composite_img = compositeH(h, template_img, image2) # Display images cv2.imshow(\"Composite Image :)\", composite_img) cv2.waitKey() if __name__ == \"__main__\": opts = get_opts() warpImage(opts) . RANSAC and Construction of Composite Image . from copy import deepcopy from dataclasses import replace from platform import python_branch import numpy as np import cv2 import matplotlib.pyplot as plt import skimage.color import math import random from scipy import ndimage from scipy.spatial import distance from matchPics import matchPics from helper import plotMatches from opts import get_opts from tqdm import tqdm def computeH(x1, x2): \"\"\" Computes the homography based on matching points in both images Args: x1: keypoints in image 1 x2: keypoints in image 2 Returns: H2to1: the homography matrix \"\"\" # Define a dummy H matrix A_build = [] # Define the A matrix for (Ah = 0) (A matrix size = N*2 x 9) for i in range(x1.shape[0]): row_1 = np.array([ x2[i,0], x2[i,1], 1, 0, 0, 0, -x1[i,0]*x2[i,0], -x1[i,0]*x2[i,1], -x1[i,0] ]) row_2 = np.array([ 0, 0, 0, x2[i,0], x2[i,1], 1, -x1[i,1]*x2[i,0], -x1[i,1]*x2[i,1], -x1[i,1] ]) A_build.append(row_1) A_build.append(row_2) A = np.stack(A_build, axis=0) # Do the least squares minimization to get the homography matrix # this is done as eigenvector coresponding to smallest eigen value of A`A = H matrix u, s, v = np.linalg.svd(np.matmul(A.T, A)) # here the linalg.svd gives v_transpose # but we need just V therefore we again transpose H2to1 = np.reshape(v.T[:,-1], (3,3)) return H2to1 def computeH_norm(x1, x2): #Q2.2.2 \"\"\" Compute the normalized coordinates and also the homography matrix using computeH Args: x1 (Mx2): the matched locations of corners in img1 x2 (Mx2): the matched locations of corners in img2 Returns: H2to1: Hmography matrix after denormalization \"\"\" # Q2.2.2 # Compute the centroid of the points centroid_img_1 = np.sum(x1, axis=0)/x1.shape[0] centroid_img_2 = np.sum(x2, axis=0)/x2.shape[0] # print(f'centroid of img1 is {centroid_img_1} \\n centroid of img2 is {centroid_img_2}') # Shift the origin of the points to the centroid # let origin for img1 be centroid_img_1 and similarly for img2 #? Now translate every point such that centroid is at [0,0] moved_x1 = x1 - centroid_img_1 moved_x2 = x2 - centroid_img_2 current_max_dist_img1 = np.max(np.linalg.norm(moved_x1),axis=0) current_max_dist_img2 = np.max(np.linalg.norm(moved_x2),axis=0) # moved and scaled image 1 points scale1 = np.sqrt(2) / (current_max_dist_img1) scale2 = np.sqrt(2) / (current_max_dist_img2) moved_scaled_x1 = moved_x1 * scale1 moved_scaled_x2 = moved_x2 * scale2 # Similarity transform 1 #? We construct the transformation matrix to be 3x3 as it has to be same shape of Homography t1 = np.diag([scale1, scale1, 1]) t1[0:2,2] = -scale1*centroid_img_1 # Similarity transform 2 t2 = np.diag([scale2, scale2, 1]) t2[0:2,2] = -scale2*centroid_img_2 # Compute homography H = computeH(moved_scaled_x1, moved_scaled_x2) # Denormalization H2to1 = np.matmul(np.linalg.inv(t1), np.matmul(H, t2)) return H2to1 def create_matched_points(matches, locs1, locs2): \"\"\" Match the corners in img1 and img2 according to the BRIEF matched points Args: matches (Mx2): Vector containing the index of locs1 and locs2 which matches locs1 (Nx2): Vector containing corner positions for img1 locs2 (Nx2): Vector containing corner positions for img2 Returns: _type_: _description_ \"\"\" matched_pts = [] for i in range(matches.shape[0]): matched_pts.append(np.array([locs1[matches[i,0],0], locs1[matches[i,0],1], locs2[matches[i,1],0], locs2[matches[i,1],1]])) # remove the first dummy value and return matched_points = np.stack(matched_pts, axis=0) return matched_points def computeH_ransac(locs1, locs2, opts): \"\"\" Every iteration we init a Homography matrix using 4 corresponding points and calculate number of inliers. Finally use the Homography matrix which had max number of inliers (and these inliers as well) to find the final Homography matrix Args: locs1: location of matched points in image1 locs2: location of matched points in image2 opts: user inputs used for distance tolerance in ransac Returns: bestH2to1 : The homography matrix with max number of inliers final_inliers : Final list of inliers considered for homography \"\"\" #Q2.2.3 #Compute the best fitting homography given a list of matching points max_iters = opts.max_iters # the number of iterations to run RANSAC for inlier_tol = opts.inlier_tol # the tolerance value for considering a point to be an inlier # define size of both locs1 and locs2 num_rows = locs1.shape[0] # define a container for keeping track of inlier counts final_inlier_count = 0 final_distance_error = 10000 #? Create a boolean vector of length N where 1 = inlier and 0 = outlier print(\"Computing RANSAC\") for i in range(max_iters): test_locs1 = deepcopy(locs1) test_locs2 = deepcopy(locs2) # chose a random sample of 4 points to find H rand_index = [] rand_index = random.sample(range(int(locs1.shape[0])),k=4) rand_points_1 = [] rand_points_2 = [] for j in rand_index: rand_points_1.append(locs1[j,:]) rand_points_2.append(locs2[j,:]) test_locs1 = np.delete(test_locs1, rand_index, axis=0) test_locs2 = np.delete(test_locs2, rand_index, axis=0) correspondence_points_1 = np.vstack(rand_points_1) correspondence_points_2 = np.vstack(rand_points_2) ref_H = computeH_norm(correspondence_points_1, correspondence_points_2) inliers, inlier_count, distance_error, error_state = compute_inliers(ref_H, test_locs1, test_locs2, inlier_tol) if error_state == 1: continue if (inlier_count &gt; final_inlier_count) and (distance_error &lt; final_distance_error): final_inlier_count = inlier_count final_inliers = inliers final_corresp_points_1 = correspondence_points_1 final_corresp_points_2 = correspondence_points_2 final_distance_error = distance_error final_test_locs1 = test_locs1 final_test_locs2 = test_locs2 if final_distance_error != 10000: # print(\"original point count is\", locs1.shape[0]) # print(\"final inlier count is\", final_inlier_count) # print(\"final inlier's cumulative distance error is\", final_distance_error) delete_indexes = np.where(final_inliers==0) final_locs_1 = np.delete(final_test_locs1, delete_indexes, axis=0) final_locs_2 = np.delete(final_test_locs2, delete_indexes, axis=0) final_locs_1 = np.vstack((final_locs_1, final_corresp_points_1)) final_locs_2 = np.vstack((final_locs_2, final_corresp_points_2)) bestH2to1 = computeH_norm(final_locs_1, final_locs_2) return bestH2to1, final_inliers else: bestH2to1 = computeH_norm(correspondence_points_1, correspondence_points_2) return bestH2to1, 0 def compute_inliers(h, x1, x2, tol): \"\"\" Compute the number of inliers for a given homography matrix Args: h: Homography matrix x1 : matched points in image 1 x2 : matched points in image 2 tol: tolerance value to check for inliers Returns: inliers : indexes of x1 or x2 which are inliers inlier_count : number of total inliers dist_error_sum : Cumulative sum of errors in reprojection error calc flag : flag to indicate if H was invertible or not \"\"\" # take H inv to map points in x1 to x2 try: H = np.linalg.inv(h) except: return [1,1,1], 1, 1, 1 x2_extd = np.append(x2, np.ones((x2.shape[0],1)), axis=1) x1_extd = (np.append(x1, np.ones((x1.shape[0],1)), axis=1)) x2_est = np.zeros((x2_extd.shape), dtype=x2_extd.dtype) for i in range(x1.shape[0]): x2_est[i,:] = H @ x1_extd[i,:] x2_est = x2_est/np.expand_dims(x2_est[:,2], axis=1) dist_error = np.linalg.norm((x2_extd-x2_est),axis=1) # print(\"dist error is\", dist_error) inliers = np.where((dist_error &lt; tol), 1, 0) inlier_count = np.count_nonzero(inliers == 1) return inliers, inlier_count, np.sum(dist_error), 0 def compositeH(H2to1, template, img): \"\"\" Create a composite image after warping the template image on top of the image using the homography Args: H2to1 : Existing(already found) homography matrix template: Harry Potter (template image) img: Base image onto which we overlay Harry Potter image Returns: composite_img: Base image with overlayed Harry Potter cover \"\"\" output_shape = (img.shape[1],img.shape[0]) # destination_img = img # source_img = template h = np.linalg.inv(H2to1) # Create mask of same size as template mask = np.ones((template.shape[0], template.shape[1]))*255 mask = np.stack((mask, mask, mask), axis=2) # Warp mask by appropriate homography warped_mask = cv2.warpPerspective(mask, h, output_shape) # Warp template by appropriate homography warped_template = cv2.warpPerspective(template, h, output_shape) # Use mask to combine the warped template and the image composite_img = np.where(warped_mask, warped_template, img) return composite_img def panorama_composite(H2to1, template, img): \"\"\" Stitch two images together to form a panorama Args: H2to1: Homography Matrix template: The pano_right image img: The pano_left image Returns: composite_img: Stitched image (panorama) \"\"\" output_shape = (img.shape[1]+240,img.shape[0]+240) # destination_img = img # source_img = template h = H2to1 img_padded = np.zeros((img.shape[0]+240,img.shape[1]+240,3), dtype=img.dtype) img_padded[0:img.shape[0], 0:img.shape[1], :] = img[:,:,:] # Create mask of same size as template mask = np.ones((template.shape[0], template.shape[1]))*255 mask = np.stack((mask, mask, mask), axis=2) # Warp mask by appropriate homography warped_mask = cv2.warpPerspective(mask, h, output_shape) # Warp template by appropriate homography cv2.imshow(\"template image\", template) cv2.waitKey() cv2.imshow(\"destination image\", img) cv2.waitKey() warped_template = cv2.warpPerspective(template, h, output_shape) cv2.imshow(\"warped template\", warped_template) cv2.waitKey() # Use mask to combine the warped template and the image composite_img = np.where(warped_mask, warped_template, img_padded) return composite_img . ",
    "url": "/planar_homography/#the-pipeline",
    
    "relUrl": "/planar_homography/#the-pipeline"
  },"221": {
    "doc": "Planar Homography",
    "title": "Applying Homography Estimation in the Real World",
    "content": " ",
    "url": "/planar_homography/#applying-homography-estimation-in-the-real-world",
    
    "relUrl": "/planar_homography/#applying-homography-estimation-in-the-real-world"
  },"222": {
    "doc": "Planar Homography",
    "title": "Basic cool applications",
    "content": "If we know how a template matches to a warped image, such as: . We can then use this homography matrix to map any plane (here a different book cover) onto our destination image . ",
    "url": "/planar_homography/#basic-cool-applications",
    
    "relUrl": "/planar_homography/#basic-cool-applications"
  },"223": {
    "doc": "Planar Homography",
    "title": "AR Video",
    "content": "Here we use the same book-cover homography mapping but onto a sequence of frames of a video . ",
    "url": "/planar_homography/#ar-video",
    
    "relUrl": "/planar_homography/#ar-video"
  },"224": {
    "doc": "Planar Homography",
    "title": "Panorama Stitching",
    "content": "During my visit to Ohiopyle, I took few pictures of the river. Let’s back to the fact that homography works well for far away scenes, where the large distance from camera to landscape makes the relative distances of objects in the landscape negligible. In such cases even small translations of the camera have a small effect on the landscape itself. However, since the scene at ohiopyle was not too far away, any translation would yield a bad homography matrix and cause shoddy stitching. Therefore I tried to mitigate this by only rotating about my hip (to ensure no translational movement) while taking the two views. The results of the stitching are shown below: . ",
    "url": "/planar_homography/#panorama-stitching",
    
    "relUrl": "/planar_homography/#panorama-stitching"
  },"225": {
    "doc": "Planar Homography",
    "title": "Acknowledgement and References",
    "content": "A lot of images are taken from the lecture slides during my computer vision class at CMU. These were taught by Prof. Kris Kitani and Prof. Deva Ramanan . These slides are publicly available (slides) . ",
    "url": "/planar_homography/#acknowledgement-and-references",
    
    "relUrl": "/planar_homography/#acknowledgement-and-references"
  },"226": {
    "doc": "Planar Homography",
    "title": "My Ohiopyle trip",
    "content": ". ",
    "url": "/planar_homography/#my-ohiopyle-trip",
    
    "relUrl": "/planar_homography/#my-ohiopyle-trip"
  },"227": {
    "doc": "Planar Homography",
    "title": "Helper Functions",
    "content": "The helper function in this framework is shown below: . import numpy as np import cv2 import scipy.io as sio from matplotlib import pyplot as plt import skimage.feature PATCHWIDTH = 9 def briefMatch(desc1,desc2,ratio): matches = skimage.feature.match_descriptors(desc1,desc2,'hamming',cross_check=True,max_ratio=ratio) return matches def plotMatches(im1,im2,matches,locs1,locs2): fig, ax = plt.subplots(nrows=1, ncols=1) im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY) im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY) plt.axis('off') skimage.feature.plot_matches(ax,im1,im2,locs1,locs2,matches,matches_color='r',only_matches=True) plt.show() return def makeTestPattern(patchWidth, nbits): np.random.seed(0) compareX = patchWidth*patchWidth * np.random.random((nbits,1)) compareX = np.floor(compareX).astype(int) np.random.seed(1) compareY = patchWidth*patchWidth * np.random.random((nbits,1)) compareY = np.floor(compareY).astype(int) return (compareX, compareY) def computePixel(img, idx1, idx2, width, center): halfWidth = width // 2 col1 = idx1 % width - halfWidth row1 = idx1 // width - halfWidth col2 = idx2 % width - halfWidth row2 = idx2 // width - halfWidth return 1 if img[int(center[0]+row1)][int(center[1]+col1)] &lt; img[int(center[0]+row2)][int(center[1]+col2)] else 0 def computeBrief(img, locs): patchWidth = 9 nbits = 256 compareX, compareY = makeTestPattern(patchWidth,nbits) m, n = img.shape halfWidth = patchWidth//2 locs = np.array(list(filter(lambda x: halfWidth &lt;= x[0] &lt; m-halfWidth and halfWidth &lt;= x[1] &lt; n-halfWidth, locs))) desc = np.array([list(map(lambda x: computePixel(img, x[0], x[1], patchWidth, c), zip(compareX, compareY))) for c in locs]) return desc, locs def corner_detection(img, sigma): # fast method result_img = skimage.feature.corner_fast(img, n=PATCHWIDTH, threshold=sigma) locs = skimage.feature.corner_peaks(result_img, min_distance=1) return locs def loadVid(path): # Create a VideoCapture object and read from input file # If the input is the camera, pass 0 instead of the video file name cap = cv2.VideoCapture(path) # Append frames to list frames = [] # Check if camera opened successfully if cap.isOpened()== False: print(\"Error opening video stream or file\") # Read until video is completed while(cap.isOpened()): # Capture frame-by-frame ret, frame = cap.read() if ret: #Store the resulting frame frames.append(frame) else: break # When everything done, release the video capture object cap.release() frames = np.stack(frames) return frames . ",
    "url": "/planar_homography/#helper-functions",
    
    "relUrl": "/planar_homography/#helper-functions"
  },"228": {
    "doc": "Planar Homography",
    "title": "Planar Homography",
    "content": ". | Before you Begin | PDFs | Some Basics on Camera Projection . | Projection of 3D to 2D image plane | Camera Matrices . | Generic Representation | Alternate notation of camera matrices | . | . | The Homography Situation . | Single View | Multiple Views | Limitations of Planar Homography | . | Implementation of Homography Estimation . | The Pipeline . | Keypoint Detection | Keypoint Descriptors | Calculating the Homography Matrix | (Bonus) RANSAC: Rejecting outliers during our homography calculation | Implementation of above steps | RANSAC and Construction of Composite Image | . | . | Applying Homography Estimation in the Real World . | Basic cool applications | AR Video | Panorama Stitching | . | Acknowledgement and References . | My Ohiopyle trip | . | Helper Functions | . ",
    "url": "/planar_homography/",
    
    "relUrl": "/planar_homography/"
  },"229": {
    "doc": "Recap on Probability",
    "title": "Before you Begin",
    "content": "Ref Book. Probabalistic Robotics . ",
    "url": "/docs/SLAM/Probability_review.html#before-you-begin",
    
    "relUrl": "/docs/SLAM/Probability_review.html#before-you-begin"
  },"230": {
    "doc": "Recap on Probability",
    "title": "Discrete and Continuous Variables",
    "content": " ",
    "url": "/docs/SLAM/Probability_review.html#discrete-and-continuous-variables",
    
    "relUrl": "/docs/SLAM/Probability_review.html#discrete-and-continuous-variables"
  },"231": {
    "doc": "Recap on Probability",
    "title": "Discrete Variables and Notations",
    "content": "Here the value X in P(X) can take on any value X=x_i, just that x_i are discrete points . However, keep in my that we formally call the below function Probability Mass Function . ",
    "url": "/docs/SLAM/Probability_review.html#discrete-variables-and-notations",
    
    "relUrl": "/docs/SLAM/Probability_review.html#discrete-variables-and-notations"
  },"232": {
    "doc": "Recap on Probability",
    "title": "Continuous Random Variables",
    "content": "Here the value of X in p(X) can take on a continuous variable X=x, where x is a smooth function . Now, here we use lower_case p to denote the p(x) since in the continuous probability world, we cannot speak in terms of absolute probability, but in terms of a density function: . ",
    "url": "/docs/SLAM/Probability_review.html#continuous-random-variables",
    
    "relUrl": "/docs/SLAM/Probability_review.html#continuous-random-variables"
  },"233": {
    "doc": "Recap on Probability",
    "title": "Get Absolute Probability from PDF",
    "content": ". ",
    "url": "/docs/SLAM/Probability_review.html#get-absolute-probability-from-pdf",
    
    "relUrl": "/docs/SLAM/Probability_review.html#get-absolute-probability-from-pdf"
  },"234": {
    "doc": "Recap on Probability",
    "title": "Understanding p(x) PDF",
    "content": "As seen above, only the integration (area under curve) gives us the absolute probability. Therefore, this p(x) must be a curve of sorts, something like this: . ",
    "url": "/docs/SLAM/Probability_review.html#understanding-px-pdf",
    
    "relUrl": "/docs/SLAM/Probability_review.html#understanding-px-pdf"
  },"235": {
    "doc": "Recap on Probability",
    "title": "Can the value of p(x) &gt; 1?",
    "content": "Yes. This is because p(x) is a PDF not absolute probability. Consider the example of a proximity sensor whose readings only range from 0m - 0.5m. The PDF for such a sensor would look like the below graph . ",
    "url": "/docs/SLAM/Probability_review.html#can-the-value-of-px--1",
    
    "relUrl": "/docs/SLAM/Probability_review.html#can-the-value-of-px--1"
  },"236": {
    "doc": "Recap on Probability",
    "title": "Joint and Contional Probability",
    "content": " ",
    "url": "/docs/SLAM/Probability_review.html#joint-and-contional-probability",
    
    "relUrl": "/docs/SLAM/Probability_review.html#joint-and-contional-probability"
  },"237": {
    "doc": "Recap on Probability",
    "title": "Joint Probability",
    "content": "Note. The calculation of absolute probability will change depending upon the nature of the variables: . ",
    "url": "/docs/SLAM/Probability_review.html#joint-probability",
    
    "relUrl": "/docs/SLAM/Probability_review.html#joint-probability"
  },"238": {
    "doc": "Recap on Probability",
    "title": "Conditional Probability",
    "content": ". ",
    "url": "/docs/SLAM/Probability_review.html#conditional-probability",
    
    "relUrl": "/docs/SLAM/Probability_review.html#conditional-probability"
  },"239": {
    "doc": "Recap on Probability",
    "title": "Marginals and Conditionals",
    "content": "To start, lets get an intuition on what a marginal or conditional may look like: . | Let’s consider a multivariate probability distribution (i.e there are say 2 random variables) | Let us consider these two variables to have their own distributions | Let these two distributions be exam grades and study time | Imagine exam_grades are distributed along y-axis, and study_time along x-axis (sorry for asking you to imagine this much :/) | Let the z-axis be a joint probability of both x and y | Now, combining everything we should have a 3D plot | . If we view this plot from the top view, we should see something like this: . ",
    "url": "/docs/SLAM/Probability_review.html#marginals-and-conditionals",
    
    "relUrl": "/docs/SLAM/Probability_review.html#marginals-and-conditionals"
  },"240": {
    "doc": "Recap on Probability",
    "title": "Crux of the Matter:",
    "content": ". | Think of conditionals as taking a slice of this cloud and evaluating distribution of exam grades given a specific study time | Think of marginals as squishing the cloud (say squishing all study-time data onto the exam-grades axis) and then studying the distribution | . ",
    "url": "/docs/SLAM/Probability_review.html#crux-of-the-matter",
    
    "relUrl": "/docs/SLAM/Probability_review.html#crux-of-the-matter"
  },"241": {
    "doc": "Recap on Probability",
    "title": "Small Leap",
    "content": "Now that you’ve understood the intuition behind marginals, here’s the math . ",
    "url": "/docs/SLAM/Probability_review.html#small-leap",
    
    "relUrl": "/docs/SLAM/Probability_review.html#small-leap"
  },"242": {
    "doc": "Recap on Probability",
    "title": "Bayes Theorem",
    "content": ". ",
    "url": "/docs/SLAM/Probability_review.html#bayes-theorem",
    
    "relUrl": "/docs/SLAM/Probability_review.html#bayes-theorem"
  },"243": {
    "doc": "Recap on Probability",
    "title": "Recap on Probability",
    "content": "{: .text-delta } . | Before you Begin | Discrete and Continuous Variables . | Discrete Variables and Notations | Continuous Random Variables | Get Absolute Probability from PDF | Understanding p(x) PDF | Can the value of p(x) &gt; 1? | . | Joint and Contional Probability . | Joint Probability | Conditional Probability | . | Marginals and Conditionals . | Crux of the Matter: | Small Leap | . | Bayes Theorem | . ",
    "url": "/docs/SLAM/Probability_review.html",
    
    "relUrl": "/docs/SLAM/Probability_review.html"
  },"244": {
    "doc": "SLAM",
    "title": "Simultaneous Localisation and Mapping",
    "content": "Understanding and Implementing SLAM for Robotics . ",
    "url": "/docs/SLAM#simultaneous-localisation-and-mapping",
    
    "relUrl": "/docs/SLAM#simultaneous-localisation-and-mapping"
  },"245": {
    "doc": "SLAM",
    "title": "SLAM",
    "content": " ",
    "url": "/docs/SLAM",
    
    "relUrl": "/docs/SLAM"
  },"246": {
    "doc": "Spatial Pyramids and Bag of Words",
    "title": "Before you Begin",
    "content": "Reference Book 1 Reference Book 2 . ",
    "url": "/docs/Computer%20Vision/bag_of_words.html#before-you-begin",
    
    "relUrl": "/docs/Computer%20Vision/bag_of_words.html#before-you-begin"
  },"247": {
    "doc": "Spatial Pyramids and Bag of Words",
    "title": "PDFs",
    "content": "Assignment Questionnaire . My Answers . ",
    "url": "/docs/Computer%20Vision/bag_of_words.html#pdfs",
    
    "relUrl": "/docs/Computer%20Vision/bag_of_words.html#pdfs"
  },"248": {
    "doc": "Spatial Pyramids and Bag of Words",
    "title": "Spatial Pyramids and Bag of Words",
    "content": ". ",
    "url": "/docs/Computer%20Vision/bag_of_words.html",
    
    "relUrl": "/docs/Computer%20Vision/bag_of_words.html"
  },"249": {
    "doc": "Camera Models and Projections",
    "title": "Introduction",
    "content": " ",
    "url": "/docs/Computer%20Vision/camera_model.html#introduction",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html#introduction"
  },"250": {
    "doc": "Camera Models and Projections",
    "title": "Basics",
    "content": ". In the above image, the division by Z happens implicitly due to homogenous coordinate notation . ",
    "url": "/docs/Computer%20Vision/camera_model.html#basics",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html#basics"
  },"251": {
    "doc": "Camera Models and Projections",
    "title": "Account for other issues in image frame",
    "content": "We will introduce 3 coordinate systems below: . | Camera Coordinate Frame | Image Coordinate Frame (where homogenous notation is used as there is no z-axis information) | World Coordinate Information | . Sometimes the camera coordinate frame and the image coordinate frame is misaligned as shown below: . ________________________________________________________________________________________________________________ . ",
    "url": "/docs/Computer%20Vision/camera_model.html#account-for-other-issues-in-image-frame",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html#account-for-other-issues-in-image-frame"
  },"252": {
    "doc": "Camera Models and Projections",
    "title": "Intrinsic and Extrinsic Decomposition",
    "content": ". Lesson Learnt: . If we follow the how a 3D point gets left multiplied by extrinsic and then by intrinsic the coordinate frame intuition we derive is: . (3D Point -&gt; Extrinsic -&gt; Intrinsic) = (World Frame -&gt; Camera Frame -&gt; Image Frame) . t = Translation (last column of extrinsic matrix) R = Rotation (first 3x3 part of extrinsic matrix) . ",
    "url": "/docs/Computer%20Vision/camera_model.html#intrinsic-and-extrinsic-decomposition",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html#intrinsic-and-extrinsic-decomposition"
  },"253": {
    "doc": "Camera Models and Projections",
    "title": "Final Version of Camera Model (I prefer this)",
    "content": ". ",
    "url": "/docs/Computer%20Vision/camera_model.html#final-version-of-camera-model-i-prefer-this",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html#final-version-of-camera-model-i-prefer-this"
  },"254": {
    "doc": "Camera Models and Projections",
    "title": "Camera Models and Projections",
    "content": ". | Introduction . | Basics | Account for other issues in image frame | Intrinsic and Extrinsic Decomposition . | Lesson Learnt: | . | Final Version of Camera Model (I prefer this) | . | . ",
    "url": "/docs/Computer%20Vision/camera_model.html",
    
    "relUrl": "/docs/Computer%20Vision/camera_model.html"
  },"255": {
    "doc": "Home",
    "title": "About Me",
    "content": "Hello! . I’m Sushanth and I currently work on Computer Vision applications for surgical robots at Intuitive Surgical. Resume . In 2024, I earned my Master’s in Robotic Systems Development (MRSD) from Carnegie Mellon University, specializing in Computer Vision. This website originally served as a resource for revisiting key vision concepts. ",
    "url": "/#about-me",
    
    "relUrl": "/#about-me"
  },"256": {
    "doc": "Home",
    "title": "Previous Work",
    "content": "Part-time research at Kantor Lab at CMU’s Field Robotics Center, working on Robotics for Agriculture. I work on a robotic grapevine pruning project involving 3D skeletonization of grapevine pointclouds for pruning weight estimation. . I worked on the perception team at Niqo Robotics. . Edhitha was a student team which has continuously taken part at the AUVSI SUAS competition held at Maryland, USA. I was part of the 2016 team where we finished 5th amongst 60 international teams and was the team lead in 2017. ",
    "url": "/#previous-work",
    
    "relUrl": "/#previous-work"
  },"257": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"258": {
    "doc": "Computer Vision Libraries in C++",
    "title": "Libraries for Computer Vision in C++",
    "content": " ",
    "url": "/docs/Vision%20with%20C++#libraries-for-computer-vision-in-c",
    
    "relUrl": "/docs/Vision with C++#libraries-for-computer-vision-in-c"
  },"259": {
    "doc": "Computer Vision Libraries in C++",
    "title": "Computer Vision Libraries in C++",
    "content": " ",
    "url": "/docs/Vision%20with%20C++",
    
    "relUrl": "/docs/Vision with C++"
  },"260": {
    "doc": "3D Reconstruction",
    "title": "Before you Begin",
    "content": "Reference Book 1 Reference Book 2 . ",
    "url": "/3D_reconstruction/#before-you-begin",
    
    "relUrl": "/3D_reconstruction/#before-you-begin"
  },"261": {
    "doc": "3D Reconstruction",
    "title": "PDFs",
    "content": "Assignment Questionnaire . My Answers . ",
    "url": "/3D_reconstruction/#pdfs",
    
    "relUrl": "/3D_reconstruction/#pdfs"
  },"262": {
    "doc": "3D Reconstruction",
    "title": "Uses of Mutli-View Geometry",
    "content": ". ",
    "url": "/3D_reconstruction/#uses-of-mutli-view-geometry",
    
    "relUrl": "/3D_reconstruction/#uses-of-mutli-view-geometry"
  },"263": {
    "doc": "3D Reconstruction",
    "title": "Background: Structure from Motion (SFM)",
    "content": "The basis for classical 3D vision is viewing an object from multiple views to give 3D understanding. In humans, we don’t have to move to perceive 3D because our eyes(cameras) are already seperated by a fixed distance. This is the same as having one camera move by a fixed distance. Hence, we see “stereo depth vision” . ",
    "url": "/3D_reconstruction/#background-structure-from-motion-sfm",
    
    "relUrl": "/3D_reconstruction/#background-structure-from-motion-sfm"
  },"264": {
    "doc": "3D Reconstruction",
    "title": "Theory: Solving for camera parameters in the presence of scale ambiguity",
    "content": "Remember we had the following equation: . The lambda above is account for a scale factor which is ambiguous. This equation can also be written as: . ",
    "url": "/3D_reconstruction/#theory-solving-for-camera-parameters-in-the-presence-of-scale-ambiguity",
    
    "relUrl": "/3D_reconstruction/#theory-solving-for-camera-parameters-in-the-presence-of-scale-ambiguity"
  },"265": {
    "doc": "3D Reconstruction",
    "title": "Solving for Camera Params: Direct Linear Transform",
    "content": "Now, the above equation is a similarity equation. To solve the equation we make use of a neat trick called Direct Linear Transform . PX should give the same ray (vector) as x, hence their cross product would be zero . ",
    "url": "/3D_reconstruction/#solving-for-camera-params-direct-linear-transform",
    
    "relUrl": "/3D_reconstruction/#solving-for-camera-params-direct-linear-transform"
  },"266": {
    "doc": "3D Reconstruction",
    "title": "Actual Derivation",
    "content": ". ",
    "url": "/3D_reconstruction/#actual-derivation",
    
    "relUrl": "/3D_reconstruction/#actual-derivation"
  },"267": {
    "doc": "3D Reconstruction",
    "title": "Theory: Epipolar Geometry",
    "content": ". | Simply put, epipolar geometry maps a point in one view, to a line in another view | Epipolar Geometry is purely determined by camera intrinsics and camera extrinsics | . ",
    "url": "/3D_reconstruction/#theory-epipolar-geometry",
    
    "relUrl": "/3D_reconstruction/#theory-epipolar-geometry"
  },"268": {
    "doc": "3D Reconstruction",
    "title": "Essential Matrix: Maps a point -&gt; to a line",
    "content": ". Derivation of Essential Matrix: Longuet Higgins . | Recall the skew-symmetric form of a matrix can encode cross products . | . | We can use this to show how three vectors can define the volume of a parallelpiped: . | . | Now, given a calibrated camera (i.e. known intrinsics) we can capture a 3D point in two views . | . | . Longuet Higgins Derivation . The above derivation tells us in simplicity: . | The volume of the parallelpiped (as seen previously is defined by a.(b x c)) is zero | This means that three vectors are in one plane | Which makes sense since the epipolar plane is what connects the 2 camera centers and the 3D point | | . Difference Between Essential Matrix and Homography Matrix . How does this Essential Matrix map a point to a line (where is the math?) . ",
    "url": "/3D_reconstruction/#essential-matrix-maps-a-point---to-a-line",
    
    "relUrl": "/3D_reconstruction/#essential-matrix-maps-a-point---to-a-line"
  },"269": {
    "doc": "3D Reconstruction",
    "title": "Fundamental Matrix",
    "content": ". ",
    "url": "/3D_reconstruction/#fundamental-matrix",
    
    "relUrl": "/3D_reconstruction/#fundamental-matrix"
  },"270": {
    "doc": "3D Reconstruction",
    "title": "Structure From Motion Step 1: Estimating Fundamental Matrix:",
    "content": ". . We are given two images of the same object from two different views: . ''' Q2.1: Eight Point Algorithm Input: pts1, Nx2 Matrix pts2, Nx2 Matrix M, a scalar parameter computed as max (imwidth, imheight) Output: F, the fundamental matrix HINTS: (1) Normalize the input pts1 and pts2 using the matrix T. (2) Setup the eight point algorithm's equation. (3) Solve for the least square solution using SVD. (4) Use the function `_singularize` (provided) to enforce the singularity condition. (5) Use the function `refineF` (provided) to refine the computed fundamental matrix. (Remember to usethe normalized points instead of the original points) (6) Unscale the fundamental matrix ''' def eightpoint(pts1, pts2, M): \"\"\" Compute the normalized coordinates and also the fundamental matrix using computeH Args: x1 (Mx2): the matched locations of corners in img1 x2 (Mx2): the matched locations of corners in img2 Returns: F2to1: Fundamental matrix after denormalization \"\"\" # Compute the centroid of the points x1, x2 = pts1, pts2 # Doing the M normaliazation moved_scaled_x1 = x1/M moved_scaled_x2 = x2/M t = np.diag([1/M, 1/M, 1]) # Compute Fundamental Matrix F = computeF(moved_scaled_x1, moved_scaled_x2) # Refine and then enforce singularity constraint F = _singularize(F) F = refineF(F, moved_scaled_x1, moved_scaled_x2) # Denormalization F2to1 = np.matmul(t.T, (F @ t)) F2to1 = F2to1/F2to1[2,2] return F2to1 def computeF(x1, x2): \"\"\" Computes the fundamental based on matching points in both images Args: x1: keypoints in image 1 x2: keypoints in image 2 Returns: H2to1: the fundamental matrix \"\"\" # Define a dummy H matrix A_build = [] # Define the A matrix for (Ah = 0) (A matrix size = N*2 x 9) for i in range(x1.shape[0]): row_1 = np.array([ x2[i,0]*x1[i,0], x2[i,0]*x1[i,1], x2[i,0], x2[i,1]*x1[i,0], x2[i,1]*x1[i,1], x2[i,1], x1[i,0], x1[i,1], 1]) A_build.append(row_1) A = np.stack(A_build, axis=0) # Do the least squares minimization to get the homography matrix # this is done as eigenvector coresponding to smallest eigen value of A`A = H matrix u, s, v = np.linalg.svd(A) # here the linalg.svd gives v_transpose # but we need just V therefore we again transpose F2to1 = np.reshape(v.T[:,-1], (3,3)) return F2to1 def check_and_create_directory(dir_path, create): \"\"\" Checks for existing directories and creates if unavailable [input] * dir_path : path to be checked * create : tag to specify only checking path or also creating path \"\"\" if create == 1: if not os.path.exists(dir_path): os.makedirs(dir_path) else: if not os.path.exists(dir_path): warnings.warn(f'following path could not be found: {dir_path}') if __name__ == \"__main__\": correspondence = np.load('data/some_corresp.npz') # Loading correspondences intrinsics = np.load('data/intrinsics.npz') # Loading the intrinscis of the camera K1, K2 = intrinsics['K1'], intrinsics['K2'] pts1, pts2 = correspondence['pts1'], correspondence['pts2'] im1 = plt.imread('data/im1.png') im2 = plt.imread('data/im2.png') F = eightpoint(pts1, pts2, M=np.max([*im1.shape, *im2.shape])) print(\"the fundamental matrix found was \\n\", F) # Q2.1 out_dir = \"/home/sush/CMU/Assignment_Sem_1/CV_A/Assignment_4/code/outputs\" check_and_create_directory(out_dir, create=1) np.savez_compressed( os.path.join(out_dir, 'q2_1.npz'), F, np.max([*im1.shape, *im2.shape]) ) displayEpipolarF(im1, im2, F) . Output: . ",
    "url": "/3D_reconstruction/#structure-from-motion-step-1-estimating-fundamental-matrix",
    
    "relUrl": "/3D_reconstruction/#structure-from-motion-step-1-estimating-fundamental-matrix"
  },"271": {
    "doc": "3D Reconstruction",
    "title": "Estimate Essential Matrix from Fundamental Matrix (given K1 and K2)",
    "content": "''' Q3.1: Compute the essential matrix E. Input: F, fundamental matrix K1, internal camera calibration matrix of camera 1 K2, internal camera calibration matrix of camera 2 Output: E, the essential matrix ''' def essentialMatrix(F, K1, K2): E = (K2.T @ F) @ K1 E = E/E[2,2] print(\"rank of E is\", np.linalg.matrix_rank(E)) return E if __name__ == \"__main__\": correspondence = np.load('data/some_corresp.npz') # Loading correspondences intrinsics = np.load('data/intrinsics.npz') # Loading the intrinscis of the camera K1, K2 = intrinsics['K1'], intrinsics['K2'] pts1, pts2 = correspondence['pts1'], correspondence['pts2'] im1 = plt.imread('data/im1.png') im2 = plt.imread('data/im2.png') # ----- TODO ----- # YOUR CODE HERE F = eightpoint(pts1, pts2, M=np.max([*im1.shape, *im2.shape])) E = essentialMatrix(F, K1, K2) print(\"E is \\n\", E) . ",
    "url": "/3D_reconstruction/#estimate-essential-matrix-from-fundamental-matrix-given-k1-and-k2",
    
    "relUrl": "/3D_reconstruction/#estimate-essential-matrix-from-fundamental-matrix-given-k1-and-k2"
  },"272": {
    "doc": "3D Reconstruction",
    "title": "Triangulation",
    "content": " ",
    "url": "/3D_reconstruction/#triangulation",
    
    "relUrl": "/3D_reconstruction/#triangulation"
  },"273": {
    "doc": "3D Reconstruction",
    "title": "triangulate3D",
    "content": ". | Here we fix one camera (extrinsic matrix = Identity matrix). Now, we know correspondence points in image1 and image2. | Using that we found the Fundamental Matrix | . Now, to estimate the 3D location of these points, we need . | Camera matrices (extrinsic*intrinsic) for both cameras M1 and M2 | Image points (x,y) which correspond to each other | Direct Linear Transform | . DLT was mentioned above, but small recap: . After finding the 3D points, we will reproject them back onto the image and compare them with our original correspondence points (which we either manually selected or got from some keypoint detector like ORB or BRIEF) . The formula for reprojection error in this case is: . def triangulate(C1, pts1, C2, pts2): \"\"\" Find the 3D coords of the keypoints We are given camera matrices and 2D correspondences. We can therefore find the 3D points (refer L17 (Camera Models) of CV slides) Note. We can't just use x = PX to compute the 3D point X because of scale ambiguity i.e the ambiguity can be rep. as x = alpha*Px (we cannot find alpha) Therefore we need to do DLT just like the case of homography (see L14 (2D transforms) CVB slide 61) Args: C1 : the 3x4 camera matrix of camera 1 pts1 : img coords of keypoints in camera 1 (Nx2) C2 : the 3x4 camera matrix of camera 2 pts2 : img coords of keypoints in camera 2 (Nx2) Returns: P : the estimated 3D point for the given pair of keypoint correspondences err : the reprojection error \"\"\" P = np.zeros(shape=(1,3)) err = 0 for i in range(len(pts1)): # get the camera 1 matrix p1_1 = C1[0,:] p2_1 = C1[1,:] p3_1 = C1[2,:] # get the camera 2 matrix p1_2 = C2[0,:] p2_2 = C2[1,:] p3_2 = C2[2,:] x, y = pts1[i,0], pts1[i,1] x2, y2 = pts2[i,0], pts2[i,1] # calculate the A matrix for this point correspondence A = np.array([y*p3_1 - p2_1 , p1_1 - x*p3_1 , y2*p3_2 - p2_2 , p1_2 - x2*p3_2]) u, s, v = np.linalg.svd(A) # here the linalg.svd gives v_transpose # but we need just V therefore we again transpose X = v.T[:,-1] # print(\"X is\", X) X = X.T X = np.expand_dims(X,axis=0) # print(\"X after transpose and expand is\", X) # convert X to homogenous coords X = X/X[0,3] # print(\"X after normalizing is\", X) P = np.concatenate((P, X[:,0:3]), axis=0) X = X.T # find the error for this projection # 3x1 = 3x4 . 3x1 pt_1 = ((C1 @ X)/(C1 @ X)[2,0])[0:2,0] pt_2 = ((C2 @ X)/(C2 @ X)[2,0])[0:2,0] # calculate the reporjection error err += np.linalg.norm(pt_1 - pts1[i,:])**2 + np.linalg.norm(pt_2 - pts2[i,:])**2 print(\"error in this iteration is\", err) P = P[1:,:] return P, err . Summary . | Given two camera matrices and keypoint correspondences for two views, we triangulated the point (found 3D point) | We found the reprojection error for this estimated 3D point | . ",
    "url": "/3D_reconstruction/#triangulate3d",
    
    "relUrl": "/3D_reconstruction/#triangulate3d"
  },"274": {
    "doc": "3D Reconstruction",
    "title": "Using Triangulate to Find Second Camera Matrix after fixing First Camera Matrix = Identity",
    "content": "Previsously we saw that we need an M2 to triangulate, but we don’t have an M2 yet :/. However, since our first camera is fixed (identity) we can find the camera matrix M2 of our second camera as: . def camera2(E): U,S,V = np.linalg.svd(E) m = S[:2].mean() E = U.dot(np.array([[m,0,0], [0,m,0], [0,0,0]])).dot(V) U,S,V = np.linalg.svd(E) W = np.array([[0,-1,0], [1,0,0], [0,0,1]]) if np.linalg.det(U.dot(W).dot(V))&lt;0: W = -W M2s = np.zeros([3,4,4]) M2s[:,:,0] = np.concatenate([U.dot(W).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,1] = np.concatenate([U.dot(W).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,2] = np.concatenate([U.dot(W.T).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) M2s[:,:,3] = np.concatenate([U.dot(W.T).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1) return M2s . Note: The above function gives Four possible values for M2 . Why the 4 options? . Now there are 2 checks we can use to find which is the right camera: . | Determinant(Rotation component of M2) = 1 (so that the rotation belongs to SO(3)) | All Z values should be positive (i.e. the 3D point should be in front of both the cameras right?) | . ",
    "url": "/3D_reconstruction/#using-triangulate-to-find-second-camera-matrix-after-fixing-first-camera-matrix--identity",
    
    "relUrl": "/3D_reconstruction/#using-triangulate-to-find-second-camera-matrix-after-fixing-first-camera-matrix--identity"
  },"275": {
    "doc": "3D Reconstruction",
    "title": "Combining the above two functions",
    "content": "Now we have point correspondences, M1 and 4 M2’s. Therefore we’ll try to triangulate points based on the correct criteria for camera orientations. Additionally we’ll also try to minimize reprojection error: . # iterate over M1(fixed) and M2(4 possibilites) by passing them to triangulate for i in range(M2.shape[2]): M2_current = M2[:,:,i] # build the C1 and C2: pts_in_3d, err = triangulate((K1 @ M1), pts1, (K2 @ M2_current), pts2) if err &lt; err_min and (np.where(pts_in_3d[:,2] &lt; 0)[0].shape[0] == 0): print(\"satisfies the error criteria\") err_min = err best_M2_i = i best_pts_3d = pts_in_3d if (best_M2_i is not None) and (best_pts_3d is not None): print(\"min err is\", err_min) # return M2, C2, w(3d points), M1, C1 return M2[:,:,best_M2_i], (K2 @ M2[:,:,best_M2_i]), best_pts_3d, M1, (K1 @ M1) # last entry is C1 . Finally we all together have: . | our best_3d_points | correct M2 matrix | . Results of Triangulation on Input Images . ",
    "url": "/3D_reconstruction/#combining-the-above-two-functions",
    
    "relUrl": "/3D_reconstruction/#combining-the-above-two-functions"
  },"276": {
    "doc": "3D Reconstruction",
    "title": "Bundle Adjustment",
    "content": ". We know that the error in the triangulation is basically difference between the projection of a 3D point and the actual point in 2D on the image. Now, we will move around the 3D points slightly and check in which orientation the reprojection error comes to a global minimum. The formula for the above operation is shown below: . The process we will follow now is very code specific. An explanation for only this below code is shown, where we will only be minimizing the rotation and translation (M2 matrix) error. High level procedure . | Use the 2D point correspondences to find the Fundamental Matrix (along with RANSAC to find the inlier points) | Use the inliers to find our best F (fundamental matrix) | Compute an initial guess for M2 by using our old findM2 function | Now, the above function would have given us 3D points (P_init) and an M2_init | Now, we have compiled the following: . | M1 and K1 | M2_init and K2 | F and E (E = (K2.T @ F) @ K1) | . | . Having the above content, we will need to derive our reprojection error. We will do this in the RodriguesResidual function: . RodriguesResidual: rodriguesResidual(x, K1, M1, p1, K2, p2) . | x basically contains the translation and rotation of camera2. We can therefore get M2 from x | We can find the camera matrices C1 = K1 @ M1, C2 = K2 @ M2 | . | Use the above equation to get p1’ and p2’ | Compare p1’ and p1, p2’ and p2, to get the reprojection error we need in both cameras | . Now we have a function which will give us reprojection error for a given M2 matrix. Now lets see how we’ll use this reporjection error to optimize our M2 . Optimization of M2 . Now that we have a function which will give us reprojection error for any given M2, lets minimize this error by moving around our 3D points slightly such that our reprojection error (for all points cumulative) reduces . We do this using the scipy.optimize.minimize function . # just some repackaging/preprocessing to give x to rodriguesResidual x0 = P_init.flatten() x0 = np.append(x0, r2_0.flatten()) x0 = np.append(x0, t2_0.flatten()) # optimization step x_opt, _ = scipy.optimize.minimze(rodriguesResidual, x0, args=(K1, M1, p1, K2, p2)) . Finally our x_opt i.e x_optimal will have the correct rotation and translation of camera 2 and the corrected 3D points . ''' Q5.3 Bundle adjustment. Input: K1, the intrinsics of camera 1 M1, the extrinsics of camera 1 p1, the 2D coordinates of points in image 1 K2, the intrinsics of camera 2 M2_init, the initial extrinsics of camera 1 (get this from findM2) p2, the 2D coordinates of points in image 2 P_init, the initial 3D coordinates of points (get this also from findM2) Output: M2, the optimized extrinsics of camera 1 P2, the optimized 3D coordinates of points o1, the starting objective function value with the initial input o2, the ending objective function value after bundle adjustment Hints: (1) Use the scipy.optimize.minimize function to minimize the objective function, rodriguesResidual. You can try different (method='..') in scipy.optimize.minimize for best results. ''' def bundleAdjustment(K1, M1, p1, K2, M2_init, p2, P_init): # given M2_init decompose it into R and t R2 = M2_init[:,0:3] t2 = M2_init[:,3] r2 = invRodrigues(R2) x_start = P_init.flatten() x_start = np.append(x_start, np.append(r2.flatten(), t2)) obj_start = rodriguesResidual(x_start, K1, M1, p1, K2, p2) print(\"x_start shape is\", x_start.shape) # optimization step from scipy.optimize import minimize x_optimized_obj = minimize(residual_norm, x_start, args=(K1, M1, p1, K2, p2), method='Powell') print(\"x_end shape is\", x_optimized_obj.x.shape) x_optimized = x_optimized_obj.x obj_end = rodriguesResidual(x_optimized, K1, M1, p1, K2, p2) # recompute the M2 and P # decompose x P_final = x_optimized[0:-6] P_shape_req = int(P_final.shape[0]/3) P_final = np.reshape(P_final, newshape=(P_shape_req,3)) r2_final = x_optimized[-6:-3] # reshape to 3x1 to feed to inverse rodrigues r2_final = r2_final.reshape(3,1) # reshape translation matrix to combine in transformation matrix t2_final = x_optimized[-3:].reshape(3,1) # compose the C2 matrix R2_final = rodrigues(r2_final) M2_final = np.hstack((R2_final, t2_final)) return M2_final, P_final, obj_start, obj_end . Results on optimizing points after bundle adjustment . ",
    "url": "/3D_reconstruction/#bundle-adjustment",
    
    "relUrl": "/3D_reconstruction/#bundle-adjustment"
  },"277": {
    "doc": "3D Reconstruction",
    "title": "Final Pipeline Including RANSAC",
    "content": "if __name__ == \"__main__\": np.random.seed(1) #Added for testing, can be commented out some_corresp_noisy = np.load('data/some_corresp_noisy.npz') # Loading correspondences intrinsics = np.load('data/intrinsics.npz') # Loading the intrinscis of the camera K1, K2 = intrinsics['K1'], intrinsics['K2'] noisy_pts1, noisy_pts2 = some_corresp_noisy['pts1'], some_corresp_noisy['pts2'] im1 = plt.imread('data/im1.png') im2 = plt.imread('data/im2.png') templeCoords = np.load('data/templeCoords.npz') temple_pts1 = np.hstack([templeCoords[\"x1\"], templeCoords[\"y1\"]]) #? getting the F matrix from noisy correspondences M = np.max([*im1.shape, *im2.shape]) F, inliers = ransacF(noisy_pts1, noisy_pts2, M, im1, im2) inlier_pts1, inlier_pts2 = inliers[0], inliers[1] print(\"shape of noisy_pts1 is\", noisy_pts1.shape) print(\"shape of inlier_pts1 is\", inlier_pts1.shape) F_naieve = eightpoint(noisy_pts1, noisy_pts2, M) # use displayEpipolarF to compare how ransac_F and naieve_F behave # displayEpipolarF(im1, im2, F) # displayEpipolarF(im1, im2, F_naieve) # Simple Tests to verify your implementation: from scipy.spatial.transform import Rotation as sRot rotVec = sRot.random() mat = rodrigues(rotVec.as_rotvec()) assert(np.linalg.norm(rotVec.as_rotvec() - invRodrigues(mat)) &lt; 1e-3) assert(np.linalg.norm(rotVec.as_matrix() - mat) &lt; 1e-3) #? Getting the initial guess for M2 and P # Assuming the rotation and translation of camera1 is zero M1 = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0]]) M2_init, C2, P_init, M1, C1 = findM2(F, inlier_pts1, inlier_pts2, intrinsics) print(\"M2 shape is\", M2_init) # Optimize the M2 M2_final, P_final, start_obj, end_obj = bundleAdjustment( K1, M1, inlier_pts1, K2, M2_init, inlier_pts2, P_init ) print(\"error before optimization is\", np.linalg.norm(start_obj)**2) print(\"error after optimization is\", np.linalg.norm(end_obj)**2) # compare the old M2 to optimized M2 plot_3D_dual(P_init, P_final) . ",
    "url": "/3D_reconstruction/#final-pipeline-including-ransac",
    
    "relUrl": "/3D_reconstruction/#final-pipeline-including-ransac"
  },"278": {
    "doc": "3D Reconstruction",
    "title": "Effects of RANSAC",
    "content": "RANSAC was used even before bundle adjustment, to remove noisy coorespondences for the initial best gues of the Fundamental Matrix. def ransacF(pts1, pts2, M, im1, im2, nIters=100, tol=10): \"\"\" Every iteration we init a Fundamental matrix using 4 corresponding points and calculate number of inliers. Finally use the Homography matrix which had max number of inliers (and these inliers as well) to find the final Fundamental matrix Args: pts1: location of matched points in image1 pts2: location of matched points in image2 opts: user inputs used for distance tolerance in ransac Returns: bestH2to1 : The Fundamental matrix with max number of inliers inlier_points : Final list of inliers found for best RANSAC iteration \"\"\" max_iters = nIters # the number of iterations to run RANSAC for inlier_tol = tol # the tolerance value for considering a point to be an inlier locs1 = pts1 locs2 = pts2 # define size of both locs1 and locs2 num_rows = locs1.shape[0] # define a container for keeping track of inlier counts final_inlier_count = 0 final_distance_error = 10000000000 #? Create a boolean vector of length N where 1 = inlier and 0 = outlier print(\"Computing RANSAC\") for i in range(max_iters): test_locs1 = deepcopy(locs1) test_locs2 = deepcopy(locs2) # chose a random sample of 4 points to find H rand_index = [] rand_index = random.sample(range(int(locs1.shape[0])), k=8) rand_points_1 = [] rand_points_2 = [] for j in rand_index: rand_points_1.append(locs1[j,:]) rand_points_2.append(locs2[j,:]) test_locs1 = np.delete(test_locs1, rand_index, axis=0) test_locs2 = np.delete(test_locs2, rand_index, axis=0) correspondence_points_1 = np.vstack(rand_points_1) correspondence_points_2 = np.vstack(rand_points_2) ref_F = eightpoint(correspondence_points_1, correspondence_points_2, M) inliers, inlier_count, distance_error, error_state = compute_inliers(ref_F, test_locs1, test_locs2, inlier_tol, im1, im2) if error_state == 1: continue if (inlier_count &gt; final_inlier_count): final_inlier_count = inlier_count final_inliers = inliers final_corresp_points_1 = correspondence_points_1 final_corresp_points_2 = correspondence_points_2 final_distance_error = distance_error final_test_locs1 = test_locs1 final_test_locs2 = test_locs2 if final_distance_error != 100000000: # print(\"original point count is\", locs1.shape[0]) print(\"final inlier count is\", final_inlier_count) print(\"final inlier's cumulative distance error is\", final_distance_error) delete_indexes = np.where(final_inliers==0) final_locs_1 = np.delete(final_test_locs1, delete_indexes, axis=0) final_locs_2 = np.delete(final_test_locs2, delete_indexes, axis=0) final_locs_1 = np.vstack((final_locs_1, final_corresp_points_1)) final_locs_2 = np.vstack((final_locs_2, final_corresp_points_2)) bestH2to1 = eightpoint(final_locs_1, final_locs_2, M) return bestH2to1, [final_locs_1, final_locs_2] else: print(\"SOMETHING WRONG\") bestH2to1 = eightpoint(correspondence_points_1, correspondence_points_2, M) return bestH2to1, 0 . ",
    "url": "/3D_reconstruction/#effects-of-ransac",
    
    "relUrl": "/3D_reconstruction/#effects-of-ransac"
  },"279": {
    "doc": "3D Reconstruction",
    "title": "Tracking Real World Objects in 3D",
    "content": ". ''' Q6.1 Multi-View Reconstruction of keypoints. Input: C1, the 3x4 camera matrix pts1, the Nx3 matrix with the 2D image coordinates and confidence per row C2, the 3x4 camera matrix pts2, the Nx3 matrix with the 2D image coordinates and confidence per row C3, the 3x4 camera matrix pts3, the Nx3 matrix with the 2D image coordinates and confidence per row Output: P, the Nx3 matrix with the corresponding 3D points for each keypoint per row err, the reprojection error. ''' def MultiviewReconstruction(C1, pts1, C2, pts2, C3, pts3, Thres = 200): vis_pts_1 = np.where(pts1[:,2] &gt; Thres) vis_pts_2 = np.where(pts2[:,2] &gt; Thres) vis_pts_3 = np.where(pts3[:,2] &gt; Thres) # create a dummy vector to save the 3D points for each corresp 2D pt pts_3d = np.zeros(pts1.shape) reproj_error = np.zeros(12) overlap_all = np.intersect1d(vis_pts_1, vis_pts_2, vis_pts_3) for i in overlap_all: pts_cam_1_2, err1 = triangulate(C1, pts1[i,:-1], C2, pts2[i,:-1]) pts_cam_2_3, err2 = triangulate(C2, pts2[i,:-1], C3, pts3[i,:-1]) pts_cam_1_3, err3 = triangulate(C1, pts1[i,:-1], C3, pts3[i,:-1]) avg_pt_i = (pts_cam_1_2 + pts_cam_2_3 + pts_cam_1_3)/3 avg_err = (err1+err2+err3)/3 pts_3d[i,:] = avg_pt_i reproj_error[i] = avg_err for i in vis_pts_1[0]: # print(\"i is\", i) if i not in overlap_all: # print(\"computing\", i) if i in vis_pts_2[0]: pts_i, err = triangulate(C1, pts1[i,:-1], C2, pts2[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err elif i in vis_pts_3[0]: pts_i, err = triangulate(C1, pts1[i,:-1], C3, pts3[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err else: print(\"point not visible in 2 views\") for i in vis_pts_2[0]: # print(\"i is\", i) if i not in overlap_all: # print(\"computing\", i) if i in vis_pts_3[0]: pts_i, err = triangulate(C2, pts2[i,:-1], C3, pts3[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err elif i in vis_pts_1[0]: pts_i, err = triangulate(C1, pts1[i,:-1], C2, pts2[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err else: print(\"point not visible in 2 views\") for i in vis_pts_3[0]: if i not in overlap_all: # print(\"computing\", i) if i in vis_pts_1[0]: pts_i, err = triangulate(C1, pts1[i,:-1], C3, pts3[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err elif i in vis_pts_2[0]: pts_i, err = triangulate(C1, pts1[i,:-1], C2, pts2[i,:-1]) pts_3d[i,:] = pts_i reproj_error[i] = err else: print(\"point not visible in 2 views\") print(\"pts1 shape is\", pts1.shape) print(\"3d points shape is\", pts_3d.shape) return pts_3d, reproj_error, [vis_pts_1, vis_pts_2, vis_pts_3] def MutliviewReconstructionError(x, C1, pts1, C2, pts2, C3, pts3, vis_pts_list): # decompose x P_init = x P_shape_req = int(P_init.shape[0]/3) P_init = np.reshape(P_init, newshape=(P_shape_req,3)) vis_pts_1 = vis_pts_list[0] vis_pts_2 = vis_pts_list[1] vis_pts_3 = vis_pts_list[2] pts1 = pts1[:,0:2] pts2 = pts2[:,0:2] pts3 = pts3[:,0:2] # list to store error values err_list = [] # build a sub_P matrix for all visible points in pts1, pts2, pts3 sub_pts1 = np.take(pts1, vis_pts_1, axis=0)[0] sub_P1 = np.take(P_init, vis_pts_1, axis=0)[0] sub_pts2 = np.take(pts2, vis_pts_2, axis=0)[0] sub_P2 = np.take(P_init, vis_pts_2, axis=0)[0] sub_pts3 = np.take(pts3, vis_pts_3, axis=0)[0] sub_P3 = np.take(P_init, vis_pts_3, axis=0)[0] P_list = [sub_P1, sub_P2, sub_P3] pts_list = [sub_pts1, sub_pts2, sub_pts3] C_list = [C1, C2, C3] for i in range(len(P_list)): P = P_list[i] p= pts_list[i] C = C_list[i] # homogenize P to contain a 1 in the end (P = Nx3 vector) P_homogenous = np.append(P, np.ones((P.shape[0],1)), axis=1) # Find the projection of P1 onto image 1 (vectorize) # Transpose P_homogenous to make it a 4xN vector and left mulitply with C1 # 3xN = 3x4 @ 4XN p_hat = (C @ P_homogenous.T) # normalize and transpose to get back to format of p1 p_hat = ((p_hat/p_hat[2,:])[0:2,:]).T error = np.linalg.norm((p-p_hat).reshape([-1]))**2 err_list.append(error) err_total = err_list[0] + err_list[1] + err_list[2] # print(\"error overall is\", err_total) return err_total def triangulate(C1, pts1, C2, pts2): \"\"\" Find the 3D coords of the keypoints We are given camera matrices and 2D correspondences. We can therefore find the 3D points (refer L17 (Camera Models) of CV slides) Note. We can't just use x = PX to compute the 3D point X because of scale ambiguity i.e the ambiguity can be rep. as x = alpha*Px (we cannot find alpha) Therefore we need to do DLT just like the case of homography (see L14 (2D transforms) CVB slide 61) Args: C1 : the 3x4 camera matrix of camera 1 pts1 : img coords of keypoints in camera 1 (Nx2) C2 : the 3x4 camera matrix of camera 2 pts2 : img coords of keypoints in camera 2 (Nx2) Returns: P : the estimated 3D point for the given pair of keypoint correspondences err : the reprojection error \"\"\" P = np.zeros(shape=(1,3)) err = 0 # get the camera 1 matrix p1_1 = C1[0,:] p2_1 = C1[1,:] p3_1 = C1[2,:] # get the camera 2 matrix p1_2 = C2[0,:] p2_2 = C2[1,:] p3_2 = C2[2,:] x, y = pts1[0], pts1[1] x2, y2 = pts2[0], pts2[1] # calculate the A matrix for this point correspondence A = np.array([y*p3_1 - p2_1 , p1_1 - x*p3_1 , y2*p3_2 - p2_2 , p1_2 - x2*p3_2]) u, s, v = np.linalg.svd(A) # here the linalg.svd gives v_transpose # but we need just V therefore we again transpose X = v.T[:,-1] # print(\"X is\", X) X = X.T X = np.expand_dims(X,axis=0) # print(\"X after transpose and expand is\", X) # convert X to homogenous coords X = X/X[0,3] # print(\"X after normalizing is\", X) P = np.concatenate((P, X[:,0:3]), axis=0) X = X.T # find the error for this projection # 3x1 = 3x4 . 3x1 pt_1 = ((C1 @ X)/(C1 @ X)[2,0])[0:2,0] pt_2 = ((C2 @ X)/(C2 @ X)[2,0])[0:2,0] # calculate the reporjection error err += np.linalg.norm(pt_1 - pts1)**2 + np.linalg.norm(pt_2 - pts2)**2 # print(\"error in this iteration is\", err) P = P[1:,:] return P[0], err ''' Q6.2 Plot Spatio-temporal (3D) keypoints :param car_points: np.array points * 3 ''' def plot_3d_keypoint_video(pts_3d_video): fig = plt.figure() # num_points = pts_3d.shape[0] ax = fig.add_subplot(111, projection='3d') vid_len = len(pts_3d_video) vals = np.linspace(0.1,1, num=vid_len, endpoint=False) for i in range(len(pts_3d_video)): pts_3d = pts_3d_video[i] for j in range(len(connections_3d)): index0, index1 = connections_3d[j] xline = [pts_3d[index0,0], pts_3d[index1,0]] yline = [pts_3d[index0,1], pts_3d[index1,1]] zline = [pts_3d[index0,2], pts_3d[index1,2]] ax.plot(xline, yline, zline, color=colors[j], alpha=vals[i]) np.set_printoptions(threshold=1e6, suppress=True) ax.set_xlabel('X Label') ax.set_ylabel('Y Label') ax.set_zlabel('Z Label') plt.show() #Extra Credit if __name__ == \"__main__\": pts_3d_video = [] for loop in range(10): print(f\"processing time frame - {loop}\") data_path = os.path.join('data/q6/','time'+str(loop)+'.npz') image1_path = os.path.join('data/q6/','cam1_time'+str(loop)+'.jpg') image2_path = os.path.join('data/q6/','cam2_time'+str(loop)+'.jpg') image3_path = os.path.join('data/q6/','cam3_time'+str(loop)+'.jpg') im1 = plt.imread(image1_path) im2 = plt.imread(image2_path) im3 = plt.imread(image3_path) data = np.load(data_path) pts1 = data['pts1'] pts2 = data['pts2'] pts3 = data['pts3'] K1 = data['K1'] K2 = data['K2'] K3 = data['K3'] M1 = data['M1'] M2 = data['M2'] M3 = data['M3'] #Note - Press 'Escape' key to exit img preview and loop further # img = visualize_keypoints(im2, pts2) C1 = K1 @ M1 C2 = K2 @ M2 C3 = K3 @ M3 pts_3d, err, vis_pts_list = MultiviewReconstruction(C1, pts1, C2, pts2, C3, pts3) x_start = pts_3d.flatten() x_optimized_obj = minimize(MutliviewReconstructionError, x_start, args=(C1, pts1, C2, pts2, C3, pts3, vis_pts_list), method='Powell') print(\"x_end shape is\", x_optimized_obj.x.shape) x_optimized = x_optimized_obj.x P_final = x_optimized P_shape_req = int(P_final.shape[0]/3) P_final = np.reshape(P_final, newshape=(P_shape_req,3)) plot_3d_keypoint(P_final) pts_3d_video.append(P_final) visualize_keypoints(im1, pts1, Threshold=200) plot_3d_keypoint_video(pts_3d_video) out_dir = \"/home/sush/CMU/Assignment_Sem_1/CV_A/Assignment_4/code/outputs\" check_and_create_directory(out_dir, create=1) np.savez_compressed( os.path.join(out_dir, 'q6_1.npz'), P_final) . ",
    "url": "/3D_reconstruction/#tracking-real-world-objects-in-3d",
    
    "relUrl": "/3D_reconstruction/#tracking-real-world-objects-in-3d"
  },"280": {
    "doc": "3D Reconstruction",
    "title": "3D Reconstruction",
    "content": ". | Before you Begin | PDFs | Uses of Mutli-View Geometry | Background: Structure from Motion (SFM) | Theory: Solving for camera parameters in the presence of scale ambiguity . | Solving for Camera Params: Direct Linear Transform | Actual Derivation | . | Theory: Epipolar Geometry . | Essential Matrix: Maps a point -&gt; to a line . | Derivation of Essential Matrix: Longuet Higgins . | Longuet Higgins Derivation | . | Difference Between Essential Matrix and Homography Matrix | How does this Essential Matrix map a point to a line (where is the math?) | . | Fundamental Matrix | . | Structure From Motion Step 1: Estimating Fundamental Matrix: . | Estimate Essential Matrix from Fundamental Matrix (given K1 and K2) | . | Triangulation . | triangulate3D | Using Triangulate to Find Second Camera Matrix after fixing First Camera Matrix = Identity | Combining the above two functions | . | Bundle Adjustment . | High level procedure . | RodriguesResidual: rodriguesResidual(x, K1, M1, p1, K2, p2) | . | Optimization of M2 | . | Final Pipeline Including RANSAC . | Effects of RANSAC | . | Tracking Real World Objects in 3D | . ",
    "url": "/3D_reconstruction/",
    
    "relUrl": "/3D_reconstruction/"
  },"281": {
    "doc": "Markdown Cheat Sheet",
    "title": "Markdown Cheat Sheet",
    "content": "# Markdown Cheat Sheet Thanks for visiting [The Markdown Guide](https://www.markdownguide.org)! This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for [basic syntax](https://www.markdownguide.org/basic-syntax) and [extended syntax](https://www.markdownguide.org/extended-syntax). ## Basic Syntax These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. ### Heading # H1 ## H2 ### H3 ### Bold **bold text** ### Italic *italicized text* ### Blockquote > blockquote ### Ordered List 1. First item 2. Second item 3. Third item ### Unordered List - First item - Second item - Third item ### Code `code` ### Horizontal Rule --- ### Link [Markdown Guide](https://www.markdownguide.org) ### Image ![alt text](https://www.markdownguide.org/assets/images/tux.png) ## Extended Syntax These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. ### Table | Syntax | Description | ----------- | ----------- | Header | Title | Paragraph | Text | ### Fenced Code Block ``` { \"firstName\": \"John\", \"lastName\": \"Smith\", \"age\": 25 } ``` ### Footnote Here's a sentence with a footnote. [^1] [^1]: This is the footnote. ### Heading ID ### My Great Heading {#custom-id} ### Definition List term : definition ### Strikethrough ~~The world is flat.~~ ### Task List - [x] Write the press release - [ ] Update the website - [ ] Contact the media ### Go to next line This will split the sentence \\ into two lines In the above format, ensure there is no whitespace between the backslash and the next word (in this case 'into') # Button size Wrap the button in a container that uses the font-size utility classes to scale buttons: [Big ass button](http://example.com/){: .btn } [Tiny ass button](http://example.com/){: .btn } ```markdown [Link button](http://example.com/){: .btn } [Tiny ass button](http://example.com/){: .btn } ### Add Image with Caption (modelled as a table) | ![](data/rendering_pipeline.png) |:--:| *Accelerating 3D Deep Learning with PyTorch3D. Ravi et. al.* | ### Add MathJax support # MathJax v3 Configuration In `_includes/head_custom.html` add, for example: {% raw %} ```html {% case page.math %} {% when \"mathjax3\" %} {% endcase %} ``` {% endraw %} See also [further MathJax v3 configuration options](http://docs.mathjax.org/en/latest/web/configuration.html). In the front matter of pages using MathJax v3 (or as a global front-matter default) add: ```yaml layout: default title: Homework 4 nav_order: 1 description: Cats Generator Playground permalink: / math: mathjax3 ``` (The suggested field name `math` and the key `mathjax3` can be replaced.) After that, simply wrap the math symbols with two double-dollar sign ```$$``` ``` $$E=mc^2$$ ``` For further examples visit: https://github.com/pdmosses/just-the-docs-tests-old/blob/master/docs/math/mathjax3/tests.md ",
    "url": "/markdown-cheat-sheet.html",
    
    "relUrl": "/markdown-cheat-sheet.html"
  }
}
