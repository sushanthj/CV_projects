

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Rebuilding ConvNext | Navigating Robotics</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Rebuilding ConvNext" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/ConvNext/" />
<meta property="og:url" content="http://localhost:4000/ConvNext/" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Rebuilding ConvNext" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"Rebuilding ConvNext","url":"http://localhost:4000/ConvNext/"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  Navigating Robotics

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D Reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/optical_flow/" class="nav-list-link">Optical Flow and Image Alignment</a></li><li class="nav-list-item"><a href="http://localhost:4000/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item active"><a href="http://localhost:4000/ConvNext/" class="nav-list-link active">Rebuilding ConvNext</a></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" class="nav-list-link">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision_General" class="nav-list-link">Computer Vision</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/camera_model.html" class="nav-list-link">Camera Models and Projections</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Numpy.html" class="nav-list-link">Numpy for CV</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/NERF.html" class="nav-list-link">Volume Rendering and NERFs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/bag_of_words.html" class="nav-list-link">Spatial Pyramids and Bag of Words</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision Libraries in C++</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ol id="markdown-toc">
  <li><a href="#convnext" id="markdown-toc-convnext">ConvNext</a></li>
  <li><a href="#why-i-love-convnext" id="markdown-toc-why-i-love-convnext">Why I Love ConvNext</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ol>
      <li><a href="#drawbacks-of-vanilla-vision-transformers-vits" id="markdown-toc-drawbacks-of-vanilla-vision-transformers-vits">Drawbacks of Vanilla Vision Transformers (ViTs)</a></li>
      <li><a href="#enter-hierarchial-vits-like-swin-transformer" id="markdown-toc-enter-hierarchial-vits-like-swin-transformer">Enter Hierarchial ViTs like SWIN Transformer</a></li>
    </ol>
  </li>
  <li><a href="#approach" id="markdown-toc-approach">Approach</a>    <ol>
      <li><a href="#training-optimizations" id="markdown-toc-training-optimizations">Training Optimizations</a></li>
      <li><a href="#network-modernization" id="markdown-toc-network-modernization">Network Modernization</a>        <ol>
          <li><a href="#understanding-resnets" id="markdown-toc-understanding-resnets">Understanding ResNets</a></li>
          <li><a href="#macro-design" id="markdown-toc-macro-design">Macro Design</a></li>
          <li><a href="#making-it-more-lean-to-reduce-params" id="markdown-toc-making-it-more-lean-to-reduce-params">Making it more Lean (to reduce params)</a></li>
          <li><a href="#resnextify" id="markdown-toc-resnextify">ResNextify</a>            <ol>
              <li><a href="#why-depthwise-convolutions" id="markdown-toc-why-depthwise-convolutions">Why Depthwise Convolutions</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#appendix" id="markdown-toc-appendix">Appendix</a>    <ol>
      <li><a href="#time-complexities-analyses" id="markdown-toc-time-complexities-analyses">Time Complexities Analyses</a>        <ol>
          <li><a href="#simple-matrix-multiplication" id="markdown-toc-simple-matrix-multiplication">Simple Matrix Multiplication</a></li>
          <li><a href="#time-complexity-analysis-in-tranformers" id="markdown-toc-time-complexity-analysis-in-tranformers">Time Complexity Analysis in Tranformers</a>            <ol>
              <li><a href="#comparison-with-rnns" id="markdown-toc-comparison-with-rnns">Comparison with RNNs</a></li>
              <li><a href="#comparisons-with-separable-and-non-separable-convs" id="markdown-toc-comparisons-with-separable-and-non-separable-convs">Comparisons with Separable and Non-Separable Convs</a></li>
              <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion:</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

</details>
      <h1 id="convnext">
        
        
          <a href="#convnext" class="anchor-heading" aria-labelledby="convnext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ConvNext
        
        
      </h1>
    

<p>This paper is best described in its abstract as <em>‘We gradually “modernize” a standard ResNet
toward the design of a vision Transformer, and discover several key components that contribute
to the performance difference along the way’.</em></p>

<p>What we’ll try to learn through building ConvNext is the meaning behind these design choices
in terms of:</p>
<ul>
  <li>Activation Functions</li>
  <li>Architechture</li>
  <li>Inductive Biases</li>
  <li>And more so ..</li>
</ul>
      <h1 id="why-i-love-convnext">
        
        
          <a href="#why-i-love-convnext" class="anchor-heading" aria-labelledby="why-i-love-convnext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why I Love ConvNext
        
        
      </h1>
    

<p>This is one of the few architechtures which I have re-written from scratch including the
Dataloaders. I used ConvNext for Face Classification and beat 250+ students and TAs in my
class on a <a href="https://www.kaggle.com/competitions/11-785-f23-hw2p2-classification/leaderboard">Kaggle Competition</a>.</p>

<p>I not only re-wrote it in a simple manner, <strong>I also had to make many design decisions in
reducing the channel widths and reducing network depth to brind down the trainable params
from 29 Million to just 11 million</strong></p>

<p><img src="/images/ConvNext/kaggle.png" alt="" /></p>
      <h1 id="introduction">
        
        
          <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction
        
        
      </h1>
    
      <h2 id="drawbacks-of-vanilla-vision-transformers-vits">
        
        
          <a href="#drawbacks-of-vanilla-vision-transformers-vits" class="anchor-heading" aria-labelledby="drawbacks-of-vanilla-vision-transformers-vits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Drawbacks of Vanilla Vision Transformers (ViTs)
        
        
      </h2>
    

<ul>
  <li>ViTs became famous due to their ability to scale</li>
  <li>With huge datasets they outperformed ResNets on Image Classification</li>
  <li>However, ironically, the cost of global attention (to all tokens i.e. all image patches
fed to the transformer) grows quadratically with image size</li>
  <li>For real world images, this issue is a big problem!</li>
</ul>
      <h2 id="enter-hierarchial-vits-like-swin-transformer">
        
        
          <a href="#enter-hierarchial-vits-like-swin-transformer" class="anchor-heading" aria-labelledby="enter-hierarchial-vits-like-swin-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enter Hierarchial ViTs like SWIN Transformer
        
        
      </h2>
    

<p><img src="/images/ConvNext/swin.png" alt="" /></p>

<ul>
  <li>Instead of just global attention, introduce attention locally to a window (red boundary)</li>
  <li>A fixed number of image patches form a window</li>
  <li>
    <p>This reduces the time complexity from being quadratic in image size for generic ViTs
to now being linear w.r.t image size</p>
  </li>
  <li><strong>This linear time complexity w.r.t image size made ViTs tractable for all vision tasks like
detection, segmentation and classification</strong></li>
</ul>
      <h1 id="approach">
        
        
          <a href="#approach" class="anchor-heading" aria-labelledby="approach"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Approach
        
        
      </h1>
    

<p>The use of shifted-windows as in Swin Transformers and the learnings from the era of ViTs
motivate the authors of ConvNext to begin ‘modernizing’ CNNs.</p>

<p>They begin by taking a simple ResNet-50 model and reshaping it from the learning of ViTs. They
do this in two steps:</p>

<ul>
  <li>New Training Methods</li>
  <li>New Network Architectures which include:
    <ul>
      <li>Macro Design changes</li>
      <li>ResNextify</li>
      <li>Inverted Bottleneck</li>
      <li>Larger Kernel Sizes</li>
      <li>Layer-wise micro designs</li>
    </ul>
  </li>
</ul>
      <h2 id="training-optimizations">
        
        
          <a href="#training-optimizations" class="anchor-heading" aria-labelledby="training-optimizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Training Optimizations
        
        
      </h2>
    

<p>This mainly included new optimizers, larger training epochs, and new augmentation methods. 
Specifically:</p>

<ul>
  <li>AdamW over Adam</li>
  <li>Augmentations such as: Mixup, Cutmix, RandAugment, Random Erasing</li>
  <li>Regularization schemes including Stochastic Depth and Label Smoothing</li>
</ul>

<p>Stochastic depth is when we choose to keep a residual block active or inactive based on
some probability (maybe bernoulli or a uniform probability distribution) as shown below:</p>

<p><img src="/images/ConvNext/stochastic_depth.png" alt="" /></p>
      <h2 id="network-modernization">
        
        
          <a href="#network-modernization" class="anchor-heading" aria-labelledby="network-modernization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Network Modernization
        
        
      </h2>
    
      <h3 id="understanding-resnets">
        
        
          <a href="#understanding-resnets" class="anchor-heading" aria-labelledby="understanding-resnets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding ResNets
        
        
      </h3>
    

<p>It’s beneficial to first see how a resnet works. Firstly note that we have two variants
of the ResNet block</p>
<ul>
  <li>Simple Block (used in ResNet34)</li>
  <li><strong>BottleNeck Block</strong> (used in all other ResNets)</li>
</ul>

<p><img src="/images/ConvNext/resnet_types.png" alt="" /></p>

<p>The overall architechture of Resnet is captured in the below diagrams:</p>

<p><img src="/images/ConvNext/resnet_arch.png" alt="" /></p>

<p>Where the final 1000x1 vector is for the 1000 ImageNet classes. Also note the number of
repeating ResNet block in each layer (50-layer or ResNet50 being referred below):</p>
<ul>
  <li>Conv2_x has 3 blocks</li>
  <li>Conv3_x has 4 blocks</li>
</ul>

<p>Overall we have (3,4,6,3) as <strong>‘stage compute ratio’</strong> as defined by authors.</p>
      <h3 id="macro-design">
        
        
          <a href="#macro-design" class="anchor-heading" aria-labelledby="macro-design"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Macro Design
        
        
      </h3>
    

<p>We saw (3,4,6,3) as <strong>‘stage compute ratio’</strong> in ResNet50 as explained previously. In Swin-Transformer the same block distribution was (1,1,9,1).</p>

<p>Hence, <strong>ConvNext tries to follow the same and uses (3,3,9,3) as the block distributions</strong>.</p>
      <h3 id="making-it-more-lean-to-reduce-params">
        
        
          <a href="#making-it-more-lean-to-reduce-params" class="anchor-heading" aria-labelledby="making-it-more-lean-to-reduce-params"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Making it more Lean (to reduce params)
        
        
      </h3>
    

<p>However, I had to cut down on this to reduce parameter limit and changed the ratios to 
<strong>(6,5,4,4)</strong>. This was chosen after a few ablations but also higher numbers for the initial
blocks were chosen to allow for an optimization on the number of channels at input/output of
each ConvNext stage. Specifically:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># number of channels at input/output of each res_blocks
# Updated Config
</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">400</span><span class="p">]</span>

<span class="c1"># Original Config
# self.channel_list = [96, 192, 384, 768]
</span>
<span class="c1"># number of repeats for each res_block
# Updated Config
</span><span class="bp">self</span><span class="p">.</span><span class="n">block_repeat_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>

<span class="c1"># Original Config
# self.block_repeat_list = [3,3,9,3]
</span></code></pre></div></div>

<p>As you can see, to maintain the relative number of channels at each stage (at least keep it
monotonically increasing as in the original config), I had to increase the initial block_repeats
where the channel size is small and decrease the block_repeats when channel size was larger</p>
      <h3 id="resnextify">
        
        
          <a href="#resnextify" class="anchor-heading" aria-labelledby="resnextify"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ResNextify
        
        
      </h3>
    

<p>ResNext utilized group convolution in the 3x3 conv layer of bottleneck blocks. What is group
convolution?</p>

<p><img src="/images/ConvNext/group_conv.png" alt="" /></p>

<p>The authors of ConvNext decided to use a special case of group convolution where the number of
groups equals number of channels. <strong>That is literally just Depthwise Seperable Convs!!!</strong></p>
      <h4 id="why-depthwise-convolutions">
        
        
          <a href="#why-depthwise-convolutions" class="anchor-heading" aria-labelledby="why-depthwise-convolutions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Depthwise Convolutions
        
        
      </h4>
    

<p>The simple answer is the computational complexitites:</p>
<ul>
  <li>Depth-wise Separable = <code class="language-plaintext highlighter-rouge">O(n**2*d + n*d**2)</code> -&gt; as per Attention is all you need</li>
  <li>Generic Convolution = <code class="language-plaintext highlighter-rouge">O(n**2 * d**2)</code> -&gt; Think of n = filter size spatial, d = filter size depth (num channels)</li>
  <li><a href="https://arxiv.org/pdf/1704.04861.pdf">Reference : <em>MobileNet</em></a></li>
  <li><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Reference : <em>Attention is All You Need</em></a></li>
</ul>

<p>However, while those numbers may seem weird, for a more practical example you can
view <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728#:~:text=two%20smaller%20kernels.-,Depthwise%20Separable%20Convolutions,it%20is%20more%20commonly%20used.">this post</a>.</p>

<p><strong>Bottomline, MobileNet shows that Depthwise Seperable Conv has much lesser FLOPs than conventional
convolution layers.</strong></p>
      <h1 id="appendix">
        
        
          <a href="#appendix" class="anchor-heading" aria-labelledby="appendix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Appendix
        
        
      </h1>
    
      <h2 id="time-complexities-analyses">
        
        
          <a href="#time-complexities-analyses" class="anchor-heading" aria-labelledby="time-complexities-analyses"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexities Analyses
        
        
      </h2>
    
      <h3 id="simple-matrix-multiplication">
        
        
          <a href="#simple-matrix-multiplication" class="anchor-heading" aria-labelledby="simple-matrix-multiplication"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Simple Matrix Multiplication
        
        
      </h3>
    

<p>In general if we are multiplying two matrices A (of size {N,D}) and B (of size {D,D}) then
<code class="language-plaintext highlighter-rouge">A@B</code> will involve three nested loops, specifically:</p>

<ul>
  <li>For each of the <strong>N rows</strong> in A
    <ul>
      <li>We perform <strong>D dot products</strong>
        <ul>
          <li>Which each involves <strong>D multiplictions</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Hence, overall time complexity = <code class="language-plaintext highlighter-rouge">N * D * D</code> = <code class="language-plaintext highlighter-rouge">N * D**2</code></p>
      <h3 id="time-complexity-analysis-in-tranformers">
        
        
          <a href="#time-complexity-analysis-in-tranformers" class="anchor-heading" aria-labelledby="time-complexity-analysis-in-tranformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexity Analysis in Tranformers
        
        
      </h3>
    

<p>The transformers are seq2seq models with desired output (during training) is just the
right shifted inputs. For example if input is ‘I am superman’ and we are building a word2word
prediciton language model given input <code class="language-plaintext highlighter-rouge">I</code> the desired output is <code class="language-plaintext highlighter-rouge">am</code> and that makes:</p>

<ul>
  <li>OurInput = <code class="language-plaintext highlighter-rouge">&lt;SOS&gt; I am Superman</code></li>
  <li>Desired output = <code class="language-plaintext highlighter-rouge">I am Superman &lt;EOS&gt;</code></li>
</ul>

<p>Consider we have <code class="language-plaintext highlighter-rouge">N</code> words which we project in embedding layer where each word
gets projected to a vector of shape <code class="language-plaintext highlighter-rouge">D</code>, then a sentence of N words will get
projected to a shape of <code class="language-plaintext highlighter-rouge">N x D</code> (just a matrix where num_rows = num_words and num_cols = projection_size)</p>

<p>Then self attention in scaled-dot-product form:</p>

<p><img src="/images/ConvNext/scaled_dot_prod_attention.png" alt="" /></p>

<p>Will have the following time comlexity</p>

<ol>
  <li>Linearly transforming the rows of <code class="language-plaintext highlighter-rouge">X</code> to compute the query <code class="language-plaintext highlighter-rouge">Q</code>, key <code class="language-plaintext highlighter-rouge">K</code>, and value <code class="language-plaintext highlighter-rouge">V</code> matrices, each of which has shape <code class="language-plaintext highlighter-rouge">(N, D)</code>. This is accomplished by post-multiplying <code class="language-plaintext highlighter-rouge">X</code> with 3 learned matrices of shape <code class="language-plaintext highlighter-rouge">(D, D)</code>, amounting to a computational complexity of <code class="language-plaintext highlighter-rouge">O(N D^2)</code>.</li>
  <li>Computing the layer output, specified in above equation of the paper as <code class="language-plaintext highlighter-rouge">SoftMax(Q @ Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code class="language-plaintext highlighter-rouge">Q @ Kt</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code>, and post-multiplying the resultant with <code class="language-plaintext highlighter-rouge">V</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> as well.</li>
</ol>

<p>Overall the time complexity would be <code class="language-plaintext highlighter-rouge">O(N^2.D + N.D^2)</code></p>

<p><strong>NOTE: In the paper, they say it takes only <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> for Self Attention, but this excludes
the calculation of Q,K,V</strong></p>
      <h4 id="comparison-with-rnns">
        
        
          <a href="#comparison-with-rnns" class="anchor-heading" aria-labelledby="comparison-with-rnns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparison with RNNs
        
        
      </h4>
    

<p>RNNs have a hidden state neuron which is connected across the time series as shown below:</p>

<p><img src="/images/ConvNext/RNN_simple.png" alt="" /></p>

<p>The hidden neuron computation is simply: <code class="language-plaintext highlighter-rouge">h(t)​ = f(U x(t)​ + W h(t−1)​)</code></p>

<p>Hence, they are modelled as O(n * d<em>*2) *(as it’s an MLP with matrix multiplication, see Appendix)</em> with O(n) sequential operations</p>
      <h4 id="comparisons-with-separable-and-non-separable-convs">
        
        
          <a href="#comparisons-with-separable-and-non-separable-convs" class="anchor-heading" aria-labelledby="comparisons-with-separable-and-non-separable-convs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparisons with Separable and Non-Separable Convs
        
        
      </h4>
    

<ul>
  <li>Depth-wise Separable = <code class="language-plaintext highlighter-rouge">O(n**2*d + n*d**2)</code> = Self Attention + Feed Forward MLP</li>
  <li>Generic Convolution = <code class="language-plaintext highlighter-rouge">O(n**2 * d**2)</code></li>
</ul>
      <h4 id="conclusion">
        
        
          <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion:
        
        
      </h4>
    

<p>The authors of <em>Attention is All You Need</em> therefore claim that Self Attention (<code class="language-plaintext highlighter-rouge">O(N**2*D)</code> or truly <code class="language-plaintext highlighter-rouge">O(N**2*D + N*D**2)</code>) is parallelizable
and faster than the next best option -&gt; i.e. Depthwise Separable Convolution (<code class="language-plaintext highlighter-rouge">O(N**2*D + N*D**2)</code>)</p>

<p>Considering the true calculation of Scaled Dot Product Attention, it seems to be the same
as Depthwise Separable Convolution.</p>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

