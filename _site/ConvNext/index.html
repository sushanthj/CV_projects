

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Rebuilding ConvNext | Navigating Robotics</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Rebuilding ConvNext" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/ConvNext/" />
<meta property="og:url" content="http://localhost:4000/ConvNext/" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Rebuilding ConvNext" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"Rebuilding ConvNext","url":"http://localhost:4000/ConvNext/"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  Navigating Robotics

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D Reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/optical_flow/" class="nav-list-link">Optical Flow and Image Alignment</a></li><li class="nav-list-item"><a href="http://localhost:4000/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item active"><a href="http://localhost:4000/ConvNext/" class="nav-list-link active">Rebuilding ConvNext</a></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" class="nav-list-link">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision_General" class="nav-list-link">Computer Vision</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/camera_model.html" class="nav-list-link">Camera Models and Projections</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Numpy.html" class="nav-list-link">Numpy for CV</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/NERF.html" class="nav-list-link">Volume Rendering and NERFs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/bag_of_words.html" class="nav-list-link">Spatial Pyramids and Bag of Words</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision Libraries in C++</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ol id="markdown-toc">
  <li><a href="#convnext" id="markdown-toc-convnext">ConvNext</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ol>
      <li><a href="#drawbacks-of-vanilla-vision-transformers-vits" id="markdown-toc-drawbacks-of-vanilla-vision-transformers-vits">Drawbacks of Vanilla Vision Transformers (ViTs)</a>        <ol>
          <li><a href="#time-complexities-of-matrix-multipications-move-to-appendix" id="markdown-toc-time-complexities-of-matrix-multipications-move-to-appendix">Time Complexities of Matrix Multipications (Move to appendix)</a></li>
          <li><a href="#time-complexity-analysis-in-tranformers" id="markdown-toc-time-complexity-analysis-in-tranformers">Time Complexity Analysis in Tranformers</a>            <ol>
              <li><a href="#comparison-with-rnns" id="markdown-toc-comparison-with-rnns">Comparison with RNNs</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#enter-hierarchial-vits-like-swin-transformer" id="markdown-toc-enter-hierarchial-vits-like-swin-transformer">Enter Hierarchial ViTs like SWIN Transformer</a></li>
    </ol>
  </li>
</ol>

</details>
      <h1 id="convnext">
        
        
          <a href="#convnext" class="anchor-heading" aria-labelledby="convnext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ConvNext
        
        
      </h1>
    

<p>This paper is best described in its abstract as <em>‘We gradually “modernize” a standard ResNet
toward the design of a vision Transformer, and discover several key components that contribute
to the performance difference along the way’.</em></p>

<p>What we’ll try to learn through building ConvNext is the meaning behind these design choices
in terms of:</p>
<ul>
  <li>Activation Functions</li>
  <li>Architechture</li>
  <li>Inductive Biases</li>
  <li>And more so ..</li>
</ul>
      <h1 id="introduction">
        
        
          <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction
        
        
      </h1>
    
      <h2 id="drawbacks-of-vanilla-vision-transformers-vits">
        
        
          <a href="#drawbacks-of-vanilla-vision-transformers-vits" class="anchor-heading" aria-labelledby="drawbacks-of-vanilla-vision-transformers-vits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Drawbacks of Vanilla Vision Transformers (ViTs)
        
        
      </h2>
    

<ul>
  <li>ViTs became famous due to their ability to scale</li>
  <li>With huge datasets they outperformed ResNets on Image Classification</li>
  <li>However, ironically, the cost of global attention (to all tokens i.e. all image patches
fed to the transformer) grows quadratically with image size</li>
  <li>For real world images, this issue is a big problem!</li>
</ul>
      <h3 id="time-complexities-of-matrix-multipications-move-to-appendix">
        
        
          <a href="#time-complexities-of-matrix-multipications-move-to-appendix" class="anchor-heading" aria-labelledby="time-complexities-of-matrix-multipications-move-to-appendix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexities of Matrix Multipications (Move to appendix)
        
        
      </h3>
    

<p>In general if we are multiplying two matrices A (of size {N,D}) and B (of size {D,D}) then
<code class="language-plaintext highlighter-rouge">A@B</code> will involve three nested loops, specifically:</p>

<ul>
  <li>For each of the <strong>N rows</strong> in A
    <ul>
      <li>We perform <strong>D dot products</strong>
        <ul>
          <li>Which each involves <strong>D multiplictions</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Hence, overall time complexity = <code class="language-plaintext highlighter-rouge">N * D * D</code> = <code class="language-plaintext highlighter-rouge">N D^2</code></p>
      <h3 id="time-complexity-analysis-in-tranformers">
        
        
          <a href="#time-complexity-analysis-in-tranformers" class="anchor-heading" aria-labelledby="time-complexity-analysis-in-tranformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexity Analysis in Tranformers
        
        
      </h3>
    

<p>The transformers are seq2seq models with desired output (during training) is just the
right shifted inputs. For example if input is ‘I am superman’ and we are building a word2word
prediciton language model given input <code class="language-plaintext highlighter-rouge">I</code> the desired output is <code class="language-plaintext highlighter-rouge">am</code> and that makes:</p>

<ul>
  <li>OurInput = <code class="language-plaintext highlighter-rouge">&lt;SOS&gt; I am Superman</code></li>
  <li>Desired output = <code class="language-plaintext highlighter-rouge">I am Superman &lt;EOS&gt;</code></li>
</ul>

<p>Consider we have <code class="language-plaintext highlighter-rouge">N</code> words which we project in embedding layer where each word
gets projected to a vector of shape <code class="language-plaintext highlighter-rouge">D</code>, then a sentence of N words will get
projected to a shape of <code class="language-plaintext highlighter-rouge">N x D</code> (just a matrix where num_rows = num_words and num_cols = projection_size)</p>

<p>Then self attention in scaled-dot-product form:</p>

<p><img src="/images/ConvNext/scaled_dot_prod_attention.png" alt="" /></p>

<p>Will have the following time comlexity</p>

<ol>
  <li>Linearly transforming the rows of <code class="language-plaintext highlighter-rouge">X</code> to compute the query <code class="language-plaintext highlighter-rouge">Q</code>, key <code class="language-plaintext highlighter-rouge">K</code>, and value <code class="language-plaintext highlighter-rouge">V</code> matrices, each of which has shape <code class="language-plaintext highlighter-rouge">(N, D)</code>. This is accomplished by post-multiplying <code class="language-plaintext highlighter-rouge">X</code> with 3 learned matrices of shape <code class="language-plaintext highlighter-rouge">(D, D)</code>, amounting to a computational complexity of <code class="language-plaintext highlighter-rouge">O(N D^2)</code>.</li>
  <li>Computing the layer output, specified in above equation of the paper as <code class="language-plaintext highlighter-rouge">SoftMax(Q @ Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code class="language-plaintext highlighter-rouge">Q @ Kt</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code>, and post-multiplying the resultant with <code class="language-plaintext highlighter-rouge">V</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> as well.</li>
</ol>

<p>Overall the time complexity would be <code class="language-plaintext highlighter-rouge">O(N^2.D + N.D^2)</code></p>

<p><strong>NOTE: In the paper, they say it takes only <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> for Self Attention, but this excludes
the calculation of Q,K,V</strong></p>
      <h4 id="comparison-with-rnns">
        
        
          <a href="#comparison-with-rnns" class="anchor-heading" aria-labelledby="comparison-with-rnns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparison with RNNs
        
        
      </h4>
    

<p>RNNs have a hidden state neuron which is connected across the time series as shown below:</p>
      <h2 id="enter-hierarchial-vits-like-swin-transformer">
        
        
          <a href="#enter-hierarchial-vits-like-swin-transformer" class="anchor-heading" aria-labelledby="enter-hierarchial-vits-like-swin-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enter Hierarchial ViTs like SWIN Transformer
        
        
      </h2>
    

<ul>
  <li>Instead of just global attention, introduce attention locally in image patches</li>
  <li>This reduces the time</li>
</ul>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

