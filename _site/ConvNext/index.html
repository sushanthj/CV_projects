

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet">

  <style id="jtd-nav-activation">
  

    
    .site-nav > ul.nav-list:first-child > li:not(:nth-child(6)) > a,
    .site-nav > ul.nav-list:first-child > li > ul > li a {
      background-image: none;
    }

    .site-nav > ul.nav-list:not(:first-child) a,
    .site-nav li.external a {
      background-image: none;
    }

    .site-nav > ul.nav-list:first-child > li:nth-child(6) > a {
      font-weight: 600;
      text-decoration: none;
    }.site-nav > ul.nav-list:first-child > li:nth-child(6) > button svg {
      transform: rotate(-90deg);
    }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(6) > ul.nav-list {
      display: block;
    }
  </style>

  
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0VNMB6VPJF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      
        gtag('config', 'G-0VNMB6VPJF', { 'anonymize_ip': true });
      
    </script>
  

  
    <script src="/assets/js/vendor/lunr.min.js"></script>
  

  <script src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  



  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ConvNext | Navigating Robotics</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="ConvNext" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/ConvNext/" />
<meta property="og:url" content="http://localhost:4000/ConvNext/" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ConvNext" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"ConvNext","url":"http://localhost:4000/ConvNext/"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
  <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>

  <symbol id="svg-menu" viewBox="0 0 24 24">
  <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>

  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
  <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
    <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>

  <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE -->
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
  <title id="svg-external-link-title">(external link)</title>
  <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>

  
    <symbol id="svg-doc" viewBox="0 0 24 24">
  <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
    <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>

    <symbol id="svg-search" viewBox="0 0 24 24">
  <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>

  
  
    <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md -->
<symbol id="svg-copy" viewBox="0 0 16 16">
  <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
    <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
  <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
    <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>

  
</svg>

  
    <div class="side-bar">
  <div class="site-header" role="banner">
    <a href="/" class="site-title lh-tight">
  Navigating Robotics

</a>
    <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false">
      <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg>
    </button>
  </div>

  <nav aria-label="Main" id="site-nav" class="site-nav">
  
  
    <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="/3D_reconstruction/" class="nav-list-link">3D Reconstruction</a></li><li class="nav-list-item"><a href="/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item"><a href="/ConvNext/" class="nav-list-link">ConvNext</a></li><li class="nav-list-item"><a href="/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Deep Learning category" aria-pressed="false">
        <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
      </button><a href="/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item"><a href="/docs/Deep%20Learning/IDL5.html" class="nav-list-link">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in SLAM category" aria-pressed="false">
        <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
      </button><a href="/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item"><a href="/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item"><a href="/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item"><a href="/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item"><a href="/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Computer Vision category" aria-pressed="false">
        <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
      </button><a href="/docs/Vision_General" class="nav-list-link">Computer Vision</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Computer%20Vision/camera_model.html" class="nav-list-link">Camera Models and Projections</a></li><li class="nav-list-item"><a href="/docs/Computer%20Vision/Numpy.html" class="nav-list-link">Numpy for CV</a></li><li class="nav-list-item"><a href="/docs/Computer%20Vision/NERF.html" class="nav-list-link">Volume Rendering and NERFs</a></li><li class="nav-list-item"><a href="/docs/Computer%20Vision/bag_of_words.html" class="nav-list-link">Spatial Pyramids and Bag of Words</a></li><li class="nav-list-item"><a href="/docs/Computer%20Vision/Optical%20Flow.html" class="nav-list-link">Optical Flow and Image Alignment</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Computer Vision Libraries in C++ category" aria-pressed="false">
        <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
      </button><a href="/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision Libraries in C++</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item"><a href="/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li><li class="nav-list-item"><a href="/markdown-cheat-sheet.html" class="nav-list-link">Markdown Cheat Sheet</a></li></ul>
  
</nav>




  
  
    <footer class="site-footer">
      This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  
</div>

  
  <div class="main" id="top">
    <div id="main-header" class="main-header">
  
    

<div class="search" role="search">
  <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
  </div>
  <div id="search-results" class="search-results"></div>
</div>

  
  
  
    <nav aria-label="Auxiliary" class="aux-nav">
  <ul class="aux-nav-list">
    
      <li class="aux-nav-list-item">
        <a href="//github.com/sushanthj" class="site-button"
          
        >
          Sushanth Jayanth's github
        </a>
      </li>
    
  </ul>
</nav>

  
</div>

    <div class="main-content-wrap">
      
      <div id="main-content" class="main-content">
        <main>
          
            <h1 id="convnext">
  
  
    <a href="#convnext" class="anchor-heading" aria-labelledby="convnext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://arxiv.org/pdf/2201.03545.pdf">ConvNext</a>
  
  
</h1>
    

<p>This paper is best described in its abstract as <em>‘We gradually “modernize” a standard ResNet
toward the design of a vision Transformer, and discover several key components that contribute
to the performance difference along the way’.</em></p>

<p>What we’ll try to learn through building ConvNext is the meaning behind these design choices
in terms of:</p>
<ul>
  <li>Activation Functions</li>
  <li>Architechture</li>
  <li>Inductive Biases</li>
  <li>And more so ..</li>
</ul>
<h1 id="why-i-love-convnext">
  
  
    <a href="#why-i-love-convnext" class="anchor-heading" aria-labelledby="why-i-love-convnext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why I Love ConvNext
  
  
</h1>
    

<p>This is one of the few architechtures which I have re-written from scratch including the
Dataloaders. I used ConvNext for Face Classification and beat 250+ students and TAs in my
class on a <a href="https://www.kaggle.com/competitions/11-785-f23-hw2p2-classification/leaderboard">Kaggle Competition</a>.</p>

<p><a href="https://github.com/sushanthj/ConvNext-Lean/blob/main/Classification.ipynb" class="btn fs-2 mb-2 mb-md-0">Github</a></p>

<p>I not only re-wrote it in a simple manner, <strong>I also had to make many design decisions in
reducing the channel widths and reducing network depth to brind down the trainable params
from 29 Million to just 11 million</strong></p>

<p><img src="/images/ConvNext/kaggle.png" alt="" /></p>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ol id="markdown-toc">
  <li><a href="#convnext" id="markdown-toc-convnext">ConvNext</a></li>
  <li><a href="#why-i-love-convnext" id="markdown-toc-why-i-love-convnext">Why I Love ConvNext</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ol>
      <li><a href="#drawbacks-of-vanilla-vision-transformers-vits" id="markdown-toc-drawbacks-of-vanilla-vision-transformers-vits">Drawbacks of Vanilla Vision Transformers (ViTs)</a></li>
      <li><a href="#enter-hierarchial-vits-like-swin-transformer" id="markdown-toc-enter-hierarchial-vits-like-swin-transformer">Enter Hierarchial ViTs like SWIN Transformer</a></li>
    </ol>
  </li>
  <li><a href="#approach" id="markdown-toc-approach">Approach</a>    <ol>
      <li><a href="#training-optimizations" id="markdown-toc-training-optimizations">Training Optimizations</a></li>
      <li><a href="#network-modernization" id="markdown-toc-network-modernization">Network Modernization</a>        <ol>
          <li><a href="#understanding-resnets" id="markdown-toc-understanding-resnets">Understanding ResNets</a></li>
          <li><a href="#macro-design" id="markdown-toc-macro-design">Macro Design</a></li>
          <li><a href="#making-it-more-lean-to-reduce-params" id="markdown-toc-making-it-more-lean-to-reduce-params">Making it more Lean (to reduce params)</a></li>
          <li><a href="#resnextify" id="markdown-toc-resnextify">ResNextify</a>            <ol>
              <li><a href="#why-depthwise-convolutions" id="markdown-toc-why-depthwise-convolutions">Why Depthwise Convolutions</a></li>
            </ol>
          </li>
          <li><a href="#inverted-bottleneck-and-large-kernels" id="markdown-toc-inverted-bottleneck-and-large-kernels">Inverted BottleNeck and Large Kernels</a></li>
          <li><a href="#activations" id="markdown-toc-activations">Activations</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#implementation" id="markdown-toc-implementation">Implementation</a>    <ol>
      <li><a href="#convnext-block" id="markdown-toc-convnext-block">ConvNext Block</a></li>
      <li><a href="#network-setup" id="markdown-toc-network-setup">Network Setup</a></li>
      <li><a href="#stochastic-depth-and-training" id="markdown-toc-stochastic-depth-and-training">Stochastic Depth and Training</a></li>
    </ol>
  </li>
  <li><a href="#appendix" id="markdown-toc-appendix">Appendix</a>    <ol>
      <li><a href="#time-complexities-analyses" id="markdown-toc-time-complexities-analyses">Time Complexities Analyses</a>        <ol>
          <li><a href="#simple-matrix-multiplication" id="markdown-toc-simple-matrix-multiplication">Simple Matrix Multiplication</a></li>
          <li><a href="#time-complexity-analysis-in-tranformers" id="markdown-toc-time-complexity-analysis-in-tranformers">Time Complexity Analysis in Tranformers</a>            <ol>
              <li><a href="#comparison-with-rnns" id="markdown-toc-comparison-with-rnns">Comparison with RNNs</a></li>
              <li><a href="#comparisons-with-separable-and-non-separable-convs" id="markdown-toc-comparisons-with-separable-and-non-separable-convs">Comparisons with Separable and Non-Separable Convs</a></li>
              <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion:</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

</details>
<h1 id="introduction">
  
  
    <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction
  
  
</h1>
    
<h2 id="drawbacks-of-vanilla-vision-transformers-vits">
  
  
    <a href="#drawbacks-of-vanilla-vision-transformers-vits" class="anchor-heading" aria-labelledby="drawbacks-of-vanilla-vision-transformers-vits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Drawbacks of Vanilla Vision Transformers (ViTs)
  
  
</h2>
    

<ul>
  <li>ViTs became famous due to their ability to scale</li>
  <li>With huge datasets they outperformed ResNets on Image Classification</li>
  <li>However, ironically, the cost of global attention (to all tokens i.e. all image patches
fed to the transformer) grows quadratically with image size</li>
  <li>For real world images, this issue is a big problem!</li>
</ul>
<h2 id="enter-hierarchial-vits-like-swin-transformer">
  
  
    <a href="#enter-hierarchial-vits-like-swin-transformer" class="anchor-heading" aria-labelledby="enter-hierarchial-vits-like-swin-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enter Hierarchial ViTs like SWIN Transformer
  
  
</h2>
    

<p><img src="/images/ConvNext/swin.png" alt="" /></p>

<ul>
  <li>Instead of just global attention, introduce attention locally to a window (red boundary)</li>
  <li>A fixed number of image patches form a window</li>
  <li>
    <p>This reduces the time complexity from being quadratic in image size for generic ViTs
to now being linear w.r.t image size</p>
  </li>
  <li><strong>This linear time complexity w.r.t image size made ViTs tractable for all vision tasks like
detection, segmentation and classification</strong></li>
</ul>
<h1 id="approach">
  
  
    <a href="#approach" class="anchor-heading" aria-labelledby="approach"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Approach
  
  
</h1>
    

<p>The use of shifted-windows as in Swin Transformers and the learnings from the era of ViTs
motivate the authors of ConvNext to begin ‘modernizing’ CNNs.</p>

<p>They begin by taking a simple ResNet-50 model and reshaping it from the learning of ViTs. They
do this in two steps:</p>

<ul>
  <li>New Training Methods</li>
  <li>New Network Architectures which include:
    <ul>
      <li>Macro Design changes</li>
      <li>ResNextify</li>
      <li>Inverted Bottleneck</li>
      <li>Larger Kernel Sizes</li>
      <li>Layer-wise micro designs</li>
    </ul>
  </li>
</ul>
<h2 id="training-optimizations">
  
  
    <a href="#training-optimizations" class="anchor-heading" aria-labelledby="training-optimizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Training Optimizations
  
  
</h2>
    

<p>This mainly included new optimizers, larger training epochs, and new augmentation methods. 
Specifically:</p>

<ul>
  <li>AdamW over Adam</li>
  <li>Augmentations such as: Mixup, Cutmix, RandAugment, Random Erasing</li>
  <li>Regularization schemes including Stochastic Depth and Label Smoothing</li>
</ul>

<p>Stochastic depth is when we choose to keep a residual block active or inactive based on
some probability (maybe bernoulli or a uniform probability distribution) as shown below:</p>

<p><img src="/images/ConvNext/stochastic_depth.png" alt="" /></p>
<h2 id="network-modernization">
  
  
    <a href="#network-modernization" class="anchor-heading" aria-labelledby="network-modernization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Network Modernization
  
  
</h2>
    
<h3 id="understanding-resnets">
  
  
    <a href="#understanding-resnets" class="anchor-heading" aria-labelledby="understanding-resnets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding ResNets
  
  
</h3>
    

<p>It’s beneficial to first see how a resnet works. Firstly note that we have two variants
of the ResNet block</p>
<ul>
  <li>Simple Block (used in ResNet34)</li>
  <li><strong>BottleNeck Block</strong> (used in all other ResNets)</li>
</ul>

<p><img src="/images/ConvNext/resnet_types.png" alt="" /></p>

<p>The overall architechture of Resnet is captured in the below diagrams:</p>

<p><img src="/images/ConvNext/resnet_arch.png" alt="" /></p>

<p>Where the final 1000x1 vector is for the 1000 ImageNet classes. Also note the number of
repeating ResNet block in each layer (50-layer or ResNet50 being referred below):</p>
<ul>
  <li>Conv2_x has 3 blocks</li>
  <li>Conv3_x has 4 blocks</li>
</ul>

<p>Overall we have (3,4,6,3) as <strong>‘stage compute ratio’</strong> as defined by authors.</p>
<h3 id="macro-design">
  
  
    <a href="#macro-design" class="anchor-heading" aria-labelledby="macro-design"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Macro Design
  
  
</h3>
    

<p>We saw (3,4,6,3) as <strong>‘stage compute ratio’</strong> in ResNet50 as explained previously. In Swin-Transformer the same block distribution was (1,1,9,1).</p>

<p>Hence, <strong>ConvNext tries to follow the same and uses (3,3,9,3) as the block distributions</strong>.</p>
<h3 id="making-it-more-lean-to-reduce-params">
  
  
    <a href="#making-it-more-lean-to-reduce-params" class="anchor-heading" aria-labelledby="making-it-more-lean-to-reduce-params"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Making it more Lean (to reduce params)
  
  
</h3>
    

<p>However, I had to cut down on this to reduce parameter limit and changed the ratios to 
<strong>(6,5,4,4)</strong>. This was chosen after a few ablations but also higher numbers for the initial
blocks were chosen to allow for an optimization on the number of channels at input/output of
each ConvNext stage. Specifically:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># number of channels at input/output of each res_blocks
# Updated Config
</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">400</span><span class="p">]</span>

<span class="c1"># Original Config
# self.channel_list = [96, 192, 384, 768]
</span>
<span class="c1"># number of repeats for each res_block
# Updated Config
</span><span class="bp">self</span><span class="p">.</span><span class="n">block_repeat_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>

<span class="c1"># Original Config
# self.block_repeat_list = [3,3,9,3]
</span></code></pre></div></div>

<p>As you can see, to maintain the relative number of channels at each stage (at least keep it
monotonically increasing as in the original config), I had to increase the initial block_repeats
where the channel size is small and decrease the block_repeats when channel size was larger</p>
<h3 id="resnextify">
  
  
    <a href="#resnextify" class="anchor-heading" aria-labelledby="resnextify"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ResNextify
  
  
</h3>
    

<p>ResNext utilized group convolution in the 3x3 conv layer of bottleneck blocks. What is group
convolution?</p>

<p><img src="/images/ConvNext/group_conv.png" alt="" /></p>

<p>The authors of ConvNext decided to use a special case of group convolution where the number of
groups equals number of channels. <strong>That is literally just Depthwise Seperable Convs!!!</strong></p>
<h4 id="why-depthwise-convolutions">
  
  
    <a href="#why-depthwise-convolutions" class="anchor-heading" aria-labelledby="why-depthwise-convolutions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Depthwise Convolutions
  
  
</h4>
    

<p><em>Depthwise Seperable Conv has two stages: Depthwise Conv (KxKx1 filters) &amp; Pointwise Conv (1x1xC filters)</em></p>

<p>The simple answer is the computational complexitites:</p>
<ul>
  <li>Depth-wise Separable = <code class="language-plaintext highlighter-rouge">O(n**2*d + n*d**2)</code> -&gt; as per Attention is all you need</li>
  <li>Generic Convolution = <code class="language-plaintext highlighter-rouge">O(n**2 * d**2)</code> -&gt; Think of n = filter size spatial, d = filter size depth (num channels)</li>
  <li><a href="https://arxiv.org/pdf/1704.04861.pdf">Reference : <em>MobileNet</em></a></li>
  <li><a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Reference : <em>Attention is All You Need</em></a></li>
</ul>

<p>However, while those numbers may seem weird, for a more practical example you can
view <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728#:~:text=two%20smaller%20kernels.-,Depthwise%20Separable%20Convolutions,it%20is%20more%20commonly%20used.">this post</a>.</p>

<p><strong>Bottomline, MobileNet shows that Depthwise Seperable Conv has much lesser FLOPs than conventional
convolution layers.</strong></p>

<p>There is also some super cool intuition on how Depthwise + Pointwise Conv is similar to
Self Attention!</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>We note that depthwise convolution is similar to the weighted sum operation in self-attention, which operates on a per-channel basis, i.e., only mixing information in the spatial dimension.
</code></pre></div></div>

<p>That’s a little painful to understand. But first read <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">this nice blog</a> to understand the process of self-attention.</p>

<p>In that blog, we have a values matrix had shape <code class="language-plaintext highlighter-rouge">values.shape: torch.Size([6, 28])</code>. We then
compute the attention weights for the second word (second token).</p>

<p>NOTE: Computing attention_weights for second token means using q_2 matrix and multiplying it
with k_1, k_2, k_3, k_4, k_5, k_6 and then some softmax. <strong>But intuition is that second word
was the query or our anchor and we wanted to see how all other words are close/far from second word.</strong></p>

<p>So, if second word is our query, and we got attention weights <code class="language-plaintext highlighter-rouge">attention_weights_2</code>, which
has shape <code class="language-plaintext highlighter-rouge">1x6</code>. We then multiply this <code class="language-plaintext highlighter-rouge">attention_weights_2</code> with the <strong>values matrix
which comprises ALL words/tokens</strong> and has shape <code class="language-plaintext highlighter-rouge">6x28</code></p>

<p>So finally, the <code class="language-plaintext highlighter-rouge">attention_weights_2 @ values</code> yields a <code class="language-plaintext highlighter-rouge">28x1</code> size context vector which
is just one output of the self-attention head.</p>

<p>But, the intuition of Depthwise seperable convolution that comes here is that:</p>

<p><img src="/images/ConvNext/self_attention_depth_conv.jpg" alt="" /></p>

<p>In the above picture, we see that attention of word_1 get’s multiplied with word 1’s positional embedding and there is no cross over. That’s the best
understanding I could infer from the statement.</p>
<h3 id="inverted-bottleneck-and-large-kernels">
  
  
    <a href="#inverted-bottleneck-and-large-kernels" class="anchor-heading" aria-labelledby="inverted-bottleneck-and-large-kernels"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Inverted BottleNeck and Large Kernels
  
  
</h3>
    

<p>Inverted bottlenecks were made famous long back by MobileNetV2 and that design stuck even with Transformers</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th style="text-align: left">ResNet Bottleneck</th>
      <th style="text-align: left">Proposed ConvNext Bottleneck</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/images/ConvNext/resnet_bottleneck.png" alt="" /></td>
      <td style="text-align: left"><img src="/images/ConvNext/convnext_bottleneck.png" alt="" /></td>
    </tr>
  </tbody>
</table></div>

<p>Another aspect to note is that depthwise conv was moved up. The reasoning for this is as follows:</p>
<ul>
  <li>We want to emulate the Swin-T’s large kernel size. We only have one spatial conv layer (that too is depthwise (<code class="language-plaintext highlighter-rouge">d3x3</code>))</li>
  <li>If we do <code class="language-plaintext highlighter-rouge">d3x3</code> as in the middle design of ConvNext Bottleneck that’ll be <code class="language-plaintext highlighter-rouge">d3x3</code> with 384 channels</li>
  <li>Instead if we move it up earlier, the spatial <code class="language-plaintext highlighter-rouge">d3x3</code> Conv happens with 96 channels, similarly the pointwise 1x1 conv happens across 384 channels</li>
  <li>So the 1x1 does the heavy lifting but it’s fast, the <code class="language-plaintext highlighter-rouge">d3x3</code> satisfies the large kernel size requirement with low number of channels</li>
</ul>

<p>To push this further, we increase <code class="language-plaintext highlighter-rouge">d3x3</code> to <code class="language-plaintext highlighter-rouge">d7x7</code> to exactly match the Swin Transformer:</p>

<p><img src="/images/ConvNext/true_convnext_bottleneck.png" alt="" /></p>
<h3 id="activations">
  
  
    <a href="#activations" class="anchor-heading" aria-labelledby="activations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Activations
  
  
</h3>
    

<p>This was very well written in the ConvNext paper, I’m directly quoting it here:
<em>Consider a Transformer
block with key/query/value linear embedding layers, the projection layer, and two linear layers in an MLP block. There
is only one activation function present in the MLP block. In
comparison, it is common practice to append an activation
function to each convolutional layer, including the 1 × 1
convs. Here we examine how performance changes when
we stick to the same strategy. As depicted in Figure 4, we
eliminate all GELU layers from the residual block except
for one between two 1 × 1 layers, replicating the style of a
Transformer block. This process improves the result by 0.7%
to 81.3%, practically matching the performance of Swin-T.</em></p>

<p>So we essentially:</p>
<ul>
  <li>Replaced ReLU with GELU</li>
  <li>Reduced the number of activations</li>
  <li>They also reduced the number of normalizations (mentioned later)</li>
</ul>

<p><img src="/images/ConvNext/ReLU_vs_GELU.png" alt="" /></p>
<h1 id="implementation">
  
  
    <a href="#implementation" class="anchor-heading" aria-labelledby="implementation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementation
  
  
</h1>
    
<h2 id="convnext-block">
  
  
    <a href="#convnext-block" class="anchor-heading" aria-labelledby="convnext-block"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ConvNext Block
  
  
</h2>
    

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Refer : https://browse.arxiv.org/pdf/2201.03545v2.pdf for detailed architechture

    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_ch</span><span class="p">,</span> <span class="n">expansion_factor</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="c1"># num_ch = number of channels at first and third layer of block
</span>        <span class="c1"># There'll be an expansion in the second layer given by expansion_factor
</span>        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="s">"""
        NOTE: To perform depthwise conv we use the param (groups=num_ch)
        to create a separate filter for each input channel
        """</span>


        <span class="bp">self</span><span class="p">.</span><span class="n">main_block</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># 1st conv layer (deptwise)
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="p">,</span>
                            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">num_ch</span><span class="p">),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_ch</span><span class="p">),</span>

            <span class="c1"># 2nd conv layer
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="o">*</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="c1"># 1x1 pointwise convs implemented as Linear Layer
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>

            <span class="c1"># 3rd conv layer
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="o">*</span><span class="n">expansion_factor</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">num_ch</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">main_block</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_mean'</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_std'</span><span class="p">])</span>
                <span class="n">init</span><span class="p">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># define the drop_path layer
</span>        <span class="k">if</span> <span class="n">drop_prob</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">drop_residual_path</span> <span class="o">=</span> <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_prob</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">drop_residual_path</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">main_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># sum the main and shortcut connection
</span>        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_residual_path</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>
<h2 id="network-setup">
  
  
    <a href="#network-setup" class="anchor-heading" aria-labelledby="network-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Network Setup
  
  
</h2>
    

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    ConvNext
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">7001</span><span class="p">,</span> <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">expand_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">backbone_out_channels</span> <span class="o">=</span> <span class="mi">400</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="c1"># number of channels at input/output of each res_blocks
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">400</span><span class="p">]</span>
        <span class="c1"># self.channel_list = [96, 192, 384, 768]
</span>
        <span class="c1"># number of repeats for each res_block
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">block_repeat_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
        <span class="c1"># self.block_repeat_list = [3,3,9,3]
</span>
        <span class="c1"># define number of stages from above
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">num_stages</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">block_repeat_list</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">drop_path_probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_rate</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">))]</span>

        <span class="c1">############## DEFINE RES BLOCK AND AUX LAYERS ########################
</span>
        <span class="c1"># # Define the Stem (the first layer which takes input images)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">stem</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="p">)</span>

        <span class="c1"># truncated normal initialization
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">stem</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">):</span>
                <span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_mean'</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_std'</span><span class="p">])</span>
                <span class="n">init</span><span class="p">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># # Store the LayerNorm and Downsampling layer when switching btw 2 types of res_blocks
</span>        <span class="c1"># self.block_to_block_ln_and_downsample = []
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">stem</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_stages</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">inter_downsample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                    <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                  <span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">inter_downsample</span><span class="p">)</span>

        <span class="c1"># Store the Res_block stages (eg. 3xres_2, 3xres_3, ...)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_stages</span><span class="p">):</span>
            <span class="n">res_block_layer</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">block_repeat_list</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                <span class="n">res_block_layer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ConvNextBlock</span><span class="p">(</span><span class="n">num_ch</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">channel_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                  <span class="n">expansion_factor</span><span class="o">=</span><span class="n">expand_factor</span><span class="p">,</span>
                                  <span class="n">drop_prob</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">drop_path_probabilities</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">j</span><span class="p">]))</span>

            <span class="c1"># append the repeated res_blocks as one layer
</span>            <span class="c1"># *res_block_layer means we add individual elements of the res_block_layer list
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">res_block_layer</span><span class="p">))</span>

        <span class="c1"># truncated normal initialization
</span>        <span class="k">for</span> <span class="n">res_block_stage</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">res_block_stage</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">):</span>
                    <span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_mean'</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_std'</span><span class="p">])</span>
                    <span class="n">init</span><span class="p">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1">#####################################################################
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
              <span class="c1"># essentially stem (replace with stem if it works)
</span>              <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
              <span class="c1"># res_1 block
</span>              <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
              <span class="c1"># res_2 block
</span>              <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
              <span class="c1"># res_3 block
</span>              <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">block_to_block_ln_and_downsample</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
              <span class="c1"># res_4 block
</span>              <span class="bp">self</span><span class="p">.</span><span class="n">res_block_stages</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
              <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
              <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">cls_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">backbone_out_channels</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">))</span>

        <span class="c1"># truncated normal initialization
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">cls_layer</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_mean'</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'truncated_normal_std'</span><span class="p">])</span>
                <span class="n">init</span><span class="p">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_feats</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="s">"""
        What is return_feats? It essentially returns the second-to-last-layer
        features of a given image. It's a "feature encoding" of the input image,
        and you can use it for the verification task. You would use the outputs
        of the final classification layer for the classification task.
        """</span>
        <span class="n">feats</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cls_layer</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_feats</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">feats</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</code></pre></div></div>
<h2 id="stochastic-depth-and-training">
  
  
    <a href="#stochastic-depth-and-training" class="anchor-heading" aria-labelledby="stochastic-depth-and-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Stochastic Depth and Training
  
  
</h2>
    

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DropPath</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Stochastic Depth (we drop the non-shortcut path inside residual blocks with
                      some probability p)
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_probability</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_probability</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># if drop prob is zero or in inference mode, skip this
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isclose</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">drop_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
          <span class="k">return</span> <span class="n">x</span>

        <span class="c1"># find output shape (eg. if input = 4D tensor, output = (1,1,1,1))
</span>        <span class="c1"># output_shape = (x.shape[0],) + (1,) * (x.ndim - 1)
</span>        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># create mask of output shape and of input type on same device
</span>        <span class="n">keep_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">).</span><span class="n">bernoulli_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">drop_prob</span><span class="p">))</span>
        <span class="c1"># Alternative: random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
</span>
        <span class="c1"># NOTE: all methods like bernoulli_ with the underscore suffix means they
</span>        <span class="c1"># are inplace operations
</span>        <span class="n">keep_mask</span><span class="p">.</span><span class="n">div_</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">drop_prob</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">keep_mask</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># multi class classification, hence CELoss and not BCELoss
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'lr'</span><span class="p">],</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">milestones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">80</span><span class="p">]</span>

<span class="c1"># scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.9, total_iters=5)
</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
<span class="c1"># scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')
# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[20, 51])
</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="n">GradScaler</span><span class="p">()</span>
</code></pre></div></div>
<h1 id="appendix">
  
  
    <a href="#appendix" class="anchor-heading" aria-labelledby="appendix"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Appendix
  
  
</h1>
    
<h2 id="time-complexities-analyses">
  
  
    <a href="#time-complexities-analyses" class="anchor-heading" aria-labelledby="time-complexities-analyses"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexities Analyses
  
  
</h2>
    
<h3 id="simple-matrix-multiplication">
  
  
    <a href="#simple-matrix-multiplication" class="anchor-heading" aria-labelledby="simple-matrix-multiplication"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Simple Matrix Multiplication
  
  
</h3>
    

<p>In general if we are multiplying two matrices A (of size {N,D}) and B (of size {D,D}) then
<code class="language-plaintext highlighter-rouge">A@B</code> will involve three nested loops, specifically:</p>

<ul>
  <li>For each of the <strong>N rows</strong> in A
    <ul>
      <li>We perform <strong>D dot products</strong>
        <ul>
          <li>Which each involves <strong>D multiplictions</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Hence, overall time complexity = <code class="language-plaintext highlighter-rouge">N * D * D</code> = <code class="language-plaintext highlighter-rouge">N * D**2</code></p>
<h3 id="time-complexity-analysis-in-tranformers">
  
  
    <a href="#time-complexity-analysis-in-tranformers" class="anchor-heading" aria-labelledby="time-complexity-analysis-in-tranformers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Time Complexity Analysis in Tranformers
  
  
</h3>
    

<p>The transformers are seq2seq models with desired output (during training) is just the
right shifted inputs. For example if input is ‘I am superman’ and we are building a word2word
prediciton language model given input <code class="language-plaintext highlighter-rouge">I</code> the desired output is <code class="language-plaintext highlighter-rouge">am</code> and that makes:</p>

<ul>
  <li>OurInput = <code class="language-plaintext highlighter-rouge">&lt;SOS&gt; I am Superman</code></li>
  <li>Desired output = <code class="language-plaintext highlighter-rouge">I am Superman &lt;EOS&gt;</code></li>
</ul>

<p>Consider we have <code class="language-plaintext highlighter-rouge">N</code> words which we project in embedding layer where each word
gets projected to a vector of shape <code class="language-plaintext highlighter-rouge">D</code>, then a sentence of N words will get
projected to a shape of <code class="language-plaintext highlighter-rouge">N x D</code> (just a matrix where num_rows = num_words and num_cols = projection_size)</p>

<p>Then self attention in scaled-dot-product form:</p>

<p><img src="/images/ConvNext/scaled_dot_prod_attention.png" alt="" /></p>

<p>Will have the following time comlexity</p>

<ol>
  <li>Linearly transforming the rows of <code class="language-plaintext highlighter-rouge">X</code> to compute the query <code class="language-plaintext highlighter-rouge">Q</code>, key <code class="language-plaintext highlighter-rouge">K</code>, and value <code class="language-plaintext highlighter-rouge">V</code> matrices, each of which has shape <code class="language-plaintext highlighter-rouge">(N, D)</code>. This is accomplished by post-multiplying <code class="language-plaintext highlighter-rouge">X</code> with 3 learned matrices of shape <code class="language-plaintext highlighter-rouge">(D, D)</code>, amounting to a computational complexity of <code class="language-plaintext highlighter-rouge">O(N D^2)</code>.</li>
  <li>Computing the layer output, specified in above equation of the paper as <code class="language-plaintext highlighter-rouge">SoftMax(Q @ Kt / sqrt(d)) V</code>, where the softmax is computed over each row. Computing <code class="language-plaintext highlighter-rouge">Q @ Kt</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code>, and post-multiplying the resultant with <code class="language-plaintext highlighter-rouge">V</code> has complexity <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> as well.</li>
</ol>

<p>Overall the time complexity would be <code class="language-plaintext highlighter-rouge">O(N^2.D + N.D^2)</code></p>

<p><strong>NOTE: In the paper, they say it takes only <code class="language-plaintext highlighter-rouge">O(N^2 D)</code> for Self Attention, but this excludes
the calculation of Q,K,V</strong></p>
<h4 id="comparison-with-rnns">
  
  
    <a href="#comparison-with-rnns" class="anchor-heading" aria-labelledby="comparison-with-rnns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparison with RNNs
  
  
</h4>
    

<p>RNNs have a hidden state neuron which is connected across the time series as shown below:</p>

<p><img src="/images/ConvNext/RNN_simple.png" alt="" /></p>

<p>The hidden neuron computation is simply: <code class="language-plaintext highlighter-rouge">h(t)​ = f(U x(t)​ + W h(t−1)​)</code></p>

<p>Hence, they are modelled as O(n * d<em>*2) *(as it’s an MLP with matrix multiplication, see Appendix)</em> with O(n) sequential operations</p>
<h4 id="comparisons-with-separable-and-non-separable-convs">
  
  
    <a href="#comparisons-with-separable-and-non-separable-convs" class="anchor-heading" aria-labelledby="comparisons-with-separable-and-non-separable-convs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparisons with Separable and Non-Separable Convs
  
  
</h4>
    

<ul>
  <li>Depth-wise Separable = <code class="language-plaintext highlighter-rouge">O(n**2*d + n*d**2)</code> = Self Attention + Feed Forward MLP</li>
  <li>Generic Convolution = <code class="language-plaintext highlighter-rouge">O(n**2 * d**2)</code></li>
</ul>
<h4 id="conclusion">
  
  
    <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion:
  
  
</h4>
    

<p>The authors of <em>Attention is All You Need</em> therefore claim that Self Attention (<code class="language-plaintext highlighter-rouge">O(N**2*D)</code> or truly <code class="language-plaintext highlighter-rouge">O(N**2*D + N*D**2)</code>) is parallelizable
and faster than the next best option -&gt; i.e. Depthwise Separable Convolution (<code class="language-plaintext highlighter-rouge">O(N**2*D + N*D**2)</code>)</p>

<p>Considering the true calculation of Scaled Dot Product Attention, it seems to be the same
as Depthwise Separable Convolution.</p>

          

          
            
          
        </main>
        

  <hr>
  <footer>
    

    <p class="text-small text-grey-dk-100 mb-0"></p>

    
      <div class="d-flex mt-2">
        
        
      </div>
    
  </footer>


      </div>
    </div>
    
      

<div class="search-overlay"></div>

    
  </div>

  
</body>
</html>

