

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Classifiers (IDL2) | My Projects</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Classifiers (IDL2)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Computer Vision projects and assignments during my time in CMU" />
<meta property="og:description" content="Computer Vision projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/docs/DL_Overview/IDL2.html" />
<meta property="og:url" content="http://localhost:4000/docs/DL_Overview/IDL2.html" />
<meta property="og:site_name" content="My Projects" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Classifiers (IDL2)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Computer Vision projects and assignments during my time in CMU","headline":"Classifiers (IDL2)","url":"http://localhost:4000/docs/DL_Overview/IDL2.html"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  My Projects

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/numpy/" class="nav-list-link">Numpy</a></li><li class="nav-list-item"><a href="http://localhost:4000/spatial_pyramid_matching/" class="nav-list-link">Spatial Pyramid Matching for Scene Classification</a></li><li class="nav-list-item"><a href="http://localhost:4000/optical_flow/" class="nav-list-link">Optical Flow and Image Alignment</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D_reconstruction</a></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/DL_overview" class="nav-list-link">DL Overview</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/DL_old_arches.html" class="nav-list-link">DL Simple Architechtures</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item  active"><a href="http://localhost:4000/docs/DL_Overview/IDL2.html" class="nav-list-link active">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/Resnet.html" class="nav-list-link">RESNET</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Intro%20to%20ML" class="nav-list-link">Intro to ML</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Intro%20to%20ML/Basics.html" class="nav-list-link">Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Intro%20to%20ML/Linear_Regression.html" class="nav-list-link">Linear Regression</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/pytorch/" class="nav-list-link">Intro to Pytorch</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/camera_model/" class="nav-list-link">Camera Models</a></li><li class="nav-list-item"><a href="http://localhost:4000/git_concepts" class="nav-list-link">Git Concepts</a></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search My Projects" aria-label="Search My Projects" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
              
                <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/DL_overview">DL Overview</a></li>
              
              <li class="breadcrumb-nav-list-item"><span>Classifiers (IDL2)</span></li>
            </ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary>
    Table of contents
  {: .text-delta }
  </summary>
<ol id="markdown-toc">
  <li><a href="#before-you-begin" id="markdown-toc-before-you-begin">Before you Begin</a></li>
  <li><a href="#binary-classifiers-and-cross-entropy-loss" id="markdown-toc-binary-classifiers-and-cross-entropy-loss">Binary Classifiers and Cross Entropy Loss</a>    <ol>
      <li><a href="#why-is-cross-entropy-better-than-l2" id="markdown-toc-why-is-cross-entropy-better-than-l2">Why is Cross Entropy better than L2?</a></li>
    </ol>
  </li>
  <li><a href="#multi-class-cross-entropy" id="markdown-toc-multi-class-cross-entropy">Multi-Class Cross Entropy</a>    <ol>
      <li><a href="#label-smoothening" id="markdown-toc-label-smoothening">Label Smoothening</a></li>
    </ol>
  </li>
  <li><a href="#simple-2-layer-network-beautiful-diagram" id="markdown-toc-simple-2-layer-network-beautiful-diagram">Simple 2 layer network (beautiful diagram!)</a>    <ol>
      <li><a href="#backprop" id="markdown-toc-backprop">Backprop</a></li>
    </ol>
  </li>
  <li><a href="#special-cases" id="markdown-toc-special-cases">Special Cases</a>    <ol>
      <li><a href="#scalar-vs-vector-activations" id="markdown-toc-scalar-vs-vector-activations">Scalar vs Vector activations</a>        <ol>
          <li><a href="#example-of-a-vector-activation-softmax-activation" id="markdown-toc-example-of-a-vector-activation-softmax-activation">Example of a vector activation: softmax activation</a></li>
        </ol>
      </li>
      <li><a href="#sub-gradients" id="markdown-toc-sub-gradients">Sub-gradients</a></li>
    </ol>
  </li>
  <li><a href="#training-process" id="markdown-toc-training-process">Training Process</a>    <ol>
      <li><a href="#vector-formulation" id="markdown-toc-vector-formulation">Vector Formulation</a>        <ol>
          <li><a href="#forward-pass" id="markdown-toc-forward-pass">Forward Pass</a></li>
          <li><a href="#backward-pass" id="markdown-toc-backward-pass">Backward Pass</a>            <ol>
              <li><a href="#special-cases-1" id="markdown-toc-special-cases-1">Special Cases</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#backward-pass-summary" id="markdown-toc-backward-pass-summary">Backward pass summary</a></li>
      <li><a href="#loss-surface" id="markdown-toc-loss-surface">Loss Surface</a></li>
    </ol>
  </li>
  <li><a href="#issues-with-convergence" id="markdown-toc-issues-with-convergence">Issues with Convergence</a>    <ol>
      <li><a href="#convergence-for-a-convex-problem" id="markdown-toc-convergence-for-a-convex-problem">Convergence for a Convex Problem</a>        <ol>
          <li><a href="#multivariate-convex" id="markdown-toc-multivariate-convex">Multivariate Convex</a>            <ol>
              <li><a href="#the-math-for-the-above-steps" id="markdown-toc-the-math-for-the-above-steps">The math for the above steps</a></li>
            </ol>
          </li>
          <li><a href="#general-case-of-convex-functions-function-has-higher-order-ie-not-a-quadratic" id="markdown-toc-general-case-of-convex-functions-function-has-higher-order-ie-not-a-quadratic">General Case of Convex Functions (function has higher order, i.e. not a quadratic)</a>            <ol>
              <li><a href="#issues-with-the-above-process" id="markdown-toc-issues-with-the-above-process">Issues with the above process</a></li>
              <li><a href="#solutions" id="markdown-toc-solutions">Solutions</a></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#sgd-vs-batch-gd" id="markdown-toc-sgd-vs-batch-gd">SGD vs Batch GD</a>    <ol>
      <li><a href="#sgd" id="markdown-toc-sgd">SGD</a></li>
      <li><a href="#batch-gd" id="markdown-toc-batch-gd">Batch GD</a></li>
      <li><a href="#problems-with-sgg" id="markdown-toc-problems-with-sgg">Problems with SGG</a></li>
      <li><a href="#middle-ground-solution-mini-batches" id="markdown-toc-middle-ground-solution-mini-batches">Middle Ground Solution: Mini Batches</a></li>
    </ol>
  </li>
</ol>

</details>
      <h2 id="before-you-begin">
        
        
          <a href="#before-you-begin" class="anchor-heading" aria-labelledby="before-you-begin"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Before you Begin
        
        
      </h2>
    

<p><a href="https://www.youtube.com/watch?v=MPtEhEsgacM&amp;list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe&amp;index=11&amp;t=1002s&amp;ab_channel=CarnegieMellonUniversityDeepLearning" class="btn fs-3 mb-4 mb-md-0">Ref: 11-785</a></p>
      <h1 id="binary-classifiers-and-cross-entropy-loss">
        
        
          <a href="#binary-classifiers-and-cross-entropy-loss" class="anchor-heading" aria-labelledby="binary-classifiers-and-cross-entropy-loss"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Binary Classifiers and Cross Entropy Loss
        
        
      </h1>
    

<p>This is usually used for classification. For a binary case, it is given by the following
formula:</p>

<p><img src="/images/IDL2/CE_loss.png" alt="" /></p>

<p>Notice that the equation uses the term divergence here. This is actually the background term
for ‘loss’ in our situation. <strong>Divergence tells us how <em>off</em> we are from the correct solution</strong>. <br />
Note. divergence is not direction dependent, it just tells how far away we are from the
desirable output.</p>

<p>More formally, loss = average divergence of our output w.r.t the ground truth</p>

<p>Therefore, in a binary setting, if we use softmax as our activation function -&gt; we get the
class probablity score as the output. In the binary case we get (y, 1-y) as our output.</p>

<ul>
  <li>let y = output of softmax for each class (we have 2 classes)</li>
  <li>let d = ground truth</li>
  <li>Now, plugging in (y, 1-y) into the Cross Entropy loss formula above</li>
  <li>We see that when y = 0 and when y = 1, since log(0) = -infinity and log(1) = 0</li>
  <li><strong>Therefore, if d=0 and y=1, we get infinity, if d=1 and y=0 we also get infinity</strong></li>
</ul>

<p>Now it’s also interesting to observe the derivative of cross entropy loss function. The 
derivative is shown below:</p>

<p><img src="/images/IDL2/CE_loss_deriv.png" alt="" /></p>

<p>Now, notice the following cases:</p>
<ul>
  <li>Case 1: When d=1 and y=1, plugging into the above formula, we get <code class="language-plaintext highlighter-rouge">derivative(CE_loss) = -1</code></li>
  <li>Case 2: When d=0 and y=0, plugging into above forumula we get <code class="language-plaintext highlighter-rouge">derivative(CE_loss) = 1</code></li>
  <li><strong>Note, if you assumed that if output(y) = desired(d) would have zero gradient, you’re wrong!</strong></li>
  <li>The above two cases are plotted below</li>
  <li>
    <div class="table-wrapper"><table>
      <thead>
        <tr>
          <th style="text-align: left">Case 1</th>
          <th style="text-align: left">Case 1 and Case 2</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: left"><img src="/images/IDL2/CE_deriv1.png" alt="" /></td>
          <td style="text-align: left"><img src="/images/IDL2/CE_deriv2.png" alt="" /></td>
        </tr>
      </tbody>
    </table></div>
  </li>
</ul>

<p>However, instead of cross entropy loss, if we were to use a simple L2 error 
(sum of sqaured diffs (quadratic function)) we would get a bowl shaped instead like:</p>

<p><img src="/images/IDL2/L2_deriv.png" alt="" /></p>

<p>An extract from the <a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier Intitialization paper</a> shows this more accurately</p>

<p><img src="/images/resnet/cr_vs_quad.png" alt="" /></p>

<p>From the above picture, one can see that the cross entropy loss surface (black)
is much steeper than the quadratic surface (red)</p>
      <h2 id="why-is-cross-entropy-better-than-l2">
        
        
          <a href="#why-is-cross-entropy-better-than-l2" class="anchor-heading" aria-labelledby="why-is-cross-entropy-better-than-l2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why is Cross Entropy better than L2?
        
        
      </h2>
    

<p>Ans. The L2 is a quadratic loss function, which is smooth bowl. Now from the above picture, <br />
you can see that doing gradient descent on L2 would take so much longer than using it on the
steeper curve of the cross entropy loss!</p>
      <h1 id="multi-class-cross-entropy">
        
        
          <a href="#multi-class-cross-entropy" class="anchor-heading" aria-labelledby="multi-class-cross-entropy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multi-Class Cross Entropy
        
        
      </h1>
    

<p><img src="/images/IDL2/multi_class_CE.png" alt="" /></p>

<p>Here we only have y_i (i.e. one class which we’re looking for in our loss function)
Therefore, the derivative will look different as seen above</p>

<p>The problem with the above definition of CE (cross entropy) is that derivative of loss
for all other classes is zero. Which isn’t desirable for fast convergence. Therefore, we
slightly modify the labels ‘d’ as shown below:</p>
      <h2 id="label-smoothening">
        
        
          <a href="#label-smoothening" class="anchor-heading" aria-labelledby="label-smoothening"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Label Smoothening
        
        
      </h2>
    

<p>Here we change our target label to (1-(K-1)*e) instead of just being 1</p>

<p><img src="/images/IDL2/Label_smoothening.png" alt="" /></p>
      <h1 id="simple-2-layer-network-beautiful-diagram">
        
        
          <a href="#simple-2-layer-network-beautiful-diagram" class="anchor-heading" aria-labelledby="simple-2-layer-network-beautiful-diagram"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Simple 2 layer network (beautiful diagram!)
        
        
      </h1>
    

<p><img src="/images/IDL2/2layernet.png" alt="" /></p>
      <h2 id="backprop">
        
        
          <a href="#backprop" class="anchor-heading" aria-labelledby="backprop"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Backprop
        
        
      </h2>
    

<ol>
  <li>Derivative w.r.t to the loss was already computer in previous section.</li>
  <li>
    <p>Now, derivative w.r.t the activation function is shown below</p>

    <p><img src="/images/IDL2/FC_backprop3.png" alt="" />
<img src="/images/IDL2/FC_backprop4.png" alt="" /></p>

    <p>Example: Sigmoid Derivative <br />
<img src="/images/IDL4/sigmoid_derivative.png" alt="" /></p>

    <p>Example: Tanh Derivative
<img src="/images/IDL4/tan_derivative.png" alt="" /></p>

    <p>Example: Logistic Derivative
<img src="/images/IDL4/logistic_derivative.png" alt="" /></p>

    <p>Note. tanh is a scaled and shifted version of sigmoid. This is shown below by
just rearranging some terms: <br />
<img src="/images/IDL4/tanh_something.png" alt="" /></p>
  </li>
  <li>
    <p>Computing derivate w.r.t one weight (one weight connects one neuron in layer N-1 to <br />
another neuron in layer 2)</p>

    <p><img src="/images/IDL2/FC_backprop1.png" alt="" /></p>
  </li>
  <li>
    <p>Computing the derivative w.r.t y (y = output of activation function) <br />
Here, one neuron will have effect on all the neurons in the next layer. This is why we need
to sum the derivates of z (z = wx + b of next layer) w.r.t y(from previous layer)
(This is explained better in the <a href="#scalar-vs-vector-activations">Scalar vs Vector activations</a>)</p>

    <p><img src="/images/IDL2/FC_backprop2.png" alt="" /></p>
  </li>
</ol>
      <h1 id="special-cases">
        
        
          <a href="#special-cases" class="anchor-heading" aria-labelledby="special-cases"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Special Cases
        
        
      </h1>
    
      <h2 id="scalar-vs-vector-activations">
        
        
          <a href="#scalar-vs-vector-activations" class="anchor-heading" aria-labelledby="scalar-vs-vector-activations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Scalar vs Vector activations
        
        
      </h2>
    

<p>We assumed activation to be neuron specific. However, this may not be the case!</p>

<p><img src="/images/IDL2/special_case1.png" alt="" /></p>

<p><img src="/images/IDL2/special_case2.png" alt="" /></p>

<p>Also, the backprop gets bit murky as well</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th style="text-align: left">Scalar Activation</th>
      <th style="text-align: left">Vector Activation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/images/IDL2/special_case3.png" alt="" /></td>
      <td style="text-align: left"><img src="/images/IDL2/special_case4.png" alt="" /></td>
    </tr>
  </tbody>
</table></div>

<p><strong><em>Note. The important aspect to remember is that for a vector activation, the derivative
of divergence w.r.t any input (input to activation func) is a sum of partial derivative
on every neuron of activation function as seen in picture above</em></strong></p>
      <h3 id="example-of-a-vector-activation-softmax-activation">
        
        
          <a href="#example-of-a-vector-activation-softmax-activation" class="anchor-heading" aria-labelledby="example-of-a-vector-activation-softmax-activation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example of a vector activation: softmax activation
        
        
      </h3>
    

<p><img src="/images/IDL2/special_case5.png" alt="" /></p>
      <h2 id="sub-gradients">
        
        
          <a href="#sub-gradients" class="anchor-heading" aria-labelledby="sub-gradients"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sub-gradients
        
        
      </h2>
    

<p>For RELU’s, the origin is not smooth and the gradient cannot be computed. Instead we use a <br />
sub-gradient which is shown as multiple lines in the figure below. However, we just use
the sub-gradient line which is parallel to the x-axis and define the gradient as = 1 at origin</p>

<p><img src="/images/IDL2/special_case6.png" alt="" /></p>
      <h1 id="training-process">
        
        
          <a href="#training-process" class="anchor-heading" aria-labelledby="training-process"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Training Process
        
        
      </h1>
    
      <h2 id="vector-formulation">
        
        
          <a href="#vector-formulation" class="anchor-heading" aria-labelledby="vector-formulation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Vector Formulation
        
        
      </h2>
    
      <h3 id="forward-pass">
        
        
          <a href="#forward-pass" class="anchor-heading" aria-labelledby="forward-pass"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Forward Pass
        
        
      </h3>
    
<p>In the below picture, the first row of weights vector represents all of the weights going to
the first neuron.</p>

<p><img src="/images/IDL2/training1.png" alt="" />
<img src="/images/IDL2/training2.png" alt="" /></p>
      <h3 id="backward-pass">
        
        
          <a href="#backward-pass" class="anchor-heading" aria-labelledby="backward-pass"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Backward Pass
        
        
      </h3>
    

<p>Now, if z = output of affine function (wx + b) and f(z) = output of activation <br />
Having vectorized activations will cause the below <strong>backprop through y = f(</strong>z)</p>

<p><img src="/images/IDL2/training3.png" alt="" /></p>

<p>The Jacobian will therefore be the multivariant form of gradient, giving us the
direction in which incresing delta(z) will cause the max increase in delta(y)</p>

<p><strong>Rule of thumb: the derivative of [y(scalar) = f(z(matrix))] = matrix.T shape</strong> <br />
<strong>Extension: the derivative of scalar(row_vector) = column vector</strong></p>
      <h4 id="special-cases-1">
        
        
          <a href="#special-cases-1" class="anchor-heading" aria-labelledby="special-cases-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Special Cases
        
        
      </h4>
    

<p><img src="/images/IDL2/training4.png" alt="" /></p>

<p><img src="/images/IDL2/training5.png" alt="" /></p>
      <h2 id="backward-pass-summary">
        
        
          <a href="#backward-pass-summary" class="anchor-heading" aria-labelledby="backward-pass-summary"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Backward pass summary
        
        
      </h2>
    

<ol>
  <li>
    <p>For the first gradient, we calculate the fastest increase in Y which gets the fastest
increase in loss, where loss is given as</p>

    <p><img src="/images/IDL2/training6.png" alt="" /></p>

    <p>The derivative therefore becomes <br />
<img src="/images/IDL2/training7.png" alt="" /></p>

    <p>Where Y = column vector, therefore derivative of Y w.r.t Divergence(loss) = delta(Y)*Div shown in picture above. This grad(y) is a row vector!</p>
  </li>
  <li>
    <p>Now we compute derivative through affine variable z = wx+b, then we do
<img src="/images/IDL2/training8.png" alt="" /></p>
  </li>
  <li>
    <p>Now, derivative w.r.t previous Y (Y(n-1)) will be
<img src="/images/IDL2/training9.png" alt="" /></p>
  </li>
  <li><strong>Remember, as we go back we just post multiply by:</strong>
    <ul>
      <li>A jacobian if it’s an activation layer (vector activation)</li>
      <li>A weight matrix for an affine layer (scalar activation)</li>
    </ul>
  </li>
  <li>Now, two more things tbd are derivatives on weights and biases!</li>
  <li>
    <p>Since bias should be the same size as our vector activation (z) therefore the
defivative w.r.t the bias is</p>

    <p><img src="/images/IDL2/training10.png" alt="" /></p>
  </li>
  <li>
    <p>Similarly, the derivative of the weights is given by
<img src="/images/IDL2/training11.png" alt="" /> which will have the same shape as the weight matrix</p>
  </li>
  <li>
    <p><strong>Remember, all the shapes of the derivatives should match the shapes of the 
weights or the bias itself</strong></p>
  </li>
  <li><img src="/images/IDL2/training12.png" alt="" /></li>
</ol>
      <h2 id="loss-surface">
        
        
          <a href="#loss-surface" class="anchor-heading" aria-labelledby="loss-surface"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Loss Surface
        
        
      </h2>
    

<ul>
  <li>The common hypothesis is that in large networks there are lot more saddle points
than global minima.</li>
  <li>A saddle point is defined as one where moving in one direction increases loss and
moving in other direction decreases. <em>I.e. depending on which direciton you’re looking at
you can be at a minima or maxima.</em> <strong>Also the slope at saddle points is zero (therefore, 
you’ll get stuck with gradient descent)</strong></li>
</ul>

<p><img src="/images/IDL2/training13.png" alt="" /></p>
      <h1 id="issues-with-convergence">
        
        
          <a href="#issues-with-convergence" class="anchor-heading" aria-labelledby="issues-with-convergence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Issues with Convergence
        
        
      </h1>
    

<p><img src="/images/IDL2/convergence1.png" alt="" /></p>

<p><img src="/images/IDL2/convergence2.png" alt="" /></p>

<p><img src="/images/IDL2/convergence3.png" alt="" /></p>
      <h2 id="convergence-for-a-convex-problem">
        
        
          <a href="#convergence-for-a-convex-problem" class="anchor-heading" aria-labelledby="convergence-for-a-convex-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convergence for a Convex Problem
        
        
      </h2>
    

<p>Consider a simple quadratic case</p>

<p><img src="/images/IDL2/convergence5.jpg" alt="" /></p>

<p><img src="/images/IDL2/convergence4.png" alt="" /></p>

<p><strong>Note. Optimizing w.r.t the second order is called Newton’s method</strong></p>
      <h3 id="multivariate-convex">
        
        
          <a href="#multivariate-convex" class="anchor-heading" aria-labelledby="multivariate-convex"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multivariate Convex
        
        
      </h3>
    

<p><img src="/images/IDL2/convergence6.png" alt="" /></p>

<p>Now, the A matrix will introduce different slopes in different axes since it’s
multivariate. To mitigate this we do</p>

<p><img src="/images/IDL2/convergence3.png" alt="" /></p>

<p>Now, you see that the A matrix has been removed <strong>(think of it as having become identity)</strong></p>

<p>Then as we saw in our derivation above, the optimal step size will then become <br />
inverse(A) = inverse(I) = 1.</p>

<p><strong>Therefore, the optimal step size is now the same in all dimensions</strong></p>
      <h4 id="the-math-for-the-above-steps">
        
        
          <a href="#the-math-for-the-above-steps" class="anchor-heading" aria-labelledby="the-math-for-the-above-steps"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The math for the above steps
        
        
      </h4>
    

<p><img src="/images/IDL2/convergence7.png" alt="" />
<img src="/images/IDL2/convergence8.png" alt="" /></p>

<p>Points to note</p>

<ol>
  <li>In the simple scalar quadratic space the optimal step size was one value</li>
  <li>In the multivariate space, we need to scale and then find the optimal step</li>
  <li>However, after scaling we can still achieve single step move to global minima
even in multivariate space, we just need to find the inverse of a matrix</li>
</ol>
      <h3 id="general-case-of-convex-functions-function-has-higher-order-ie-not-a-quadratic">
        
        
          <a href="#general-case-of-convex-functions-function-has-higher-order-ie-not-a-quadratic" class="anchor-heading" aria-labelledby="general-case-of-convex-functions-function-has-higher-order-ie-not-a-quadratic"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> General Case of Convex Functions (function has higher order, i.e. not a quadratic)
        
        
      </h3>
    

<ul>
  <li>Even in such cases we can find Taylor expansions and just truncate upto second order</li>
  <li>In such cases it’ll just be an approximation, but let’s live with that</li>
  <li><img src="/images/IDL2/convergence9.png" alt="" /></li>
  <li><strong>Here we see that the second derivative is replaced by a Hessian</strong></li>
  <li>Therefore in this case, the <strong>optimum step size would be the inverse(Hessian)</strong></li>
  <li>The normalized and optimal update step in gradient descent form is shown below
<img src="/images/IDL2/convergence10.png" alt="" /></li>
</ul>
      <h4 id="issues-with-the-above-process">
        
        
          <a href="#issues-with-the-above-process" class="anchor-heading" aria-labelledby="issues-with-the-above-process"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Issues with the above process
        
        
      </h4>
    

<p><img src="/images/IDL2/convergence11.png" alt="" /></p>

<p><img src="/images/IDL2/convergence12.png" alt="" /></p>
      <h4 id="solutions">
        
        
          <a href="#solutions" class="anchor-heading" aria-labelledby="solutions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Solutions
        
        
      </h4>
    

<p><img src="/images/IDL2/convergence13.png" alt="" /></p>

<p><img src="/images/IDL2/convergence14.png" alt="" /></p>

<p><img src="/images/IDL2/convergence15.png" alt="" /></p>

<p><img src="/images/IDL2/Convergence16.png" alt="" /></p>

<p>In the momentum method, the first term in RHS is the
scaled term of previous weight update being addes to the current
update step.</p>

<ul>
  <li>Big red vector = previous update step</li>
  <li>Blue vector = 2nd term of RHS above</li>
  <li>Small red vector = scaled version of big red vector</li>
  <li>black vector = final update (LHS term)</li>
</ul>
      <h1 id="sgd-vs-batch-gd">
        
        
          <a href="#sgd-vs-batch-gd" class="anchor-heading" aria-labelledby="sgd-vs-batch-gd"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> SGD vs Batch GD
        
        
      </h1>
    
      <h2 id="sgd">
        
        
          <a href="#sgd" class="anchor-heading" aria-labelledby="sgd"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> SGD
        
        
      </h2>
    

<p>In SGD we update after every training instance. The caveat for convergence is that
the increments should be small and not too large. The increments should also shrink
so that we don’t keep shifting around the decision boundary too much due to to just
one training instance.</p>

<p><img src="/images/IDL2/sgd1.png" alt="" /></p>

<p>If we define epsilon to be the margin by which we need to be within to have ‘converged’, then
using the above optimal learning rate of (1/k) where <strong>k = no. of layers</strong>, we see that after
one iteration we should be within (1/k)*desired_range.</p>

<p><img src="/images/IDL2/sgd2.png" alt="" /></p>

<p>Therefore if we only need to be epsilon*desired range, we can reach it in O(1/epsilon)</p>
      <h2 id="batch-gd">
        
        
          <a href="#batch-gd" class="anchor-heading" aria-labelledby="batch-gd"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Batch GD
        
        
      </h2>
    

<p><img src="/images/IDL2/batch_gd1.png" alt="" /></p>
      <h2 id="problems-with-sgg">
        
        
          <a href="#problems-with-sgg" class="anchor-heading" aria-labelledby="problems-with-sgg"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Problems with SGG
        
        
      </h2>
    

<ol>
  <li>If our job was to minimize the shaded area in the below picture (shaded area = divergence)
then, we would want to push the red line up or down (blue = ground truth)</li>
  <li>If we look at the curve at it’s current location, we would want to move the red curve down
drastically.
<img src="/images/IDL2/sgd23.png" alt="" /></li>
  <li>In the below picture, we would want to push up our red curve drastically
<img src="/images/IDL2/sgd4.png" alt="" /></li>
  <li>Therefore, the problem becomes that the estimated loss and subsequent update has too
high a variance!</li>
  <li>However, despite all this SGD is fast since it works only on 1 sample at a time</li>
</ol>
      <h2 id="middle-ground-solution-mini-batches">
        
        
          <a href="#middle-ground-solution-mini-batches" class="anchor-heading" aria-labelledby="middle-ground-solution-mini-batches"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Middle Ground Solution: Mini Batches
        
        
      </h2>
    

<p>Here we compute the average loss over a mini-batch and use this averaged loss for update</p>

<p><img src="/images/IDL2/mini_batch1.png" alt="" /></p>

<p><strong>But how does the variance of mini-batch compare to that of full-batch gradient descent?</strong></p>

<ul>
  <li>
    <p>Variance of mini-batch GD where b = batch size is:
<img src="/images/IDL2/batch_gd_1.png" alt="" /></p>
  </li>
  <li>
    <p>Variace of full-batch GD will be (1/N) instead of (1/b)</p>
  </li>
  <li>
    <p>Now, if we have 1000 training samples, it can be seen that 1/100 is small enough
that it won’t make that much of a difference if it’s 1/100 or 1/1000. This is why
mini-batching works, i.e. even with 100 samples we capture almost the same variance
as we would if we took all training samples into consideration</p>
  </li>
</ul>

<p><img src="/images/IDL2/mini_batch_gd2.png" alt="" /></p>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

