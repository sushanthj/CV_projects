

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Intro to CNNs | My Projects</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Intro to CNNs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Computer Vision projects and assignments during my time in CMU" />
<meta property="og:description" content="Computer Vision projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/docs/DL_Overview/IDL4.html" />
<meta property="og:url" content="http://localhost:4000/docs/DL_Overview/IDL4.html" />
<meta property="og:site_name" content="My Projects" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Intro to CNNs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Computer Vision projects and assignments during my time in CMU","headline":"Intro to CNNs","url":"http://localhost:4000/docs/DL_Overview/IDL4.html"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  My Projects

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/numpy/" class="nav-list-link">Numpy</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D_reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/optical_flow/" class="nav-list-link">Optical Flow and Image Alignment</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/spatial_pyramid_matching/" class="nav-list-link">Spatial Pyramid Matching for Scene Classification</a></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/DL_overview" class="nav-list-link">DL Overview</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/DL_old_arches.html" class="nav-list-link">DL Simple Architechtures</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item  active"><a href="http://localhost:4000/docs/DL_Overview/IDL4.html" class="nav-list-link active">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/DL_Overview/Resnet.html" class="nav-list-link">RESNET</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Intro%20to%20ML" class="nav-list-link">Intro to ML</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Intro%20to%20ML/Basics.html" class="nav-list-link">Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Intro%20to%20ML/Linear_Regression.html" class="nav-list-link">Linear Regression</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/pytorch/" class="nav-list-link">Intro to Pytorch</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><a href="http://localhost:4000/camera_model/" class="nav-list-link">Camera Models</a></li><li class="nav-list-item"><a href="http://localhost:4000/git_concepts" class="nav-list-link">Git Concepts</a></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search My Projects" aria-label="Search My Projects" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
              
                <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/DL_overview">DL Overview</a></li>
              
              <li class="breadcrumb-nav-list-item"><span>Intro to CNNs</span></li>
            </ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary>
    Table of contents
  {: .text-delta }
  </summary>
<ol id="markdown-toc">
  <li><a href="#before-you-begin" id="markdown-toc-before-you-begin">Before you Begin</a></li>
  <li><a href="#simple-method-of-achieving-shift-invariance" id="markdown-toc-simple-method-of-achieving-shift-invariance">Simple method of achieving shift invariance</a>    <ol>
      <li><a href="#important-backprop-theory" id="markdown-toc-important-backprop-theory">Important Backprop Theory</a></li>
      <li><a href="#summary" id="markdown-toc-summary">Summary</a>        <ol>
          <li><a href="#q-in-which-layer-would-you-expect-to-see-something-that-looks-like-a-flower-" id="markdown-toc-q-in-which-layer-would-you-expect-to-see-something-that-looks-like-a-flower-">Q. In which layer would you expect to see something that looks like a flower? \</a></li>
          <li><a href="#q-why-do-we-need-to-distribute-the-scanning-and-not-have-one-level-of-neurons-scan-the-entire-window-at-the-same-time" id="markdown-toc-q-why-do-we-need-to-distribute-the-scanning-and-not-have-one-level-of-neurons-scan-the-entire-window-at-the-same-time">Q. Why do we need to distribute the scanning and not have one level of neurons scan the entire window at the same time?</a>            <ol>
              <li><a href="#distributed-vs-undistributed-scanning-for-images" id="markdown-toc-distributed-vs-undistributed-scanning-for-images">Distributed vs Undistributed scanning for images</a></li>
              <li><a href="#main-intuition" id="markdown-toc-main-intuition">Main Intuition</a></li>
            </ol>
          </li>
        </ol>
      </li>
      <li><a href="#nice-take-on-max-pooling-why-is-it-needed" id="markdown-toc-nice-take-on-max-pooling-why-is-it-needed">Nice take on Max Pooling (Why is it needed?)</a>        <ol>
          <li><a href="#nice-consequence-of-above-logic" id="markdown-toc-nice-consequence-of-above-logic">Nice consequence of above logic</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#convolutional-nns" id="markdown-toc-convolutional-nns">Convolutional NNs</a>    <ol>
      <li><a href="#number-of-parameters-in-a-conv-layer" id="markdown-toc-number-of-parameters-in-a-conv-layer">Number of Parameters in a Conv Layer</a></li>
      <li><a href="#types-of-filters" id="markdown-toc-types-of-filters">Types of Filters</a>        <ol>
          <li><a href="#more-on-1x1-convolution" id="markdown-toc-more-on-1x1-convolution">More on 1x1 Convolution</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#importatnt-to-remember" id="markdown-toc-importatnt-to-remember">Importatnt to Remember</a></li>
</ol>

</details>
      <h2 id="before-you-begin">
        
        
          <a href="#before-you-begin" class="anchor-heading" aria-labelledby="before-you-begin"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Before you Begin
        
        
      </h2>
    

<p><a href="https://www.youtube.com/watch?v=VINm_uHUgF0&amp;list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe&amp;index=16&amp;ab_channel=CarnegieMellonUniversityDeepLearning" class="btn fs-3 mb-4 mb-md-0">Ref: 11-785</a></p>
      <h1 id="simple-method-of-achieving-shift-invariance">
        
        
          <a href="#simple-method-of-achieving-shift-invariance" class="anchor-heading" aria-labelledby="simple-method-of-achieving-shift-invariance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Simple method of achieving shift invariance
        
        
      </h1>
    

<p>Assume we have a simple MLP which was trained to identify a flower. Now, if we run the MLP
blindly on the image, we will NOT have shift invariance.</p>

<p>A simple solution would be to scan the image at different positions and take the region which
gave the max output of an activation (say largest softmax class score).</p>

<p><img src="/images/IDL4/scanMLP0.png" alt="" />
<img src="/images/IDL4/scanMLP1.png" alt="" /></p>
      <h2 id="important-backprop-theory">
        
        
          <a href="#important-backprop-theory" class="anchor-heading" aria-labelledby="important-backprop-theory"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Important Backprop Theory
        
        
      </h2>
    
<ul>
  <li>Now, to train this, the initial 3 layers are seen to have the same weights (<strong>shared weights</strong>)</li>
  <li>
    <p>Therefore treat it like a vector function where each and every window affects the final
classification head. Now, if we want to backprop through such a function which depends
on each and every window, we need to sum over the activations.</p>

    <p><img src="/images/IDL4/scanMLP2.png" alt="" /></p>
  </li>
  <li>Similaraly, the update step will also be such that the updated weights effect each and
every one of the input weights equally as shown below:
<img src="/images/IDL4/scanMLP3.png" alt="" /></li>
</ul>
      <h2 id="summary">
        
        
          <a href="#summary" class="anchor-heading" aria-labelledby="summary"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Summary
        
        
      </h2>
    

<p><img src="/images/IDL4/scanMLP5.png" alt="" /></p>
      <h3 id="q-in-which-layer-would-you-expect-to-see-something-that-looks-like-a-flower-">
        
        
          <a href="#q-in-which-layer-would-you-expect-to-see-something-that-looks-like-a-flower-" class="anchor-heading" aria-labelledby="q-in-which-layer-would-you-expect-to-see-something-that-looks-like-a-flower-"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Q. In which layer would you expect to see something that looks like a flower? \
        
        
      </h3>
    
<p>Ans. Deeper Layers</p>

<p><img src="/images/IDL4/scanMLP6.png" alt="" />
<img src="/images/IDL4/scanMLP7.png" alt="" /></p>
      <h3 id="q-why-do-we-need-to-distribute-the-scanning-and-not-have-one-level-of-neurons-scan-the-entire-window-at-the-same-time">
        
        
          <a href="#q-why-do-we-need-to-distribute-the-scanning-and-not-have-one-level-of-neurons-scan-the-entire-window-at-the-same-time" class="anchor-heading" aria-labelledby="q-why-do-we-need-to-distribute-the-scanning-and-not-have-one-level-of-neurons-scan-the-entire-window-at-the-same-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Q. Why do we need to distribute the scanning and not have one level of neurons scan the entire window at the same time?
        
        
      </h3>
    
<p>Ans. It reduces the number of learnable parameters</p>

<p><img src="/images/IDL4/mlpSCAN1.png" alt="" /></p>

<p>However, to get a better understanding I strongly suggest going through the slides and video
links attached below to get a better understanding.</p>

<ul>
  <li>In the references below, understand what the K,N,D,L terms represent</li>
  <li>
    <p>If we consider a frequency spectrum of a voice recording, then considering some timestep
we get an input vector of size ‘LxD’ where L = length of recording used and D = height here
<img src="/images/IDL4/timestep.png" alt="" /></p>
  </li>
  <li>Now, let’s consider a case like this, where input vector of size L (here size 8) are 
feeding into layer1 which has N1 neurons (4 neurons in below picture)
<img src="/images/IDL4/input_vector.png" alt="" /></li>
  <li>We’ll use this in a more generic scanning case as shwon below
<img src="/images/IDL4/scanning1.png" alt="" />
    <ul>
      <li>From the above picture we calculate that each input vector has 8 timesteps (L)</li>
      <li>Each vector has a dimensionality D (think height of the frequency plot image)</li>
      <li>Therefore, number of weights connecting the input layer of size LD with 1st layer
of size N1 leads to the first term LDN1</li>
      <li>The next term is simply dependent on number of neurons in layer1 and layer2 = N1*N2</li>
    </ul>
  </li>
</ul>

<p>Now that we’ve seen the number of parameters in non-distributed scanning, let’s compare
it to an example of non-distributed scanning as shown below:</p>

<p><img src="/images/IDL4/scanning2.png" alt="" /></p>

<p>Clearly, the distributed scanning has more shared computations which gives it fewer parameters. We also have many identical weights as shown below:</p>

<p><img src="/images/IDL4/scanning3.png" alt="" /></p>
<ul>
  <li>In the above image, only the ones circled in greeen have unique parameters (weights)
and the remaining are shared (thereby saving computation)</li>
  <li><strong>We also have a notion where saved computation has more gains over having more weights</strong></li>
</ul>

<p>Now, if we think of the same logic in Image terms, it’s just changing the dimensions
of the vectors (say K changes to a 2D patch which gets flattened to K^2)</p>
      <h4 id="distributed-vs-undistributed-scanning-for-images">
        
        
          <a href="#distributed-vs-undistributed-scanning-for-images" class="anchor-heading" aria-labelledby="distributed-vs-undistributed-scanning-for-images"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Distributed vs Undistributed scanning for images
        
        
      </h4>
    

<p>Note. Sometimes there’s a (K + 1) term. That’s just for the bias I’m assuming.</p>

<p><img src="/images/IDL4/scanning4.png" alt="" />
<img src="/images/IDL4/scanning5.png" alt="" /></p>

<p>Finally, by doing distributed scanning we see the quantifiable effect as shown below:
<img src="/images/IDL4/scanning6.png" alt="" /></p>
      <h4 id="main-intuition">
        
        
          <a href="#main-intuition" class="anchor-heading" aria-labelledby="main-intuition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Main Intuition
        
        
      </h4>
    

<p><img src="/images/IDL4/scanMLP7.png" alt="" /></p>
      <h2 id="nice-take-on-max-pooling-why-is-it-needed">
        
        
          <a href="#nice-take-on-max-pooling-why-is-it-needed" class="anchor-heading" aria-labelledby="nice-take-on-max-pooling-why-is-it-needed"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Nice take on Max Pooling (Why is it needed?)
        
        
      </h2>
    

<p>When we scan the image, if we find that one pixel which should belong to a petal has been
shifted, we somehow have to account for it. A nice solution is to not be too local focused
as to where the activation occured, but to just take the max of a small window.</p>

<p>By taking the max of the small window say 4x4, we’re effectively not caring as to where the activation in that window occured, as long as it is within 4x4</p>
      <h3 id="nice-consequence-of-above-logic">
        
        
          <a href="#nice-consequence-of-above-logic" class="anchor-heading" aria-labelledby="nice-consequence-of-above-logic"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Nice consequence of above logic
        
        
      </h3>
    

<ul>
  <li>A little jitter in images can be expected due to irregularity of objects in the real world.</li>
  <li>However, in the speech world, jitter would mess up any phonetics that convey meaning</li>
  <li>As a result, in Speech recog there isn’t much max-pooling</li>
</ul>

<p>Note. The max pool occurs for each channel (unlike the conv filters which across all channels of the image). <strong>Therefore, the output of a maxpool will retain the number of
channels.</strong></p>
      <h1 id="convolutional-nns">
        
        
          <a href="#convolutional-nns" class="anchor-heading" aria-labelledby="convolutional-nns"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convolutional NNs
        
        
      </h1>
    
      <h2 id="number-of-parameters-in-a-conv-layer">
        
        
          <a href="#number-of-parameters-in-a-conv-layer" class="anchor-heading" aria-labelledby="number-of-parameters-in-a-conv-layer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Number of Parameters in a Conv Layer
        
        
      </h2>
    

<p><img src="/images/IDL4/conv_filters.png" alt="" /></p>
      <h2 id="types-of-filters">
        
        
          <a href="#types-of-filters" class="anchor-heading" aria-labelledby="types-of-filters"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Types of Filters
        
        
      </h2>
    

<ol>
  <li>Typically the first layer people have used large filter sizes of 5x5
(emperically proven to provide better results in feature extraction)</li>
  <li>Most lower levels have smaller filters of 3x3</li>
  <li>Now, there also exists a 3x3 filter. What is that?
<img src="/images/IDL4/conv_filter2.png" alt="" />
It’s just a single perceptron</li>
</ol>
      <h3 id="more-on-1x1-convolution">
        
        
          <a href="#more-on-1x1-convolution" class="anchor-heading" aria-labelledby="more-on-1x1-convolution"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> More on 1x1 Convolution
        
        
      </h3>
    

<ol>
  <li>Here too we find element-wise products</li>
  <li>Then as usual we apply a ReLU</li>
</ol>

<p>You can think of it as a <strong>single neuron</strong> layer which takes a vector input of 32
and has 32 weights which gets multiplied by the input. These then go through an
activation like ReLU as well.</p>

<p>It’s a fully connected network (single layer perceptron) which takes 32 vector input and outputs 1 number.</p>

<p>(Add DEVA’s content here too !!!)</p>
      <h1 id="importatnt-to-remember">
        
        
          <a href="#importatnt-to-remember" class="anchor-heading" aria-labelledby="importatnt-to-remember"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Importatnt to Remember
        
        
      </h1>
    

<ol>
  <li>Number of Parameters in Conv Layer <br />
<img src="/images/IDL4/conv_filter3.png" alt="" /></li>
  <li>While it may good to lose information during max-pooling, since it’s primarily
to account for jitter and noise and it’s okay to loose that information.
However, we also saw in the MLP decision boundaries case, that deeper layers
with complete information from previous layer, can learn complex shapes.
(Just imagine the MLP if it lost some information from the input layer
how our final learnt shape would be?)</li>
</ol>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

