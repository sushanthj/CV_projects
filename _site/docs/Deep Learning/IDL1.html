

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MLPs (IDL1) | Navigating Robotics</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="MLPs (IDL1)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" />
<meta property="og:url" content="http://localhost:4000/docs/Deep%20Learning/IDL1.html" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="MLPs (IDL1)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"MLPs (IDL1)","url":"http://localhost:4000/docs/Deep%20Learning/IDL1.html"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  Navigating Robotics

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D_reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/optical_flow/" class="nav-list-link">Optical Flow and Image Alignment</a></li><li class="nav-list-item"><a href="http://localhost:4000/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item"><a href="http://localhost:4000/spatial_pyramid_matching/" class="nav-list-link">Spatial Pyramid Matching for Scene Classification</a></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item  active"><a href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" class="nav-list-link active">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" class="nav-list-link">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/pytorch/" class="nav-list-link">Intro to Pytorch</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><a href="http://localhost:4000/camera_model/" class="nav-list-link">Camera Models</a></li><li class="nav-list-item"><a href="http://localhost:4000/numpy/" class="nav-list-link">Numpy</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision using C++</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
              
                <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/Deep%20Learning">Deep Learning</a></li>
              
              <li class="breadcrumb-nav-list-item"><span>MLPs (IDL1)</span></li>
            </ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary>
    Table of contents
  {: .text-delta }
  </summary>
<ol id="markdown-toc">
  <li><a href="#before-you-begin" id="markdown-toc-before-you-begin">Before you Begin</a></li>
  <li><a href="#multi-layer-perceptrons-basics" id="markdown-toc-multi-layer-perceptrons-basics">Multi-Layer Perceptrons Basics</a>    <ol>
      <li><a href="#perceptron-as-a-boolean-gate" id="markdown-toc-perceptron-as-a-boolean-gate">Perceptron as a boolean gate</a>        <ol>
          <li><a href="#recap-types-of-gates" id="markdown-toc-recap-types-of-gates">Recap types of gates:</a></li>
          <li><a href="#xor-gate" id="markdown-toc-xor-gate">XOR Gate</a></li>
        </ol>
      </li>
      <li><a href="#why-do-we-need-depth" id="markdown-toc-why-do-we-need-depth">Why do we need depth?</a></li>
      <li><a href="#perceptrons-as-linear-classifiers" id="markdown-toc-perceptrons-as-linear-classifiers">Perceptrons as Linear Classifiers</a>        <ol>
          <li><a href="#complex-decision-boundaries" id="markdown-toc-complex-decision-boundaries">Complex Decision Boundaries</a></li>
          <li><a href="#another-case-for-depth" id="markdown-toc-another-case-for-depth">Another case for depth</a></li>
        </ol>
      </li>
      <li><a href="#sufficiency-of-architecture" id="markdown-toc-sufficiency-of-architecture">Sufficiency of Architecture</a></li>
    </ol>
  </li>
  <li><a href="#further-on-mlps" id="markdown-toc-further-on-mlps">Further on MLPs</a>    <ol>
      <li><a href="#include-bias-as-an-input-for-simplifying-downstream-computations" id="markdown-toc-include-bias-as-an-input-for-simplifying-downstream-computations">Include bias as an input for simplifying downstream computations</a></li>
      <li><a href="#proceeding-from-simple-boolean-functions" id="markdown-toc-proceeding-from-simple-boolean-functions">Proceeding from simple boolean functions</a></li>
      <li><a href="#the-perceptron-algorithm" id="markdown-toc-the-perceptron-algorithm">The Perceptron algorithm</a></li>
      <li><a href="#why-is-the-perceptron-algorithm-not-good" id="markdown-toc-why-is-the-perceptron-algorithm-not-good">Why is the perceptron algorithm not good?</a>        <ol>
          <li><a href="#the-primary-issue-is-that-the-simple-perceptron-is-flat-and-non-differentiable" id="markdown-toc-the-primary-issue-is-that-the-simple-perceptron-is-flat-and-non-differentiable">The primary issue is that the simple perceptron is flat and non-differentiable.</a></li>
          <li><a href="#data-is-never-fully-clean" id="markdown-toc-data-is-never-fully-clean">Data is never fully clean</a></li>
          <li><a href="#the-solution-differentiable-activations" id="markdown-toc-the-solution-differentiable-activations">The solution: Differentiable activations</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><a href="#thinking-about-derivatives" id="markdown-toc-thinking-about-derivatives">Thinking about Derivatives</a></li>
</ol>

</details>
      <h2 id="before-you-begin">
        
        
          <a href="#before-you-begin" class="anchor-heading" aria-labelledby="before-you-begin"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Before you Begin
        
        
      </h2>
    

<p><a href="https://www.youtube.com/watch?v=tO3xU2t_wTY&amp;list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe&amp;index=11&amp;ab_channel=CarnegieMellonUniversityDeepLearning" class="btn fs-3 mb-4 mb-md-0">Ref: 11-785</a></p>
      <h1 id="multi-layer-perceptrons-basics">
        
        
          <a href="#multi-layer-perceptrons-basics" class="anchor-heading" aria-labelledby="multi-layer-perceptrons-basics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multi-Layer Perceptrons Basics
        
        
      </h1>
    

<p>These are machines that can model any function in the world! For now, let’s start with simple functions like boolean gates and build our way up.</p>

<p>The basic working is shown below:</p>

<p><img src="/images/IDL/MLP0.png" alt="" /></p>
      <h2 id="perceptron-as-a-boolean-gate">
        
        
          <a href="#perceptron-as-a-boolean-gate" class="anchor-heading" aria-labelledby="perceptron-as-a-boolean-gate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Perceptron as a boolean gate
        
        
      </h2>
    

<p><img src="/images/IDL/MLP1.png" alt="" /></p>

<ul>
  <li>Each perceptron seen above is a an addition gate</li>
  <li>The sum is computed, and the threshold value is given by the number inside the circle</li>
  <li>Therefore, the number dictates what type of gate it functions as</li>
</ul>
      <h3 id="recap-types-of-gates">
        
        
          <a href="#recap-types-of-gates" class="anchor-heading" aria-labelledby="recap-types-of-gates"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Recap types of gates:
        
        
      </h3>
    

<p><a href="https://cs231n.github.io/optimization-2/">Andrej Reference</a></p>

<ul>
  <li>Add gate</li>
  <li>Max gate</li>
  <li>Multiply gate</li>
</ul>
      <h3 id="xor-gate">
        
        
          <a href="#xor-gate" class="anchor-heading" aria-labelledby="xor-gate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> XOR Gate
        
        
      </h3>
    

<p>These gates are activated only if the inputs are (1,0) or (0,1). These are bit
tricky and need to be modelled with a network of perceptrons:</p>

<p><img src="/images/IDL/MLP3.png" alt="" /></p>

<p><strong>Therefore, it can be seen that combining MLPs in such a manner, one can 
say that MLPs are universal boolean functions</strong></p>

<p><strong>We can also claim that any boolean function can be modelled with just 1 hidden layer</strong></p>

<p>Reason:
<img src="/images/IDL/MLP4.png" alt="" /></p>
      <h2 id="why-do-we-need-depth">
        
        
          <a href="#why-do-we-need-depth" class="anchor-heading" aria-labelledby="why-do-we-need-depth"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why do we need depth?
        
        
      </h2>
    

<p>Let’s take a slightly difficult case (say an XOR)</p>

<p><img src="/images/IDL/MLP5.png" alt="" /></p>

<p><img src="/images/IDL/MLP6.png" alt="" /></p>

<p>However, if we model the same with XORs depthwise, we get:</p>

<p><img src="/images/IDL/MLP7.png" alt="" /></p>
      <h2 id="perceptrons-as-linear-classifiers">
        
        
          <a href="#perceptrons-as-linear-classifiers" class="anchor-heading" aria-labelledby="perceptrons-as-linear-classifiers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Perceptrons as Linear Classifiers
        
        
      </h2>
    

<p>If we have 2 boolean inputs, we can have 4 combinations:</p>
<ul>
  <li>(0,0)</li>
  <li>(0,1)</li>
  <li>(1,0)</li>
  <li>(1,1)</li>
</ul>

<p>Now, using an <strong>OR gate, NOT Y gate, XOR gate</strong> we can model some basic classifiers:</p>

<p><img src="/images/IDL/MLP8.png" alt="" /></p>

<p>Note. clearly the XOR needs to boundaries <strong>(we call these decision boundaries)</strong> <br />
Therefore, we say that the XOR cannot be modelled with just one perceptron</p>
      <h3 id="complex-decision-boundaries">
        
        
          <a href="#complex-decision-boundaries" class="anchor-heading" aria-labelledby="complex-decision-boundaries"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Complex Decision Boundaries
        
        
      </h3>
    

<p>If we create multiple decision boundaries, we can do the following:</p>

<ul>
  <li>Find output of each decision boundary (i.e. does my point lie to the left or right of decision boundary)</li>
  <li>The above step happens in the hidden layer</li>
  <li>Then we can cumulate these decision boundary inputs</li>
  <li>From below fig. notice that only if sum == 5, the final neuron fires</li>
</ul>

<p><img src="/images/IDL/MLP9.png" alt="" /></p>

<p>This way, we can model complex geometries, even complex ones like:</p>

<p><img src="/images/IDL/MLP10.png" alt="" /></p>
      <h3 id="another-case-for-depth">
        
        
          <a href="#another-case-for-depth" class="anchor-heading" aria-labelledby="another-case-for-depth"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Another case for depth
        
        
      </h3>
    

<p>Now, consider the above double pentagon figure. What if we were to do it
using just one layer?</p>

<p>We would have to approximate it using cylindrical regions (basically polygons with large number of sides, say 1000 sides)</p>

<p><img src="/images/IDL/MLP11.png" alt="" /></p>

<p>We can then use this cylinder decision boundary (multiples of them) to sort of make up
our double pentagon as shown below:</p>

<p><img src="/images/IDL/MLP12.png" alt="" /></p>

<p>But as seen above, <strong>the major drawback is that the first layer will have an infinite
number of neurons!</strong></p>

<p>Now, comparing our depthwise vs spanwise solutions:
<img src="/images/IDL/MLP13.png" alt="" /></p>
      <h2 id="sufficiency-of-architecture">
        
        
          <a href="#sufficiency-of-architecture" class="anchor-heading" aria-labelledby="sufficiency-of-architecture"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sufficiency of Architecture
        
        
      </h2>
    
<ul>
  <li>
    <p>A network arch is sufficient (i.e. sufficiently braod and sufficiently deep) it can
represent any function.</p>
  </li>
  <li>
    <p>Conversely if a network is not sufficient, it can miss out on information, and this
lack of information can be propagated deeper causing major loss of information</p>

    <p><img src="/images/IDL/MLP15.png" alt="" />
In the above image, if the red lines our the first layer, the information passed to the
second layer is that we are in those tiny diamond regions. However, we have no idea
where we are in those diamonds. <strong>(This is loss of info to the next layer!)</strong></p>

    <p>To mitigate this loss, instead of doing hard thresholding, we can use softer decision
boundaries as shown below:</p>
  </li>
</ul>

<p><img src="/images/IDL/MLP14.png" alt="" /></p>
      <h1 id="further-on-mlps">
        
        
          <a href="#further-on-mlps" class="anchor-heading" aria-labelledby="further-on-mlps"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Further on MLPs
        
        
      </h1>
    
      <h3 id="include-bias-as-an-input-for-simplifying-downstream-computations">
        
        
          <a href="#include-bias-as-an-input-for-simplifying-downstream-computations" class="anchor-heading" aria-labelledby="include-bias-as-an-input-for-simplifying-downstream-computations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Include bias as an input for simplifying downstream computations
        
        
      </h3>
    

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th style="text-align: left">Bias as a separate term</th>
      <th style="text-align: left">Bias included in input</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/images/IDL/MLP16.png" alt="" /></td>
      <td style="text-align: left"><img src="/images/IDL/MLP17.png" alt="" /></td>
    </tr>
  </tbody>
</table></div>

<p>This also helps in simplifying the (z = Wx + b) equation from being affine to a 
linear form of (z = Wx)</p>
      <h2 id="proceeding-from-simple-boolean-functions">
        
        
          <a href="#proceeding-from-simple-boolean-functions" class="anchor-heading" aria-labelledby="proceeding-from-simple-boolean-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proceeding from simple boolean functions
        
        
      </h2>
    

<ul>
  <li>We cannot handcraft our network like how we did for the double pentagon</li>
  <li>Therefore, we need a learnable method</li>
  <li>Also, most real functions are very complex and don’t have nice visualizations
like the <a href="#complex-decision-boundaries">double pentagon</a></li>
  <li>Therefore, we also need a way of learning such complex functions with only few samples
and not relying on continuous data</li>
  <li>We do this by a sampling approach, where we calculate the error for every sample in
our training data
<img src="/images/IDL/MLP18.png" alt="" /></li>
</ul>
      <h2 id="the-perceptron-algorithm">
        
        
          <a href="#the-perceptron-algorithm" class="anchor-heading" aria-labelledby="the-perceptron-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Perceptron algorithm
        
        
      </h2>
    

<p><img src="/images/IDL/MLP19.png" alt="" />
<img src="/images/IDL/MLP20.png" alt="" /></p>
      <h2 id="why-is-the-perceptron-algorithm-not-good">
        
        
          <a href="#why-is-the-perceptron-algorithm-not-good" class="anchor-heading" aria-labelledby="why-is-the-perceptron-algorithm-not-good"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why is the perceptron algorithm not good?
        
        
      </h2>
    
      <h3 id="the-primary-issue-is-that-the-simple-perceptron-is-flat-and-non-differentiable">
        
        
          <a href="#the-primary-issue-is-that-the-simple-perceptron-is-flat-and-non-differentiable" class="anchor-heading" aria-labelledby="the-primary-issue-is-that-the-simple-perceptron-is-flat-and-non-differentiable"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The primary issue is that the simple perceptron is flat and non-differentiable.
        
        
      </h3>
    

<p><img src="/images/IDL/MLP21.png" alt="" /></p>
      <h3 id="data-is-never-fully-clean">
        
        
          <a href="#data-is-never-fully-clean" class="anchor-heading" aria-labelledby="data-is-never-fully-clean"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Data is never fully clean
        
        
      </h3>
    

<p>We mostly never have nicely linearly separable data</p>

<p><img src="/images/IDL/MLP22.png" alt="" /></p>
      <h3 id="the-solution-differentiable-activations">
        
        
          <a href="#the-solution-differentiable-activations" class="anchor-heading" aria-labelledby="the-solution-differentiable-activations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The solution: Differentiable activations
        
        
      </h3>
    

<p><img src="/images/IDL/MLP23.png" alt="" /></p>

<p>Now, making this activation differentiable has two benefits:</p>
<ol>
  <li>Let’s us know if our changes is having a positive or negative effect on prediction</li>
  <li>It allows us to do <strong>backprop!</strong></li>
</ol>

<p><img src="/images/IDL/MLP24.png" alt="" /></p>
      <h1 id="thinking-about-derivatives">
        
        
          <a href="#thinking-about-derivatives" class="anchor-heading" aria-labelledby="thinking-about-derivatives"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Thinking about Derivatives
        
        
      </h1>
    

<ul>
  <li>
    <p>Instead of thinking of derivatives as <code class="language-plaintext highlighter-rouge">dy/dx</code> where if we have y and x as vectors, dividing
them would not make much sense, instead we define it as <code class="language-plaintext highlighter-rouge">y' = alpha*x'</code>, where alpha is
now a vector and alpha*x’ can be though of as a dot product. Therefore, this alpha
will now define the vector which when dot product with x gives the direction
of the fastest increase in y.
<img src="/images/IDL/MLP27.png" alt="" /></p>
  </li>
  <li>
    <p>Adavantage of doing it as <code class="language-plaintext highlighter-rouge">y' = alpha*x'</code> now is that for a multivariate form like above, 
we can write the alpha vector as a partial derivate of y with x.
<img src="/images/IDL/MLP28.png" alt="" /></p>
  </li>
  <li>
    <p>Now, we can clearly see how the gradient gives the direction of fastest increase in
in the function. Therefore, if we want to minimize, we go in the direction exactly
opposite to the gradient.
<img src="/images/IDL/MLP25.png" alt="" />
<img src="/images/IDL/MLP26.png" alt="" /></p>
  </li>
</ul>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

