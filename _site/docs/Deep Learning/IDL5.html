

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lessons Learnt 1 | Navigating Robotics</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Lessons Learnt 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" />
<meta property="og:url" content="http://localhost:4000/docs/Deep%20Learning/IDL5.html" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lessons Learnt 1" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"Lessons Learnt 1","url":"http://localhost:4000/docs/Deep%20Learning/IDL5.html"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  Navigating Robotics

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D Reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item"><a href="http://localhost:4000/ConvNext/" class="nav-list-link">ConvNext</a></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item  active"><a href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" class="nav-list-link active">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision_General" class="nav-list-link">Computer Vision</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/camera_model.html" class="nav-list-link">Camera Models and Projections</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Numpy.html" class="nav-list-link">Numpy for CV</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/NERF.html" class="nav-list-link">Volume Rendering and NERFs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/bag_of_words.html" class="nav-list-link">Spatial Pyramids and Bag of Words</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Optical%20Flow.html" class="nav-list-link">Optical Flow and Image Alignment</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision Libraries in C++</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
              
                <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/Deep%20Learning">Deep Learning</a></li>
              
              <li class="breadcrumb-nav-list-item"><span>Lessons Learnt 1</span></li>
            </ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">
        
          <details open="">
  <summary>
    Table of contents
  {: .text-delta }
  </summary>
<ol id="markdown-toc">
  <li><a href="#before-you-begin" id="markdown-toc-before-you-begin">Before you Begin</a></li>
  <li><a href="#cnn-architechtures-studied" id="markdown-toc-cnn-architechtures-studied">CNN Architechtures Studied</a>    <ol>
      <li><a href="#resnet" id="markdown-toc-resnet">RESNET</a></li>
    </ol>
  </li>
  <li><a href="#numbers-and-math" id="markdown-toc-numbers-and-math">Numbers and Math</a>    <ol>
      <li><a href="#shared-parameter-network-thinking-wise-neurons---filters" id="markdown-toc-shared-parameter-network-thinking-wise-neurons---filters">Shared Parameter Network (Thinking wise: Neurons -&gt; Filters)</a>        <ol>
          <li><a href="#implications" id="markdown-toc-implications">Implications</a></li>
        </ol>
      </li>
      <li><a href="#kernal-stride-padding-and-output-shape-calculation" id="markdown-toc-kernal-stride-padding-and-output-shape-calculation">Kernal, Stride, Padding and Output Shape Calculation</a></li>
    </ol>
  </li>
  <li><a href="#divergence-and-loss-functions" id="markdown-toc-divergence-and-loss-functions">Divergence and Loss Functions</a>    <ol>
      <li><a href="#l2-loss" id="markdown-toc-l2-loss">L2 Loss</a>        <ol>
          <li><a href="#intuition-behind-the-derivative" id="markdown-toc-intuition-behind-the-derivative">Intuition behind the derivative</a></li>
        </ol>
      </li>
      <li><a href="#kl-divergence-and-ce-loss" id="markdown-toc-kl-divergence-and-ce-loss">KL Divergence and CE Loss</a>        <ol>
          <li><a href="#intuition-on-loss-function" id="markdown-toc-intuition-on-loss-function">Intuition on Loss Function</a></li>
          <li><a href="#intuition-behind-derivative" id="markdown-toc-intuition-behind-derivative">Intuition behind Derivative</a></li>
        </ol>
      </li>
      <li><a href="#why-kl-over-l2" id="markdown-toc-why-kl-over-l2">Why KL over L2?</a></li>
    </ol>
  </li>
  <li><a href="#activation-functions" id="markdown-toc-activation-functions">Activation Functions</a>    <ol>
      <li><a href="#what-is-inductive-bias-how-does-relu-have-it" id="markdown-toc-what-is-inductive-bias-how-does-relu-have-it">What is Inductive Bias? How does ReLU have it?</a>        <ol>
          <li><a href="#when-is-it-bad-to-have-inductive-bias" id="markdown-toc-when-is-it-bad-to-have-inductive-bias">When is it bad to have inductive bias?</a></li>
          <li><a href="#when-is-it-good-to-have-this-inductive-bias" id="markdown-toc-when-is-it-good-to-have-this-inductive-bias">When is it good to have this inductive bias?</a></li>
          <li><a href="#what-is-the-probabalistic-interpretation-of-sigmoid" id="markdown-toc-what-is-the-probabalistic-interpretation-of-sigmoid">What is the probabalistic interpretation of Sigmoid?</a></li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

</details>
      <h2 id="before-you-begin">
        
        
          <a href="#before-you-begin" class="anchor-heading" aria-labelledby="before-you-begin"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Before you Begin
        
        
      </h2>
    

<p><a href="https://mediaservices.cmu.edu/media/Deep+Learning+%28Fall+2021%29+HW1P1P2_Bootcamp+2/1_m6nkdrxy" class="btn fs-3 mb-4 mb-md-0">Ref: 11-785</a></p>

<p>This course has become quite intense to try and document. I’ll be posting highlights here
which are my biggest takeaways</p>
      <h1 id="cnn-architechtures-studied">
        
        
          <a href="#cnn-architechtures-studied" class="anchor-heading" aria-labelledby="cnn-architechtures-studied"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> CNN Architechtures Studied
        
        
      </h1>
    
      <h2 id="resnet">
        
        
          <a href="#resnet" class="anchor-heading" aria-labelledby="resnet"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> RESNET
        
        
      </h2>
    

<p>Convolutional Neural Nets are really good at extracting features.</p>

<p><img src="/images/resnet/resnet1.png" alt="" /></p>

<p>My understanding on why we use RESNETs:</p>
<ol>
  <li>It prevents overfitting and limits the non-linearity to the necessary amount
by allowing for gradients to skip neurons on the backward pass</li>
  <li>It solves the issue of vanishing gradients</li>
</ol>

<p>A simple network would have following form:
<img src="/images/resnet/resnet2.png" alt="" /></p>

<p>Now, if we add a skip connection, we get the following structure:
<img src="/images/resnet/resnet3.png" alt="" /></p>

<p>Note. The new activation is g(z + a)</p>
      <h1 id="numbers-and-math">
        
        
          <a href="#numbers-and-math" class="anchor-heading" aria-labelledby="numbers-and-math"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Numbers and Math
        
        
      </h1>
    
      <h2 id="shared-parameter-network-thinking-wise-neurons---filters">
        
        
          <a href="#shared-parameter-network-thinking-wise-neurons---filters" class="anchor-heading" aria-labelledby="shared-parameter-network-thinking-wise-neurons---filters"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Shared Parameter Network (Thinking wise: Neurons -&gt; Filters)
        
        
      </h2>
    

<p>We previously saw that the number of params of a CNN were much lesser than an MLP. A better
understand of why this would be from the below diagram</p>

<p><img src="/images/IDL5/scanning_mlp_to_cnn.png" alt="" /></p>

<ul>
  <li>The above shows an MLP scanning 1D data. now, the MLP in the raw form above can be seen
taking <strong>8 inputs at time</strong>.</li>
  <li>Additionally, each layer of the MLP is seen to have neurons which do the same work. Therefore,
such neurons can be set to hold the same <strong>shared parameters (same color)</strong></li>
</ul>

<p>If we think of only the most minimalist structure of the MLP which is required (discarding
any duplicates nuerons), we can make it a scanning MLP of the below structure:</p>

<p><img src="/images/IDL5/mlp_to_cnn.jpg" alt="" /></p>

<p>From the above image the equivalent CNN will have the following structure:</p>

<p><strong>The first hidden layer has 4 filters of kernel-width 2 and stride 2; the second layer has 3 filters of
kernel-width 2 and stride 2; the third layer has 2 filters of kernel-width 2 and stride 2</strong></p>

<p>Also, such a CNN which is moving over 1D input (mostly time) is called a Time Delay Neural
Network <strong>(TDNN)</strong></p>
      <h3 id="implications">
        
        
          <a href="#implications" class="anchor-heading" aria-labelledby="implications"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implications
        
        
      </h3>
    

<ul>
  <li>Lower nubmer of params</li>
  <li>
    <p>Due to shared params, if 4 filters in the lower layer feeds 1 filter above, then the
gradient at the higher filter will be equally rerouted (not split!) amongst the 4 lower layer
fiters. See the below Andrej’s explanation for a backprop refresher:</p>

    <p><img src="/images/IDL5/backprop_refresher.png" alt="" /></p>
  </li>
</ul>
      <h2 id="kernal-stride-padding-and-output-shape-calculation">
        
        
          <a href="#kernal-stride-padding-and-output-shape-calculation" class="anchor-heading" aria-labelledby="kernal-stride-padding-and-output-shape-calculation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Kernal, Stride, Padding and Output Shape Calculation
        
        
      </h2>
    

<p>The pytorch website has a bad looking equation, I prefer the simple one below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">padding</span> <span class="o">-</span> <span class="n">kernel</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>
      <h1 id="divergence-and-loss-functions">
        
        
          <a href="#divergence-and-loss-functions" class="anchor-heading" aria-labelledby="divergence-and-loss-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Divergence and Loss Functions
        
        
      </h1>
    
      <h2 id="l2-loss">
        
        
          <a href="#l2-loss" class="anchor-heading" aria-labelledby="l2-loss"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> L2 Loss
        
        
      </h2>
    

<p><img src="/images/DL_Lessons/L2.png" alt="" /></p>

<ul>
  <li>The 1/2 as you can see is a scaling factor which makes derivative clean</li>
</ul>
      <h3 id="intuition-behind-the-derivative">
        
        
          <a href="#intuition-behind-the-derivative" class="anchor-heading" aria-labelledby="intuition-behind-the-derivative"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intuition behind the derivative
        
        
      </h3>
    

<ul>
  <li>d = desired value / label = 1</li>
  <li>y = network output = 1.5</li>
  <li>In the above case, increasing y will increase the loss</li>
  <li>Hence, the derivative (think of it like slope here) will be positive (i.e rise/run is positive)</li>
  <li>The magnitude of this derivative will also be dependent on error (y - d)</li>
  <li>Hence, the derivative formula above = (y - d) and will be positive when y &gt; d</li>
</ul>

<p>To do gradient descent, we will therefore go in the negative direction of this derivative
by doing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_value_of_weights</span> <span class="o">=</span> <span class="n">current_value_of_weights</span> <span class="o">-</span> <span class="p">(</span><span class="n">derivative_of_divergence</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">)</span>
</code></pre></div></div>
      <h2 id="kl-divergence-and-ce-loss">
        
        
          <a href="#kl-divergence-and-ce-loss" class="anchor-heading" aria-labelledby="kl-divergence-and-ce-loss"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> KL Divergence and CE Loss
        
        
      </h2>
    

<p><img src="/images/DL_Lessons/KL.png" alt="" /></p>

<p>Try to think about why we have the log term in this divergence?</p>
      <h3 id="intuition-on-loss-function">
        
        
          <a href="#intuition-on-loss-function" class="anchor-heading" aria-labelledby="intuition-on-loss-function"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intuition on Loss Function
        
        
      </h3>
    

<ul>
  <li>In binary classification, we can either be <strong>fully correct</strong> (d=0 and y=0) or <strong>fully wrong</strong>(d=0 and y=1)</li>
  <li>Now, ideally we don’t want to do anything when we are fully correct</li>
  <li><strong><em>‘don’t want to do anything’</em></strong> is just english for make the penalty zero</li>
  <li>When we are fully wrong, we want a harsh penalty on the network and ask it to correct itself</li>
  <li>
    <p><strong><em>‘harsh penalty’</em></strong> is english for make the penalty infinite</p>
  </li>
  <li>Let’s plug in the values and see if the above intuition is applied. <em>Let d = 1 and Y = 0</em></li>
  <li>Plugging in values, <code class="language-plaintext highlighter-rouge">Div = - 1 * log(0)</code>, and <code class="language-plaintext highlighter-rouge">log(0) = - infinity</code>, therefore <code class="language-plaintext highlighter-rouge">Div = + infinity</code></li>
  <li>In the above line, note that because log(0) is negative infinity, that’s why we multiply by -1 in the begging</li>
</ul>
      <h3 id="intuition-behind-derivative">
        
        
          <a href="#intuition-behind-derivative" class="anchor-heading" aria-labelledby="intuition-behind-derivative"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intuition behind Derivative
        
        
      </h3>
    

<ul>
  <li>d = desired, y = actual output</li>
  <li>If <strong>d = 1</strong> and <strong>y = 0.9</strong>, then increasing y will decrease the loss</li>
  <li>As you can see above, since increase in y will make loss go lower, the slope would be negative, i.e. derivative will be negative</li>
  <li>
    <p>The above intuition is verified from the equation <img src="/images/DL_Lessons/div_case_1.png" alt="" /></p>
  </li>
  <li>Now if <strong>d = 0</strong> and <strong>y = 0.9</strong>, then increasing y will increase loss. Hence, derivative is positive</li>
</ul>
      <h2 id="why-kl-over-l2">
        
        
          <a href="#why-kl-over-l2" class="anchor-heading" aria-labelledby="why-kl-over-l2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why KL over L2?
        
        
      </h2>
    
      <h1 id="activation-functions">
        
        
          <a href="#activation-functions" class="anchor-heading" aria-labelledby="activation-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Activation Functions
        
        
      </h1>
    

<p><img src="/images/DL_Lessons/kl_vs_l2.png" alt="" /></p>
      <h2 id="what-is-inductive-bias-how-does-relu-have-it">
        
        
          <a href="#what-is-inductive-bias-how-does-relu-have-it" class="anchor-heading" aria-labelledby="what-is-inductive-bias-how-does-relu-have-it"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What is Inductive Bias? How does ReLU have it?
        
        
      </h2>
    

<p>Inductive bias is the inherent, baked-in, bias that some activation functions like ReLU has
which has started becoming evident. This is what gave rise to GELU as mentioned in
the <a href="https://arxiv.org/pdf/2201.03545.pdf">ConvNext Paper</a> and why it’s gained prominance
in vision transformers and modern CNNs.</p>
      <h3 id="when-is-it-bad-to-have-inductive-bias">
        
        
          <a href="#when-is-it-bad-to-have-inductive-bias" class="anchor-heading" aria-labelledby="when-is-it-bad-to-have-inductive-bias"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> When is it bad to have inductive bias?
        
        
      </h3>
    

<ul>
  <li>The inductive bias of ReLU is that it prioritizes smooth linear boundaries which is useful
when working with very high dimensional data (as it prevents overfitting I assume).</li>
  <li>However, if we work on low-dimensional models, which can be simple cases like trying to
reconstruct a singular image, or even super useful networks like NERFs</li>
  <li>In this <a href="https://www.youtube.com/watch?v=JOVPWLBIo5Q&amp;t=1641s&amp;ab_channel=CMURoboticsInstitute">RI Seminar by Prof. Simon Lucey</a>
he talks about the case where a simple network with ReLU activations is used in an encoder-decoder
like fashion to reconstruct <strong>one image</strong>.
    <ul>
      <li><img src="/images/DL_Lessons/inductive_bias.gif" alt="" /></li>
    </ul>
  </li>
  <li>The above seems to fail and the reasoning is stated to be the inductive bias of ReLU</li>
</ul>

<p>Here’s an example of what happens when we remove the inductive bias by replacing ReLU with
another activation function (Gaussians here) which are said to have higher bandwidth (i.e. can
represent higher number of frequencies (think of a neural net as giving a signal as an output))</p>

<p><img src="/images/DL_Lessons/without_inductive_bias.png" alt="" /></p>
      <h3 id="when-is-it-good-to-have-this-inductive-bias">
        
        
          <a href="#when-is-it-good-to-have-this-inductive-bias" class="anchor-heading" aria-labelledby="when-is-it-good-to-have-this-inductive-bias"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> When is it good to have this inductive bias?
        
        
      </h3>
    

<p>For large networks like vision transformers which are trained on many classes, and a ton of
data which should not be overfit, in those cases the inductive bias of ReLU is okay</p>

<p><strong>Bottom Line:
Do not use ReLU in combination with any form of position encoding (like in NERFs)</strong></p>
      <h3 id="what-is-the-probabalistic-interpretation-of-sigmoid">
        
        
          <a href="#what-is-the-probabalistic-interpretation-of-sigmoid" class="anchor-heading" aria-labelledby="what-is-the-probabalistic-interpretation-of-sigmoid"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What is the probabalistic interpretation of Sigmoid?
        
        
      </h3>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

