

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  

  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0VNMB6VPJF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-0VNMB6VPJF', { 'anonymize_ip': true });
    </script>

  

  
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Volume Rendering and NERFs | Navigating Robotics</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Volume Rendering and NERFs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Projects and assignments during my time in CMU" />
<meta property="og:description" content="Projects and assignments during my time in CMU" />
<link rel="canonical" href="http://localhost:4000/docs/Computer%20Vision/NERF.html" />
<meta property="og:url" content="http://localhost:4000/docs/Computer%20Vision/NERF.html" />
<meta property="og:site_name" content="Navigating Robotics" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Volume Rendering and NERFs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Projects and assignments during my time in CMU","headline":"Volume Rendering and NERFs","url":"http://localhost:4000/docs/Computer%20Vision/NERF.html"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>

  <div class="side-bar">
    <div class="site-header">
      <a href="http://localhost:4000/" class="site-title lh-tight">
  Navigating Robotics

</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      
        <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="http://localhost:4000/intro/" class="nav-list-link">Building this Page</a></li><li class="nav-list-item"><a href="http://localhost:4000/planar_homography/" class="nav-list-link">Planar Homography</a></li><li class="nav-list-item"><a href="http://localhost:4000/3D_reconstruction/" class="nav-list-link">3D Reconstruction</a></li><li class="nav-list-item"><a href="http://localhost:4000/constr_rrt/" class="nav-list-link">Constrained RRT</a></li><li class="nav-list-item"><a href="http://localhost:4000/ConvNext/" class="nav-list-link">ConvNext</a></li><li class="nav-list-item"><a href="http://localhost:4000/mrsd_proj/" class="nav-list-link">MRSD Capstone Project</a></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Deep%20Learning" class="nav-list-link">Deep Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/Basics.html" class="nav-list-link">ML Basics</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/DL.html" class="nav-list-link">Deep Learning Starter</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL1.html" class="nav-list-link">MLPs (IDL1)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL2.html" class="nav-list-link">Classifiers (IDL2)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL3.html" class="nav-list-link">Optimizers and Regularizers (IDL3)</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL4.html" class="nav-list-link">Intro to CNNs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Deep%20Learning/IDL5.html" class="nav-list-link">Lessons Learnt 1</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/SLAM" class="nav-list-link">SLAM</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Probability_review.html" class="nav-list-link">Recap on Probability</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Expectation_and_cov.html" class="nav-list-link">Expectation and Covariance</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Particle%20Filter_theory.html" class="nav-list-link">Particle Filters Theory</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/EKF.html" class="nav-list-link">EKF</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/SLAM/Non_linear_slam.html" class="nav-list-link">Least Squares SLAM</a></li></ul></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision_General" class="nav-list-link">Computer Vision</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/camera_model.html" class="nav-list-link">Camera Models and Projections</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Numpy.html" class="nav-list-link">Numpy for CV</a></li><li class="nav-list-item  active"><a href="http://localhost:4000/docs/Computer%20Vision/NERF.html" class="nav-list-link active">Volume Rendering and NERFs</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/bag_of_words.html" class="nav-list-link">Spatial Pyramids and Bag of Words</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Computer%20Vision/Optical%20Flow.html" class="nav-list-link">Optical Flow and Image Alignment</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/docs/Vision%20with%20C++" class="nav-list-link">Computer Vision Libraries in C++</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen.html" class="nav-list-link">Linear Algebra in Eigen</a></li><li class="nav-list-item "><a href="http://localhost:4000/docs/Vision%20with%20C++/Eigen_applied.html" class="nav-list-link">Eigen, OpenCV, and Images</a></li></ul></li></ul>

      
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
      
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigating Robotics" aria-label="Search Navigating Robotics" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
      
      
      
        <nav aria-label="Auxiliary" class="aux-nav">
          <ul class="aux-nav-list">
            
              <li class="aux-nav-list-item">
                <a href="//github.com/sushanthj" class="site-button"
                  
                >
                  Sushanth Jayanth's github
                </a>
              </li>
            
          </ul>
        </nav>
      
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
              
                <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/docs/Vision_General">Computer Vision</a></li>
              
              <li class="breadcrumb-nav-list-item"><span>Volume Rendering and NERFs</span></li>
            </ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">
        
          <h1 id="introduction">
        
        
          <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction
        
        
      </h1>
    

<p>NERF gave us a whole new way of approaching general computer vision tasks like
Novel View Synthesis (NVS), 3D Reconstruction, etc. in a more physics informed way.</p>

<p>The NeRF is basically an MLP that learns the 3D density and 5D light field of a given scene from image
observations and corresponding perspective transforms.</p>

<p>Q. What’s 5D Light Field?</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ans. Choosing any ray (i.e. choosing any starting origin and direction) in the scene,
     we should be able to find the total radiance coming from that ray.
</code></pre></div></div>

<p>Q. What is Radiance?</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ans. Measure of Radiant Energy (joules) per unit time, per unit area, per unit solid angle
     AKA: The amount of irradiance per unit solid angle
</code></pre></div></div>

<p>Since the formulation of rendering is crucial to understanding NERFs we’ll do that first.</p>
      <h1 id="part-1--volume-rendering">
        
        
          <a href="#part-1--volume-rendering" class="anchor-heading" aria-labelledby="part-1--volume-rendering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 1 : Volume Rendering
        
        
      </h1>
    
      <h2 id="understanding-absorption-and-trasmittance">
        
        
          <a href="#understanding-absorption-and-trasmittance" class="anchor-heading" aria-labelledby="understanding-absorption-and-trasmittance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding Absorption and Trasmittance
        
        
      </h2>
    
      <h3 id="absorption-emission-model">
        
        
          <a href="#absorption-emission-model" class="anchor-heading" aria-labelledby="absorption-emission-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Absorption Emission Model
        
        
      </h3>
    

<p>Let’s consider an infinitesimal (small) volume through which we have a light ray travelling.
Now, this volume can do two things:</p>

<ul>
  <li>Absorb some intensity of incoming light</li>
  <li>Emit some light of it’s own</li>
</ul>

<p><img src="/images/Computer_Vision/NERFs/emission_absorption.png" alt="" /></p>

<p>For both cases, we see the factor <code class="language-plaintext highlighter-rouge">σ</code> this is the absorption coefficient of the volume.
Furthermore, we see that <strong>both incoming light and light produced in this volume will be
affected by this <code class="language-plaintext highlighter-rouge">σ</code></strong></p>
      <h3 id="absorption-only-model">
        
        
          <a href="#absorption-only-model" class="anchor-heading" aria-labelledby="absorption-only-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Absorption-only Model
        
        
      </h3>
    

<p>Modelling only absoroption through a non-homogenous volume, <strong>we derive the relationship
between incoming radiation and outgoing radiation</strong> as follows:</p>

<p><img src="/images/Computer_Vision/NERFs/transmittance_derivation.png" alt="" /></p>

<p>Note in above figure, <code class="language-plaintext highlighter-rouge">x0</code> is where the light ray enters the volume. We assume the volume
is perfectly oriented in the ray’s direction <code class="language-plaintext highlighter-rouge">ω</code>. Then <code class="language-plaintext highlighter-rouge">ωz</code> would be the length of the
volume along the <code class="language-plaintext highlighter-rouge">ω</code> unit vector. That’s why the final radiance output is <strong><code class="language-plaintext highlighter-rouge">L(x0 + ωz, ω)</code></strong></p>

<p>As you can observe, we have a new term here called <strong>Transmittance</strong>. <strong>The intuitive meaning
of transmittance is the proportion of incoming light that eventually leaves the volume (gets
transmitted).</strong></p>

<p>Think of it like absorption is a coefficient (say 0.2) meaning that 20% of all incoming light
is absorbed. For transmittance (say 0.8) the intuition would be that 80% of all light is let
through the medium.</p>
      <h3 id="why-the-importance-on-transmittance">
        
        
          <a href="#why-the-importance-on-transmittance" class="anchor-heading" aria-labelledby="why-the-importance-on-transmittance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why The Importance on Transmittance
        
        
      </h3>
    

<p>Trasmittance has some nice properties which simple absorption would not have. Specifically:</p>

<ul>
  <li>Monotonic Function</li>
  <li>Multiplicativity</li>
</ul>

<p>In the below picture, we see that even though <code class="language-plaintext highlighter-rouge">σ</code> might vary in the volume, the transmittance
is always a monotonic function:</p>

<p><img src="/images/Computer_Vision/NERFs/monotonic_transmittance.png" alt="" /></p>

<p>Now, previously we saw Transmittance for a non-homogenous medium. It can be easily adapted
to a homogenous medium as well as shown below:</p>

<p><img src="/images/Computer_Vision/NERFs/Homogenous_vs_non_homo.png" alt="" /></p>

<p>Now, above we see that it’s basically an exponential. The <strong>multiplicativity property of
transmittance is due to the this multiplicativity of exponentials</strong></p>

<p><img src="/images/Computer_Vision/NERFs/multiplicativity_transmittance.png" alt="" /></p>

<p>Using our transmittance terminology above, we finnaly get for <strong>absorption only transmittance</strong>:</p>

<p><img src="/images/Computer_Vision/NERFs/updated_radiance_eq.png" alt="" /></p>
      <h3 id="emission-absorption-transmittance">
        
        
          <a href="#emission-absorption-transmittance" class="anchor-heading" aria-labelledby="emission-absorption-transmittance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Emission-Absorption Transmittance
        
        
      </h3>
    

<p>As a recap of what was done above, let’s see the basic absorption model</p>

<p><img src="/images/Computer_Vision/NERFs/transmittance_in_vacuum.png" alt="" /></p>

<p>In the above picture note the following:</p>
<ul>
  <li>The transmittance in vacuum is 1</li>
  <li>Only the cloud has some transmittance value less than 1</li>
  <li>Therefore <code class="language-plaintext highlighter-rouge">T(x,x_z) = T_cloud</code> in the above scenario</li>
</ul>

<p><strong>Now, lets make some assumptions to go from absorption only model to absorption-emission model:</strong></p>
<ul>
  <li>Let’s divide the cloud into small sections (small volumes)</li>
  <li>Let each volume not only absorb (have transmittance &lt; 1) but also be able to emit light</li>
  <li>The final radiation at the eye will be a combination of emission and absorption</li>
</ul>

<p>The context above is baked into the picture below:</p>

<p><img src="/images/Computer_Vision/NERFs/emission_absorption_vol_rendering.png" alt="" /></p>
      <h3 id="ray-marching">
        
        
          <a href="#ray-marching" class="anchor-heading" aria-labelledby="ray-marching"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Ray Marching
        
        
      </h3>
    

<p>Now, the issue with the emission-absorption model is that the integral cannot be solved numerically
without some simplifications. We will make the following simplifications:</p>

<ul>
  <li>Discretize the space into small volumes</li>
  <li>Let each small volume have it’s own <code class="language-plaintext highlighter-rouge">σ</code></li>
  <li>Our final radiation at the eye/camera will be the summation of each of these small volumes</li>
</ul>

<p><img src="/images/Computer_Vision/NERFs/ray_marching_1.png" alt="" /></p>

<p><img src="/images/Computer_Vision/NERFs/ray_marching_2.png" alt="" /></p>

<p><img src="/images/Computer_Vision/NERFs/ray_marching_3.png" alt="" /></p>

<p><img src="/images/Computer_Vision/NERFs/ray_marching_final.png" alt="" /></p>

<p>Finally, we see that computing Transmittance is recursive, where the i+1’th segment’s
transmittance <code class="language-plaintext highlighter-rouge">(T_i+1) = T(i) * T(small volume of i+1)</code></p>
      <h3 id="practice-transmittance-calculations">
        
        
          <a href="#practice-transmittance-calculations" class="anchor-heading" aria-labelledby="practice-transmittance-calculations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practice Transmittance Calculations
        
        
      </h3>
    

<p><img src="/images/Computer_Vision/NERFs/transmittance_question.png" alt="" /></p>

<p><img src="/images/Computer_Vision/NERFs/transmittance_solution.png" alt="" /></p>
      <h3 id="implementing-ray-marching">
        
        
          <a href="#implementing-ray-marching" class="anchor-heading" aria-labelledby="implementing-ray-marching"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing Ray Marching
        
        
      </h3>
    

<p>This will be done in two steps:</p>
      <h4 id="1-discretize-the-space-into-small-volumes-just-define-the-sampling-points">
        
        
          <a href="#1-discretize-the-space-into-small-volumes-just-define-the-sampling-points" class="anchor-heading" aria-labelledby="1-discretize-the-space-into-small-volumes-just-define-the-sampling-points"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Discretize the space into small volumes (just define the sampling points)
        
        
      </h4>
    

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">ray_utils</span> <span class="kn">import</span> <span class="n">RayBundle</span>
<span class="kn">from</span> <span class="nn">pytorch3d.renderer.cameras</span> <span class="kn">import</span> <span class="n">CamerasBase</span>


<span class="c1"># Sampler which implements stratified (uniform) point sampling along rays
</span><span class="k">class</span> <span class="nc">StratifiedRaysampler</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cfg</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">n_pts_per_ray</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">n_pts_per_ray</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_depth</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">min_depth</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">max_depth</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ray_bundle</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Compute z values for self.n_pts_per_ray points uniformly sampled between [near, far]
</span>        <span class="n">z_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">min_depth</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_pts_per_ray</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">ray_bundle</span><span class="p">.</span><span class="n">origins</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Sample points from z values
</span>        <span class="s">"""
        NOTE: if image_plane_points.shape = torch.Size([65536, 3]),
              then rays_origin.shape = torch.Size([65536, 3])
              and sample_lenths.shape = torch.Size([65536, 1, 3])
        """</span>

        <span class="n">origins_expanded</span> <span class="o">=</span> <span class="n">ray_bundle</span><span class="p">.</span><span class="n">origins</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (N, 1, 3)
</span>        <span class="n">origins_expanded</span> <span class="o">=</span> <span class="n">origins_expanded</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_pts_per_ray</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (N, D, 3)
</span>        <span class="n">directions_expanded</span> <span class="o">=</span> <span class="n">ray_bundle</span><span class="p">.</span><span class="n">directions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (N, 1, 3)
</span>        <span class="n">directions_expanded</span> <span class="o">=</span> <span class="n">directions_expanded</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_pts_per_ray</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (N, D, 3)
</span>        <span class="n">z_vals_expanded</span> <span class="o">=</span> <span class="n">z_vals</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="n">ray_bundle</span><span class="p">.</span><span class="n">origins</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (1, D, 1)
</span>
        <span class="n">new_sample_points</span> <span class="o">=</span> <span class="n">origins_expanded</span> <span class="o">+</span> <span class="n">z_vals_expanded</span> <span class="o">*</span> <span class="n">directions_expanded</span>

        <span class="c1"># Return
</span>        <span class="k">return</span> <span class="n">ray_bundle</span><span class="p">.</span><span class="n">_replace</span><span class="p">(</span>
            <span class="n">sample_points</span><span class="o">=</span><span class="n">new_sample_points</span><span class="p">,</span>
            <span class="n">sample_lengths</span><span class="o">=</span><span class="n">z_vals_expanded</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">new_sample_points</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="c1"># shape = (N, D, 1)
</span>        <span class="p">)</span>
</code></pre></div></div>
      <h4 id="2-get-the-density-and-color-of-each-small-volume-this-step-is-done-by-the-nerf-mlp">
        
        
          <a href="#2-get-the-density-and-color-of-each-small-volume-this-step-is-done-by-the-nerf-mlp" class="anchor-heading" aria-labelledby="2-get-the-density-and-color-of-each-small-volume-this-step-is-done-by-the-nerf-mlp"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Get the density and color of each small volume (This step is Done by the NERF MLP.)
        
        
      </h4>
    
      <h4 id="3-aggregate-the-density-and-color-of-each-small-volume-to-get-the-final-color-at-the-origin-of-each-ray">
        
        
          <a href="#3-aggregate-the-density-and-color-of-each-small-volume-to-get-the-final-color-at-the-origin-of-each-ray" class="anchor-heading" aria-labelledby="3-aggregate-the-density-and-color-of-each-small-volume-to-get-the-final-color-at-the-origin-of-each-ray"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Aggregate the density and color of each small volume to get the final color at the origin of each ray
        
        
      </h4>
    
      <h4 id="note">
        
        
          <a href="#note" class="anchor-heading" aria-labelledby="note"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> NOTE:
        
        
      </h4>
    
<p>During training, density is not explicitly trained for. Instead, we check what the final color
of a ray should be and compare with what the NERF MLP is telling us. This comparison gives us our
loss which will update ray marching.</p>

<p>Because we don’t optimize for density directly, that’s why the NeRF Depth output is bad. That’s
what prompted people to move to Neural SDFs so that geometry would be better optimized.</p>
      <h1 id="part-2--nerf-pipeline">
        
        
          <a href="#part-2--nerf-pipeline" class="anchor-heading" aria-labelledby="part-2--nerf-pipeline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2 : NERF Pipeline
        
        
      </h1>
    
      <h2 id="overview">
        
        
          <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Overview:
        
        
      </h2>
    

<ul>
  <li>Define a set of rays (origin, direction):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="n">RayBundle</span>
<span class="p">(</span>
  <span class="n">rays_origin</span><span class="p">,</span>
  <span class="n">rays_d</span><span class="p">,</span>
  <span class="n">sample_lengths</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rays_origin</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="n">sample_points</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rays_origin</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Give rays (origin, direction) to StratifiedRaysampler to get the sample points along the rays
    <ul>
      <li>i.e. Do the discretization step of Ray Marching
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sample points along the ray
</span><span class="n">cur_ray_bundle</span> <span class="o">=</span> <span class="n">StratifiedSampler</span><span class="p">(</span><span class="n">cur_ray_bundle</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Pass the sample points to the NERF MLP to get the density and color of each sample point
    <ul>
      <li>Note: These sample points are anywhere along the rays (think of it like random camera
positions and randomly close to the object) and we want to predict the density and color
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="n">NeRF_MLP</span><span class="p">(</span><span class="n">cur_ray_bundle</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p><strong>The next two steps are a bit involved</strong></p>
<ul>
  <li>Aggregate the density and color of each sample point to get the final color at the origin
of each ray. (the final color of all rays forms the IMAGE!)</li>
  <li>Compare the final colors (Image) with the GT image to get the loss</li>
</ul>

<p>Check out the forward function in the Volume Renderer class below reference:</p>

<p class="btn fs-5 mb-4 mb-md-0"><a href="https://github.com/sushanthj/L3D/blob/main/HW3/renderer.py">Reference</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predicted_density_for_all_samples_for_all_rays_in_chunk</span> <span class="o">=</span> <span class="n">NERF_MLP_output</span><span class="p">[</span><span class="s">'density'</span><span class="p">]</span> <span class="c1"># shape = (self._chunk_size*n_pts, 1) : The density value of that discrete volume
</span><span class="n">predicted_colors_for_all_samples_for_all_rays_in_chunk</span> <span class="o">=</span> <span class="n">NERF_MLP_output</span><span class="p">[</span><span class="s">'feature'</span><span class="p">]</span> <span class="c1"># shape = (self._chunk_size*n_pts, 3) : Emittance for each discrete volume for RGB channels
</span>
<span class="c1"># Compute length of each ray segment
# NOTE: cur_ray_bundle.sample_lengths.shape = (self._chunk_size, n_pts, n_pts)
</span><span class="n">depth_values</span> <span class="o">=</span> <span class="n">cur_ray_bundle</span><span class="p">.</span><span class="n">sample_lengths</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># depth_values.shape = (self._chunk_size, n_pts)
# deltas are the distance between each sample
</span><span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="n">depth_values</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">depth_values</span><span class="p">[...,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="mf">1e10</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">depth_values</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span>
    <span class="p">),</span>
    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)[...,</span> <span class="bp">None</span><span class="p">]</span>

<span class="c1"># Compute aggregation weights (weights = overall transmittance for all rays in the chunk)
</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_compute_weights</span><span class="p">(</span>
    <span class="n">deltas</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_pts</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># shape = (self._chunk_size, n_pts, 1)
</span>    <span class="n">predicted_density_for_all_samples_for_all_rays_in_chunk</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_pts</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># shape = (self._chunk_size, n_pts, 1)
</span><span class="p">)</span>

<span class="c1"># TODO (1.5): Render (color) features using weights
# weights.shape = (self._chunk_size, n_pts, 1)
# color.shape = (self._chunk_size*n_pts, 3)
</span><span class="n">color_of_all_rays</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_aggregate</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">predicted_colors_for_all_samples_for_all_rays_in_chunk</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_pts</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># feature = RGB color
</span>
<span class="c1"># TODO (1.5): Render depth map
# depth_values.shape = (self._chunk_size, n_pts)
</span><span class="n">depth_of_all_rays</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_aggregate</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">depth_values</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_pts</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Return
</span><span class="n">cur_out</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'feature'</span><span class="p">:</span> <span class="n">color_of_all_rays</span><span class="p">,</span>
    <span class="s">'depth'</span><span class="p">:</span> <span class="n">depth_of_all_rays</span><span class="p">,</span>
<span class="p">}</span>
<span class="c1"># shape = (N, 3) for feature and (N, 1) for depth
</span></code></pre></div></div>

<p>The function <strong>compute_weights will find the overall transmittance for each ray</strong> and
<strong>compute_aggregate will use this transmittance to find either color or depth for each ray</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_compute_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">deltas</span><span class="p">,</span>
        <span class="n">rays_density</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">):</span>
        <span class="s">"""

        Args:
            deltas : distance between each sample (self._chunk_size, n_pts, 1)
            rays_density (torch.Tensor): (self._chunk_size, n_pts, 1) predicting density values of each sample (from NERF MLP)
            eps (float, optional): Defaults to 1e-10.

        Returns:
            _type_: _description_
        """</span>
        <span class="c1"># TODO (1.5): Compute transmittance using the equation described in the README
</span>        <span class="n">num_rays</span><span class="p">,</span> <span class="n">num_sample_points</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deltas</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">transmittances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">transmittances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_rays</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">deltas</span><span class="p">.</span><span class="n">device</span><span class="p">))</span> <span class="c1"># first transmittance is 1
</span>
        <span class="c1">#! Find the transmittance for each discrete volume = T(x, x_i)
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_sample_points</span><span class="p">):</span>
            <span class="c1"># recursive formula for transmittance
</span>            <span class="n">transmittances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">transmittances</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rays_density</span><span class="p">[:,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">deltas</span><span class="p">[:,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>

        <span class="c1">#! Find = T(x, x_t) * (1 - e^{−σ(x) * δx})
</span>        <span class="n">transmittances_stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">transmittances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># the below line implements the T(x, x_t) * (1 - e^{−σ(x) * δx}) part of the equation =&gt; we'll call this 'weights'
</span>        <span class="k">return</span> <span class="n">transmittances_stacked</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rays_density</span><span class="o">*</span><span class="n">deltas</span><span class="o">+</span><span class="n">eps</span><span class="p">))</span> <span class="c1"># -&gt; weights
</span>
    <span class="k">def</span> <span class="nf">_aggregate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rays_feature</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""

        Args:
            weights (torch.Tensor): (self._chunk_size, n_pts, 1) (Overall Transmittance for each ray)
            rays_feature (torch.Tensor): (self._chunk_size*n_pts, 3) feature = color/depth

        Returns:
            feature : Final Attribute (color or depth) for each ray
        """</span>
        <span class="c1"># TODO (1.5): Aggregate (weighted sum of) features using weights
</span>        <span class="n">feature</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">weights</span><span class="o">*</span><span class="n">rays_feature</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feature</span>
</code></pre></div></div>

<p>Basically, compute_weights finds the <code class="language-plaintext highlighter-rouge">T(x, x_t) * (1 - e^{−σ(x) * δx})</code> part of the equation below:</p>

<p><img src="/images/Computer_Vision/NERFs/color.png" alt="" /></p>

<p><img src="/images/Computer_Vision/NERFs/transmittance.png" alt="" /></p>

<p>And _aggreate finds the <code class="language-plaintext highlighter-rouge">L(x,ω)</code> which can be color or depth for each ray</p>
      <h2 id="nerf-training-loop-simple">
        
        
          <a href="#nerf-training-loop-simple" class="anchor-heading" aria-labelledby="nerf-training-loop-simple"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> NeRF Training Loop (Simple)
        
        
      </h2>
    

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">t_range</span><span class="p">:</span>
            <span class="n">image</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">camera_idx</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span><span class="p">()</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">cuda</span><span class="p">().</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">camera</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="c1"># Sample rays
</span>            <span class="n">xy_grid</span> <span class="o">=</span> <span class="n">get_random_pixels_from_image</span><span class="p">(</span>
                <span class="n">cfg</span><span class="p">.</span><span class="n">training</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">camera</span>
            <span class="p">)</span>
            <span class="n">ray_bundle</span> <span class="o">=</span> <span class="n">get_rays_from_pixels</span><span class="p">(</span>
                <span class="n">xy_grid</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">camera</span>
            <span class="p">)</span>
            <span class="n">rgb_gt</span> <span class="o">=</span> <span class="n">sample_images_at_xy</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">xy_grid</span><span class="p">)</span>

            <span class="c1"># Run model forward
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">ray_bundle</span><span class="p">)</span>

            <span class="c1"># TODO (Q3.1): Calculate loss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s">'feature'</span><span class="p">],</span> <span class="n">rgb_gt</span><span class="p">)</span>

            <span class="c1"># Take the training step.
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

        

        

        
        
          <hr>
          <footer>
            

            <p class="text-small text-grey-dk-100 mb-0"></p>

            
              <div class="d-flex mt-2">
                
                
              </div>
            
          </footer>
        

      </div>
    </div>

    
      

      <div class="search-overlay"></div>
    
  </div>
</body>
</html>

